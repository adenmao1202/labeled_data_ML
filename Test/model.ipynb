{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n┌─────────────────────────────────────────────────────────────────┐\\n│                        數據載入與初始化                           │\\n│  ・讀取訓練數據集                                                │\\n│  ・將列名轉換為字符串類型                                        │\\n│  ・分離特徵(X)和目標變量(y)                                      │\\n└───────────────────────────────┬─────────────────────────────────┘\\n                                ↓\\n┌─────────────────────────────────────────────────────────────────┐\\n│                           數據分割                               │\\n│  ・使用train_test_split分割訓練集和測試集                        │\\n│  ・採用分層抽樣(stratify=y)                                      │\\n│  ・測試集比例為20%                                               │\\n└───────────────────────────────┬─────────────────────────────────┘\\n                                ↓\\n┌─────────────────────────────────────────────────────────────────┐\\n│                       特徵預處理流程                             │\\n├─────────────────────────────────────────────────────────────────┤\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               特徵轉換(Yeo-Johnson)                    │      │\\n│  │  ・對偏斜特徵進行轉換                                  │      │\\n│  │  ・排除二元特徵                                        │      │\\n│  │  ・計算並儲存lambda參數                                │      │\\n│  └─────────────────────────┬─────────────────────────────┘      │\\n│                            ↓                                    │\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               特徵標準化(Z-score)                      │      │\\n│  │  ・對數值特徵進行標準化                                │      │\\n│  │  ・生成標準化特徵({feature}_std)                       │      │\\n│  └─────────────────────────┬─────────────────────────────┘      │\\n│                            ↓                                    │\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               特徵歸一化(Min-Max)                      │      │\\n│  │  ・用於神經網絡模型                                    │      │\\n│  │  ・特徵值縮放到0-1範圍                                 │      │\\n│  │  ・生成歸一化特徵({feature}_norm)                      │      │\\n│  └─────────────────────────┬─────────────────────────────┘      │\\n│                            ↓                                    │\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               降維處理(可選)                           │      │\\n│  │  ・PCA降維(效果不彰)                                   │      │\\n│  │  ・嘗試LDA但不適合                                     │      │\\n│  └───────────────────────────────────────────────────────┘      │\\n└───────────────────────────────┬─────────────────────────────────┘\\n                                ↓\\n┌─────────────────────────────────────────────────────────────────┐\\n│                       類別不平衡處理                             │\\n├─────────────────────────────────────────────────────────────────┤\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               SMOTE過採樣(效果不彰)                    │      │\\n│  │  ・生成少數類別的合成樣本                              │      │\\n│  └─────────────────────────┬─────────────────────────────┘      │\\n│                            ↓                                    │\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               類別權重計算                             │      │\\n│  │  ・反比於類別頻率的權重                                │      │\\n│  │  ・為不同模型準備權重格式                              │      │\\n│  └───────────────────────────────────────────────────────┘      │\\n└───────────────────────────────┬─────────────────────────────────┘\\n                                ↓\\n┌─────────────────────────────────────────────────────────────────┐\\n│                         模型訓練                                 │\\n├─────────────────────────────────────────────────────────────────┤\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               XGBoost模型                              │      │\\n│  │  ・貝葉斯參數優化                                      │      │\\n│  │  ・早停機制                                            │      │\\n│  │  ・特徵重要性分析                                      │      │\\n│  └─────────────────────────┐                             │      │\\n│                            │                             │      │\\n│  ┌───────────────────────────────────────────────────────┐      │\\n│  │               LightGBM模型                             │      │\\n│  │  ・類別權重平衡                                        │      │\\n│  │  ・並行訓練                                            │      │\\n│  │  ・特徵重要性分析                                      │      │\\n│  └─────────────────────────┘     ┌─────────────────────┐ │      │\\n│                                   │                     │ │      │\\n│  ┌───────────────────────────────┐│    堆疊集成模型    │ │      │\\n│  │               神經網絡模型     ││  ・5折交叉驗證     │ │      │\\n│  │  ・多層網絡架構               ││  ・生成元特徵      │ │      │\\n│  │  ・正則化技術                 ││  ・邏輯回歸元模型  │ │      │\\n│  │  ・早停和學習率調度           │└─────────────────────┘ │      │\\n│  └───────────────────────────────┘                        │      │\\n└───────────────────────────────┬─────────────────────────────────┘\\n                                ↓\\n┌─────────────────────────────────────────────────────────────────┐\\n│                         模型評估                                 │\\n├─────────────────────────────────────────────────────────────────┤\\n│  ・計算準確率、平衡準確率和F1分數                                │\\n│  ・生成分類報告                                                  │\\n│  ・繪製混淆矩陣熱圖                                              │\\n│  ・分析特徵重要性                                                │\\n│  ・基於F1宏平均選擇最佳模型                                      │\\n└───────────────────────────────┬─────────────────────────────────┘\\n                                ↓\\n┌─────────────────────────────────────────────────────────────────┐\\n│                       模型保存與應用                             │\\n├─────────────────────────────────────────────────────────────────┤\\n│  ・使用joblib保存完整模型                                        │\\n│  ・包含預處理流程和訓練好的模型                                  │\\n│  ・對新數據應用相同的預處理步驟                                  │\\n│  ・生成預測結果和概率                                            │\\n│  ・評估新數據上的模型表現                                        │\\n└─────────────────────────────────────────────────────────────────┘\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                        數據載入與初始化                           │\n",
    "│  ・讀取訓練數據集                                                │\n",
    "│  ・將列名轉換為字符串類型                                        │\n",
    "│  ・分離特徵(X)和目標變量(y)                                      │\n",
    "└───────────────────────────────┬─────────────────────────────────┘\n",
    "                                ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                           數據分割                               │\n",
    "│  ・使用train_test_split分割訓練集和測試集                        │\n",
    "│  ・採用分層抽樣(stratify=y)                                      │\n",
    "│  ・測試集比例為20%                                               │\n",
    "└───────────────────────────────┬─────────────────────────────────┘\n",
    "                                ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       特徵預處理流程                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               特徵轉換(Yeo-Johnson)                    │      │\n",
    "│  │  ・對偏斜特徵進行轉換                                  │      │\n",
    "│  │  ・排除二元特徵                                        │      │\n",
    "│  │  ・計算並儲存lambda參數                                │      │\n",
    "│  └─────────────────────────┬─────────────────────────────┘      │\n",
    "│                            ↓                                    │\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               特徵標準化(Z-score)                      │      │\n",
    "│  │  ・對數值特徵進行標準化                                │      │\n",
    "│  │  ・生成標準化特徵({feature}_std)                       │      │\n",
    "│  └─────────────────────────┬─────────────────────────────┘      │\n",
    "│                            ↓                                    │\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               特徵歸一化(Min-Max)                      │      │\n",
    "│  │  ・用於神經網絡模型                                    │      │\n",
    "│  │  ・特徵值縮放到0-1範圍                                 │      │\n",
    "│  │  ・生成歸一化特徵({feature}_norm)                      │      │\n",
    "│  └─────────────────────────┬─────────────────────────────┘      │\n",
    "│                            ↓                                    │\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               降維處理(可選)                           │      │\n",
    "│  │  ・PCA降維(效果不彰)                                   │      │\n",
    "│  │  ・嘗試LDA但不適合                                     │      │\n",
    "│  └───────────────────────────────────────────────────────┘      │\n",
    "└───────────────────────────────┬─────────────────────────────────┘\n",
    "                                ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       類別不平衡處理                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               SMOTE過採樣(效果不彰)                    │      │\n",
    "│  │  ・生成少數類別的合成樣本                              │      │\n",
    "│  └─────────────────────────┬─────────────────────────────┘      │\n",
    "│                            ↓                                    │\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               類別權重計算                             │      │\n",
    "│  │  ・反比於類別頻率的權重                                │      │\n",
    "│  │  ・為不同模型準備權重格式                              │      │\n",
    "│  └───────────────────────────────────────────────────────┘      │\n",
    "└───────────────────────────────┬─────────────────────────────────┘\n",
    "                                ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         模型訓練                                 │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               XGBoost模型                              │      │\n",
    "│  │  ・貝葉斯參數優化                                      │      │\n",
    "│  │  ・早停機制                                            │      │\n",
    "│  │  ・特徵重要性分析                                      │      │\n",
    "│  └─────────────────────────┐                             │      │\n",
    "│                            │                             │      │\n",
    "│  ┌───────────────────────────────────────────────────────┐      │\n",
    "│  │               LightGBM模型                             │      │\n",
    "│  │  ・類別權重平衡                                        │      │\n",
    "│  │  ・並行訓練                                            │      │\n",
    "│  │  ・特徵重要性分析                                      │      │\n",
    "│  └─────────────────────────┘     ┌─────────────────────┐ │      │\n",
    "│                                   │                     │ │      │\n",
    "│  ┌───────────────────────────────┐│    堆疊集成模型    │ │      │\n",
    "│  │               神經網絡模型     ││  ・5折交叉驗證     │ │      │\n",
    "│  │  ・多層網絡架構               ││  ・生成元特徵      │ │      │\n",
    "│  │  ・正則化技術                 ││  ・邏輯回歸元模型  │ │      │\n",
    "│  │  ・早停和學習率調度           │└─────────────────────┘ │      │\n",
    "│  └───────────────────────────────┘                        │      │\n",
    "└───────────────────────────────┬─────────────────────────────────┘\n",
    "                                ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         模型評估                                 │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ・計算準確率、平衡準確率和F1分數                                │\n",
    "│  ・生成分類報告                                                  │\n",
    "│  ・繪製混淆矩陣熱圖                                              │\n",
    "│  ・分析特徵重要性                                                │\n",
    "│  ・基於F1宏平均選擇最佳模型                                      │\n",
    "└───────────────────────────────┬─────────────────────────────────┘\n",
    "                                ↓\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                       模型保存與應用                             │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│  ・使用joblib保存完整模型                                        │\n",
    "│  ・包含預處理流程和訓練好的模型                                  │\n",
    "│  ・對新數據應用相同的預處理步驟                                  │\n",
    "│  ・生成預測結果和概率                                            │\n",
    "│  ・評估新數據上的模型表現                                        │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流程解釋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. 數據載入與初始化\n",
    "這是流程的起點，主要任務是準備數據以便後續處理。將所有列名轉換為字符串類型是為了確保一致性，避免在後續處理中因數據類型不一致而產生錯誤。\n",
    "\n",
    "### 2. 數據分割\n",
    "使用分層抽樣確保訓練集和測試集中的類別分佈一致，這對於不平衡數據集尤為重要。固定隨機種子（random_state=42）確保結果可重現。\n",
    "\n",
    "### 3. 特徵預處理流程\n",
    "這是一個多步驟的流程，每個步驟都針對特定的數據特性：\n",
    "\n",
    "- **Yeo-Johnson轉換**：處理偏斜數據，使其更接近正態分佈，有助於提高許多模型的性能。特別排除二元特徵，因為轉換對它們沒有意義。\n",
    "\n",
    "- **Z-score標準化**：將特徵調整為零均值和單位方差，使不同尺度的特徵可比較，對於基於距離的算法和神經網絡尤為重要。\n",
    "\n",
    "- **Min-Max歸一化**：專為神經網絡設計，將特徵縮放到0-1範圍，有助於加速神經網絡的收斂。\n",
    "\n",
    "- **降維處理**：嘗試了PCA和LDA，但效果不彰。這表明原始特徵可能已經包含了重要的分類信息，降維會損失這些信息。\n",
    "\n",
    "### 4. 類別不平衡處理\n",
    "處理不平衡數據集的兩種方法：\n",
    "\n",
    "- **SMOTE過採樣**：嘗試生成少數類別的合成樣本，但效果不彰，可能是因為合成樣本無法捕捉真實數據的複雜性。\n",
    "\n",
    "- **類別權重計算**：為不同類別分配不同權重，使模型更關注少數類別。這種方法在實踐中效果更好，可能是因為它保留了原始數據的真實分佈。\n",
    "\n",
    "### 5. 模型訓練\n",
    "訓練了三種基礎模型，並將它們組合成堆疊集成模型：\n",
    "\n",
    "- **XGBoost模型**：使用貝葉斯優化尋找最佳參數，這比傳統的網格搜索更高效。早停機制防止過擬合，特徵重要性分析提供了模型解釋性。\n",
    "\n",
    "- **LightGBM模型**：另一種梯度提升樹實現，通常比XGBoost更快。使用類別權重處理不平衡問題，並行訓練加速計算。\n",
    "\n",
    "- **神經網絡模型**：使用多層架構捕捉複雜模式，應用多種正則化技術防止過擬合，使用早停和學習率調度優化訓練過程。\n",
    "\n",
    "- **堆疊集成模型**：將上述基礎模型的預測結果作為新特徵（元特徵），使用邏輯回歸作為元模型進行最終預測。選擇邏輯回歸是為了保持簡單避免過擬合，5折交叉驗證確保元特徵的可靠性。\n",
    "\n",
    "### 6. 模型評估\n",
    "使用多種指標全面評估模型性能：\n",
    "\n",
    "- 準確率、平衡準確率和F1分數提供了不同角度的性能度量\n",
    "- 分類報告詳細展示了每個類別的精確率、召回率和F1分數\n",
    "- 混淆矩陣熱圖直觀顯示了預測錯誤的分佈\n",
    "- 特徵重要性分析揭示了哪些特徵對預測最有貢獻\n",
    "- 基於F1宏平均選擇最佳模型，這對於不平衡數據集是一個合理的選擇\n",
    "\n",
    "### 7. 模型保存與應用\n",
    "將完整模型（包括預處理流程）保存為單一文件，確保在應用階段使用完全相同的處理步驟。這種方法避免了因預處理不一致導致的預測錯誤，是機器學習部署的最佳實踐。\n",
    "\n",
    "### 關鍵發現與優化\n",
    "\n",
    "1. **預處理的重要性**：完整的預處理流程對模型性能至關重要, 且能提升模型效果（雖然可能沒有大幅提升）\n",
    "\n",
    "2. **降維與過採樣的局限性**：PCA降維和SMOTE過採樣在此數據集上效果不彰，說明並非所有常用技術都適用於每個問題。\n",
    "\n",
    "3. **類別權重的有效性**：相比於過採樣，類別權重策略更有效地處理了不平衡問題。\n",
    "\n",
    "4. **堆疊集成的優勢**：通過組合不同模型的優勢，堆疊集成提高了整體預測性能。\n",
    "\n",
    "5. **簡單元模型的選擇**：使用邏輯回歸作為元模型是為了避免過擬合，在元特徵空間中，簡單模型通常已足夠。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path1 = '/Users/mouyasushi/Desktop/vici holdings/Test/data/train_data.npy'\n",
    "train_data = np.load(path1)\n",
    "\n",
    "path2 = '/Users/mouyasushi/Desktop/vici holdings/Test/data/train_labels.npy'\n",
    "train_labels = np.load(path2)\n",
    "\n",
    "# 轉換為 DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df['label'] = train_labels  # 添加標籤列\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score, \n",
    "                             f1_score, balanced_accuracy_score, roc_auc_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import libraries for neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import tools for handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "class FeatureNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_normalize=None, range=(0, 1)):\n",
    "        self.features_to_normalize = features_to_normalize\n",
    "        self.range = range\n",
    "        self.normalizers_ = {}  # Store individual normalizers for each feature\n",
    "        self.normalized_feature_names_ = []  # Store names of normalized features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no features are specified, use all numeric features\n",
    "        if self.features_to_normalize is None:\n",
    "            self.features_to_normalize = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature in tqdm(self.features_to_normalize, desc=\"Fitting normalization\"):\n",
    "            if feature in X.columns and np.issubdtype(X[feature].dtype, np.number):\n",
    "                # Create an individual normalizer for each feature\n",
    "                normalizer = MinMaxScaler(feature_range=self.range)\n",
    "                normalizer.fit(X[[feature]])\n",
    "                self.normalizers_[feature] = normalizer\n",
    "                self.normalized_feature_names_.append(f\"{feature}_norm\")\n",
    "                print(f\"Normalization for {feature} fitted, range: [{self.range[0]}, {self.range[1]}]\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature, normalizer in tqdm(self.normalizers_.items(), desc=\"Applying normalization\"):\n",
    "            if feature in X.columns:\n",
    "                # Apply saved normalizer for normalization\n",
    "                X[f\"{feature}_norm\"] = normalizer.transform(X[[feature]])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        # Return normalized feature names\n",
    "        return self.normalized_feature_names_\n",
    "\n",
    "\n",
    "class StackingClassifier:\n",
    "    \n",
    "    def __init__(self, base_models, meta_model, n_folds=5, use_proba=True, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize Stacking Classifier\n",
    "        \n",
    "        Parameters:\n",
    "        base_models: Dictionary of base models {name: model}\n",
    "        meta_model: Meta model\n",
    "        n_folds: Number of cross-validation folds\n",
    "        use_proba: Whether to use probability predictions as meta features\n",
    "        random_state: Random seed\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.use_proba = use_proba\n",
    "        self.random_state = random_state\n",
    "        self.base_model_preds = {}  # Store predictions from base models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the stacking model\n",
    "        \n",
    "        Parameters:\n",
    "        X: Training features\n",
    "        y: Training labels\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        \n",
    "        # Create cross-validation object\n",
    "        kf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Store meta features\n",
    "        meta_features = {}\n",
    "        \n",
    "        # Perform cross-validation predictions for each base model\n",
    "        for name, model in self.base_models.items():\n",
    "            print(f\"\\nTraining base model: {name}\")\n",
    "            \n",
    "            # Initialize an array to store predictions\n",
    "            if self.use_proba:\n",
    "                n_classes = len(np.unique(y))\n",
    "                train_meta_preds = np.zeros((X.shape[0], n_classes))\n",
    "            else:\n",
    "                train_meta_preds = np.zeros(X.shape[0])\n",
    "            \n",
    "            # Perform K-fold cross-validation\n",
    "            for i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "                print(f\"  Fold {i+1}/{self.n_folds}\")\n",
    "                \n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                else:\n",
    "                    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "\n",
    "                if isinstance(y, pd.Series):\n",
    "                    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                else:\n",
    "                    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Generate predictions\n",
    "    \n",
    "                if self.use_proba:\n",
    "                    fold_preds = model.predict_proba(X_val_fold)\n",
    "    \n",
    "                    for i, idx in enumerate(val_idx):\n",
    "                        train_meta_preds[idx] = fold_preds[i]\n",
    "                else:\n",
    "                    fold_preds = model.predict(X_val_fold)\n",
    "    \n",
    "                    for i, idx in enumerate(val_idx):\n",
    "                        train_meta_preds[idx] = fold_preds[i]\n",
    "            \n",
    "            meta_features[name] = train_meta_preds\n",
    "            \n",
    "            # Retrain the model on the entire dataset\n",
    "            print(f\"  Training {name} on the entire data\")\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        # Prepare training data for meta model\n",
    "        meta_X = self._prepare_meta_features(meta_features)\n",
    "        \n",
    "        # Train the meta model\n",
    "        print(\"\\nTraining meta model...\")\n",
    "        self.meta_model.fit(meta_X, y)\n",
    "        \n",
    "        # Save base models' predictions\n",
    "        self.base_model_preds = meta_features\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained stacking model to predict\n",
    "        \n",
    "        Parameters:\n",
    "        X: Test features\n",
    "        \n",
    "        Returns:\n",
    "        Predicted labels\n",
    "        \"\"\"\n",
    "        meta_features = {}\n",
    "        \n",
    "        # Generate predictions using each base model\n",
    "        for name, model in self.base_models.items():\n",
    "            if self.use_proba:\n",
    "                preds = model.predict_proba(X)\n",
    "            else:\n",
    "                preds = model.predict(X)\n",
    "            meta_features[name] = preds\n",
    "        \n",
    "        # Prepare meta features for meta model\n",
    "        meta_X = self._prepare_meta_features(meta_features)\n",
    "        \n",
    "        # Final prediction using meta model\n",
    "        return self.meta_model.predict(meta_X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained stacking model to predict probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        X: Test features\n",
    "        \n",
    "        Returns:\n",
    "        Predicted probabilities\n",
    "        \"\"\"\n",
    "        meta_features = {}\n",
    "        \n",
    "        # Generate predictions using each base model\n",
    "        for name, model in self.base_models.items():\n",
    "            if self.use_proba:\n",
    "                preds = model.predict_proba(X)\n",
    "            else:\n",
    "                preds = model.predict(X)\n",
    "            meta_features[name] = preds\n",
    "        \n",
    "        # Prepare meta features for meta model\n",
    "        meta_X = self._prepare_meta_features(meta_features)\n",
    "        \n",
    "        # Final probability prediction using meta model\n",
    "        return self.meta_model.predict_proba(meta_X)\n",
    "    \n",
    "    def _prepare_meta_features(self, meta_features):\n",
    "        \"\"\"\n",
    "        Prepare meta features\n",
    "        \n",
    "        Parameters:\n",
    "        meta_features: Dictionary of predictions from base models\n",
    "        \n",
    "        Returns:\n",
    "        Combined meta features array\n",
    "        \"\"\"\n",
    "        # Combine predictions from all base models into one array\n",
    "        all_features = []\n",
    "        \n",
    "        for name, preds in meta_features.items():\n",
    "            # Ensure predictions are 2D arrays\n",
    "            if preds.ndim == 1:\n",
    "                preds = preds.reshape(-1, 1)\n",
    "            all_features.append(preds)\n",
    "        \n",
    "        # Concatenate all features horizontally\n",
    "        return np.hstack(all_features)\n",
    "\n",
    "\n",
    "# Custom transformer: Feature transformation (Yeo-Johnson)\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_transform=None, binary_features=None, method='yeo-johnson'):\n",
    "        self.features_to_transform = features_to_transform\n",
    "        self.binary_features = binary_features if binary_features is not None else []\n",
    "        self.method = method\n",
    "        self.lambdas_ = {}  # Store lambda values for each feature\n",
    "        self.transformed_feature_names_ = []  # Store transformed feature names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no features specified, use all numeric features\n",
    "        if self.features_to_transform is None:\n",
    "            self.features_to_transform = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Exclude binary features\n",
    "        features_to_process = list(set(self.features_to_transform) - set(self.binary_features))\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature in tqdm(features_to_process, desc=\"Fitting transformations\"):\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "                \n",
    "            if not np.issubdtype(X[feature].dtype, np.number):\n",
    "                continue\n",
    "            \n",
    "            # For Yeo-Johnson transformation, save lambda values\n",
    "            if self.method == 'yeo-johnson':\n",
    "                try:\n",
    "                    # Fit transformer and save lambda value\n",
    "                    _, lmbda = stats.yeojohnson(X[feature])\n",
    "                    self.lambdas_[feature] = lmbda\n",
    "                    self.transformed_feature_names_.append(f\"{feature}_yeojohnson\")\n",
    "                    print(f\"Fitted yeo-johnson transformation for {feature} with lambda={lmbda:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting {self.method} transformation to {feature}: {str(e)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature, lmbda in tqdm(self.lambdas_.items(), desc=\"Applying transformations\"):\n",
    "            if feature in X.columns:\n",
    "                try:\n",
    "                    # Apply transformation using saved lambda value\n",
    "                    X[f\"{feature}_yeojohnson\"] = stats.yeojohnson(X[feature], lmbda=lmbda)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying {self.method} transformation to {feature}: {str(e)}\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        # Return transformed feature names\n",
    "        return self.transformed_feature_names_\n",
    "\n",
    "\n",
    "# Custom transformer: Feature standardization\n",
    "class FeatureStandardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_standardize=None):\n",
    "        self.features_to_standardize = features_to_standardize\n",
    "        self.scalers_ = {}  # Store a separate scaler for each feature\n",
    "        self.standardized_feature_names_ = []  # Store standardized feature names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no features specified, use all numeric features\n",
    "        if self.features_to_standardize is None:\n",
    "            self.features_to_standardize = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature in tqdm(self.features_to_standardize, desc=\"Fitting standardization\"):\n",
    "            if feature in X.columns and np.issubdtype(X[feature].dtype, np.number):\n",
    "                # Create a separate scaler for each feature\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X[[feature]])\n",
    "                self.scalers_[feature] = scaler\n",
    "                self.standardized_feature_names_.append(f\"{feature}_std\")\n",
    "                print(f\"Fitted standardization for {feature} with mean={scaler.mean_[0]:.4f}, scale={scaler.scale_[0]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature, scaler in tqdm(self.scalers_.items(), desc=\"Applying standardization\"):\n",
    "            if feature in X.columns:\n",
    "                # Apply standardization using saved scaler\n",
    "                X[f\"{feature}_std\"] = scaler.transform(X[[feature]])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        # Return standardized feature names\n",
    "        return self.standardized_feature_names_\n",
    "\n",
    "\n",
    "# Feature selector: Select specified feature columns\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names=None):\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # Select specified features\n",
    "        if self.feature_names:\n",
    "            # Check which features exist in the data\n",
    "            available_features = [f for f in self.feature_names if f in X.columns]\n",
    "            if len(available_features) < len(self.feature_names):\n",
    "                missing = set(self.feature_names) - set(available_features)\n",
    "                print(f\"Warning: Some features are missing: {missing}\")\n",
    "            \n",
    "            return X[available_features]\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with confusion matrix and metrics\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: True labels\n",
    "    y_pred: Predicted labels\n",
    "    dataset_name: Name of the dataset for display purposes\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Evaluation on {dataset_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    \n",
    "    # Generate classification report\n",
    "    try:\n",
    "        report = classification_report(y_true, y_pred, labels=classes, output_dict=False)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating classification report: {str(e)}\")\n",
    "    \n",
    "    # Attempt to plot confusion matrix\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=classes, \n",
    "                    yticklabels=classes)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{dataset_name.replace(\" \", \"_\")}.png')\n",
    "        print(f\"Confusion matrix saved to confusion_matrix_{dataset_name.replace(' ', '_')}.png\")\n",
    "        plt.close()  # Close the plot to avoid display issues\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix: {str(e)}\")\n",
    "        # Print confusion matrix as fallback\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return accuracy, cm, report\n",
    "\n",
    "\n",
    "class PCATransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=None, variance_threshold=0.95):\n",
    "        self.n_components = n_components\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.pca_ = None\n",
    "        self.feature_names_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If n_components is None, use variance threshold\n",
    "        if self.n_components is None:\n",
    "            # First, fit a full PCA\n",
    "            temp_pca = PCA()\n",
    "            temp_pca.fit(X)\n",
    "            \n",
    "            # Determine how many components are needed to reach the variance threshold\n",
    "            cumulative_variance = np.cumsum(temp_pca.explained_variance_ratio_)\n",
    "            self.n_components = np.argmax(cumulative_variance >= self.variance_threshold) + 1\n",
    "            print(f\"Selected {self.n_components} components to explain {self.variance_threshold*100:.1f}% of variance\")\n",
    "        \n",
    "        # Create and fit PCA\n",
    "        self.pca_ = PCA(n_components=self.n_components)\n",
    "        self.pca_.fit(X)\n",
    "        \n",
    "        # Create feature names\n",
    "        self.feature_names_ = [f\"PC{i+1}\" for i in range(self.n_components)]\n",
    "        \n",
    "        # Print explained variance ratios\n",
    "        explained_variance = self.pca_.explained_variance_ratio_\n",
    "        print(f\"Top 5 components explain: {explained_variance[:5].sum()*100:.2f}% of variance\")\n",
    "        print(f\"All {self.n_components} components explain: {explained_variance.sum()*100:.2f}% of variance\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # Apply PCA transformation\n",
    "        X_pca = self.pca_.transform(X)\n",
    "        \n",
    "        # Convert to DataFrame and add column names\n",
    "        X_pca_df = pd.DataFrame(X_pca, columns=self.feature_names_, index=X.index)\n",
    "        \n",
    "        return X_pca_df\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.feature_names_\n",
    "\n",
    "\n",
    "class TrainingVisualizationCallback(Callback):\n",
    "    def __init__(self, validation_data=None):\n",
    "        super(TrainingVisualizationCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.train_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_loss = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_acc.append(logs.get('accuracy'))\n",
    "        \n",
    "        if self.validation_data:\n",
    "            val_logs = self.model.evaluate(\n",
    "                self.validation_data[0], self.validation_data[1], \n",
    "                verbose=0\n",
    "            )\n",
    "            self.val_loss.append(val_logs[0])\n",
    "            self.val_acc.append(val_logs[1])\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_loss) + 1)\n",
    "        \n",
    "        # Create a 2x1 subplot layout\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot loss curves\n",
    "        ax1.plot(epochs, self.train_loss, 'r-', label='Training')\n",
    "        if self.validation_data:\n",
    "            ax1.plot(epochs, self.val_loss, 'b-', label='Validation')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot accuracy curves\n",
    "        ax2.plot(epochs, self.train_acc, 'r-', label='Training')\n",
    "        if self.validation_data:\n",
    "            ax2.plot(epochs, self.val_acc, 'b-', label='Validation')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.close()\n",
    "        print(\"Training metrics plot saved to training_metrics.png\")\n",
    "        \n",
    "        # Calculate error rate and plot\n",
    "        train_error = [1 - acc for acc in self.train_acc]\n",
    "        if self.validation_data:\n",
    "            val_error = [1 - acc for acc in self.val_acc]\n",
    "            \n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(epochs, train_error, 'r-', label='Training')\n",
    "            plt.plot(epochs, val_error, 'b-', label='Validation')\n",
    "            plt.title('Model Error Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Error Rate')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('error_rate.png')\n",
    "            plt.close()\n",
    "            print(\"Error rate plot saved to error_rate.png\")\n",
    "\n",
    "class NeuralNetworkClassifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, input_dim=200, epochs=15, batch_size=64, learning_rate=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.history = None\n",
    "        self.visualization_callback = None\n",
    "        \n",
    "    def build_model(self, input_shape, n_classes):\n",
    "        model = Sequential([\n",
    "            # First layer\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,),\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Second layer\n",
    "            Dense(64, activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Third layer\n",
    "            Dense(32, activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(n_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        n_classes = len(np.unique(y_encoded))\n",
    "        \n",
    "        # Convert to one-hot encoding\n",
    "        y_onehot = to_categorical(y_encoded)\n",
    "        \n",
    "        # Prepare validation data\n",
    "        validation_data = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            if isinstance(X_val, pd.DataFrame):\n",
    "                X_val = X_val.values\n",
    "            y_val_encoded = self.label_encoder.transform(y_val)\n",
    "            y_val_onehot = to_categorical(y_val_encoded)\n",
    "            validation_data = (X_val, y_val_onehot)\n",
    "        \n",
    "        # Compute class weights\n",
    "        y_integers = np.argmax(y_onehot, axis=1)\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(X.shape[1], n_classes)\n",
    "        \n",
    "        # Create visualization callback\n",
    "        self.visualization_callback = TrainingVisualizationCallback(validation_data)\n",
    "        \n",
    "        # Add callbacks\n",
    "        callbacks = [\n",
    "            # Early stopping\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            # Learning rate scheduler\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            # Visualization callback\n",
    "            self.visualization_callback\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            X, y_onehot,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            validation_data=validation_data\n",
    "        )\n",
    "        \n",
    "        # Plot training metrics\n",
    "        self.visualization_callback.plot_metrics()\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history curves\"\"\"\n",
    "        history = self.history\n",
    "        \n",
    "        # Create a 2x1 subplot layout\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot loss curve\n",
    "        ax1.plot(history.history['loss'], 'r-', label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            ax1.plot(history.history['val_loss'], 'b-', label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot accuracy curve\n",
    "        ax2.plot(history.history['accuracy'], 'r-', label='Training Accuracy')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            ax2.plot(history.history['val_accuracy'], 'b-', label='Validation Accuracy')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('nn_training_history.png')\n",
    "        plt.close()\n",
    "        print(\"Neural network training history saved to nn_training_history.png\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Predict probabilities\n",
    "        y_pred_probs = self.model.predict(X, verbose=0)\n",
    "        \n",
    "        # Adjust the threshold for class 1 to improve recall\n",
    "        adjusted_probs = y_pred_probs.copy()\n",
    "        adjusted_probs[:, 1] = adjusted_probs[:, 1] * 1.15  # Increase probability for class 1\n",
    "        \n",
    "        # Convert to class labels\n",
    "        y_pred = np.argmax(adjusted_probs, axis=1)\n",
    "        \n",
    "        # Convert back to original labels\n",
    "        return self.label_encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Predict probabilities\n",
    "        return self.model.predict(X, verbose=0)        \n",
    "\n",
    "\n",
    "def train_advanced_models(train_df, target_column, skewed_features=None, binary_features=None, \n",
    "                          high_kurtosis_features=None, use_pca=False, use_smote=True,\n",
    "                          n_pca_components=None, pca_variance=0.95, use_bayes_opt=False,\n",
    "                          use_stacking=False):\n",
    "    \"\"\"\n",
    "    Train advanced models and evaluate performance\n",
    "    \n",
    "    Parameters:\n",
    "    train_df: Training data DataFrame\n",
    "    target_column: Target variable column name\n",
    "    skewed_features: List of skewed features\n",
    "    binary_features: List of binary features\n",
    "    high_kurtosis_features: List of high kurtosis features\n",
    "    use_pca: Whether to use PCA\n",
    "    use_smote: Whether to use SMOTE oversampling\n",
    "    n_pca_components: Number of PCA components\n",
    "    pca_variance: PCA variance threshold\n",
    "    use_bayes_opt: Whether to use Bayesian Optimization for XGBoost parameters\n",
    "    use_stacking: Whether to use Stacking ensemble method\n",
    "    \n",
    "    Returns:\n",
    "    Best model, results dictionary, preprocessor\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    train_df.columns = train_df.columns.astype(str)\n",
    "    \n",
    "    # Convert features to string if needed\n",
    "    if skewed_features is not None:\n",
    "        skewed_features = [str(f) for f in skewed_features]\n",
    "    \n",
    "    if binary_features is not None:\n",
    "        binary_features = [str(f) for f in binary_features]\n",
    "    \n",
    "    if high_kurtosis_features is not None:\n",
    "        high_kurtosis_features = [str(f) for f in high_kurtosis_features]\n",
    "    \n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ADVANCED MODEL TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Target column: {target_column}\")\n",
    "    print(f\"Using PCA: {use_pca}\")\n",
    "    print(f\"Using SMOTE: {use_smote}\")\n",
    "    print(f\"Using Bayes Optimization: {use_bayes_opt}\")\n",
    "    print(f\"Using Stacking: {use_stacking}\")\n",
    "    \n",
    "    \n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "    print(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
    "    \n",
    "    \n",
    "    if skewed_features is not None:\n",
    "        skewed_in_data = [f for f in skewed_features if f in X.columns]\n",
    "        features_to_transform = list(set(skewed_in_data) - set(binary_features or []))\n",
    "    else:\n",
    "        features_to_transform = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Features to standardize\n",
    "    if high_kurtosis_features is not None:\n",
    "        kurt_in_data = [f for f in high_kurtosis_features if f in X.columns]\n",
    "        features_to_standardize = kurt_in_data.copy()\n",
    "    else:\n",
    "        features_to_standardize = []\n",
    "    \n",
    "    # Create preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        ('transformer', FeatureTransformer(features_to_transform=features_to_transform, \n",
    "                                             binary_features=binary_features)),\n",
    "        ('standardizer', FeatureStandardizer(features_to_standardize=None))  # Standardizes all features\n",
    "    ]\n",
    "    \n",
    "    if use_pca:\n",
    "        preprocessing_steps.append(\n",
    "            ('pca', PCATransformer(n_components=n_pca_components, variance_threshold=pca_variance))\n",
    "        )\n",
    "    \n",
    "    # Create preprocessor pipeline\n",
    "    preprocessing = Pipeline(preprocessing_steps)\n",
    "    \n",
    "    # Create preprocessing pipeline for neural network (first standardize then normalize)\n",
    "    nn_preprocessing_steps = [\n",
    "        ('transformer', FeatureTransformer(features_to_transform=features_to_transform, \n",
    "                                             binary_features=binary_features)),\n",
    "        ('standardizer', FeatureStandardizer(features_to_standardize=None)),  # Standardize first\n",
    "        ('normalizer', FeatureNormalizer())  # Then normalize\n",
    "    ]\n",
    "    \n",
    "    if use_pca:\n",
    "        nn_preprocessing_steps.append(\n",
    "            ('pca', PCATransformer(n_components=n_pca_components, variance_threshold=pca_variance))\n",
    "        )\n",
    "    \n",
    "    nn_preprocessing = Pipeline(nn_preprocessing_steps)\n",
    "    \n",
    "    # Fit preprocessor\n",
    "    print(\"\\nFitting regular preprocessor pipeline...\")\n",
    "    X_train_processed = preprocessing.fit_transform(X_train)\n",
    "    X_test_processed = preprocessing.transform(X_test)\n",
    "    \n",
    "    print(\"\\nFitting neural network preprocessor pipeline...\")\n",
    "    X_train_nn_processed = nn_preprocessing.fit_transform(X_train)\n",
    "    X_test_nn_processed = nn_preprocessing.transform(X_test)\n",
    "    \n",
    "    print(f\"Shape of training data after regular processing: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of test data after regular processing: {X_test_processed.shape}\")\n",
    "    print(f\"Shape of training data after neural network processing: {X_train_nn_processed.shape}\")\n",
    "    print(f\"Shape of test data after neural network processing: {X_test_nn_processed.shape}\")\n",
    "    \n",
    "    # Apply SMOTE oversampling if enabled\n",
    "    if use_smote:\n",
    "        print(\"\\nApplying SMOTE oversampling...\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "        X_train_nn_resampled, y_train_nn_resampled = smote.fit_resample(X_train_nn_processed, y_train)\n",
    "        print(f\"Shape of training data after resampling: {X_train_resampled.shape}\")\n",
    "        print(f\"Class distribution after resampling: {np.bincount(y_train_resampled)}\")\n",
    "    else:\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "        X_train_nn_resampled, y_train_nn_resampled = X_train_nn_processed, y_train\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    print(f\"Class weights: {weight_dict}\")\n",
    "    \n",
    "    # Prepare class weights for neural network\n",
    "    nn_class_weight = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "    # If using Bayesian Optimization, optimize XGBoost parameters\n",
    "    if use_bayes_opt:\n",
    "        print(\"\\nUsing Bayesian Optimization to tune XGBoost parameters...\")\n",
    "        # Split training set into training and validation sets\n",
    "        X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "            X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
    "        )\n",
    "        \n",
    "        # Perform Bayesian Optimization\n",
    "        best_params, _ = optimize_xgboost_params(\n",
    "            X_train_opt, y_train_opt, X_val_opt, y_val_opt, n_iter=30\n",
    "        )\n",
    "        \n",
    "        # Create XGBoost model using best parameters\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            **best_params,\n",
    "            objective='multi:softprob',\n",
    "            num_class=len(np.unique(y)),\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=['mlogloss', 'merror']\n",
    "        )\n",
    "    else:\n",
    "        # Use default parameters\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.01,\n",
    "            gamma=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            colsample_bylevel=0.8,\n",
    "            objective='multi:softprob',\n",
    "            num_class=len(np.unique(y)),\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            reg_alpha=0.2,\n",
    "            reg_lambda=1.5,\n",
    "            # Note: scale_pos_weight should be a scalar value. For multiclass problems, you may omit this parameter or use class weights.\n",
    "            tree_method='hist',\n",
    "            eval_metric=['mlogloss', 'merror']\n",
    "        )\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        \"XGBoost\": xgb_model,\n",
    "        \n",
    "        \"LightGBM\": lgb.LGBMClassifier(\n",
    "            n_estimators=200, max_depth=8, learning_rate=0.05, num_leaves=31,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "            objective='multiclass', num_class=len(np.unique(y)), \n",
    "            class_weight='balanced',\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        \"NeuralNetwork\": NeuralNetworkClassifier(\n",
    "            input_dim=X_train_nn_processed.shape[1],\n",
    "            learning_rate=0.001,\n",
    "            batch_size=128,\n",
    "            epochs=30\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Compare results\n",
    "    results = {}\n",
    "    \n",
    "    # If using Stacking, create Stacking model\n",
    "    if use_stacking:\n",
    "        print(\"\\nPreparing Stacking model...\")\n",
    "        \n",
    "        # Prepare base models\n",
    "        base_models = {\n",
    "            \"XGBoost\": models[\"XGBoost\"],\n",
    "            \"LightGBM\": models[\"LightGBM\"]\n",
    "        }\n",
    "        \n",
    "        # Create meta model (using Logistic Regression)\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        meta_model = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create Stacking Classifier\n",
    "        stacking_model = StackingClassifier(\n",
    "            base_models=base_models,\n",
    "            meta_model=meta_model,\n",
    "            n_folds=5,\n",
    "            use_proba=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "class FeatureNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_normalize=None, range=(0, 1)):\n",
    "        self.features_to_normalize = features_to_normalize\n",
    "        self.range = range\n",
    "        self.normalizers_ = {}  # Store individual normalizers for each feature\n",
    "        self.normalized_feature_names_ = []  # Store names of normalized features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no features are specified, use all numeric features\n",
    "        if self.features_to_normalize is None:\n",
    "            self.features_to_normalize = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature in tqdm(self.features_to_normalize, desc=\"Fitting normalization\"):\n",
    "            if feature in X.columns and np.issubdtype(X[feature].dtype, np.number):\n",
    "                # Create an individual normalizer for each feature\n",
    "                normalizer = MinMaxScaler(feature_range=self.range)\n",
    "                normalizer.fit(X[[feature]])\n",
    "                self.normalizers_[feature] = normalizer\n",
    "                self.normalized_feature_names_.append(f\"{feature}_norm\")\n",
    "                print(f\"Normalization for {feature} fitted, range: [{self.range[0]}, {self.range[1]}]\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature, normalizer in tqdm(self.normalizers_.items(), desc=\"Applying normalization\"):\n",
    "            if feature in X.columns:\n",
    "                # Apply saved normalizer for normalization\n",
    "                X[f\"{feature}_norm\"] = normalizer.transform(X[[feature]])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        # Return normalized feature names\n",
    "        return self.normalized_feature_names_\n",
    "\n",
    "\n",
    "class StackingClassifier:\n",
    "    \n",
    "    def __init__(self, base_models, meta_model, n_folds=5, use_proba=True, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize Stacking Classifier\n",
    "        \n",
    "        Parameters:\n",
    "        base_models: Dictionary of base models {name: model}\n",
    "        meta_model: Meta model\n",
    "        n_folds: Number of cross-validation folds\n",
    "        use_proba: Whether to use probability predictions as meta features\n",
    "        random_state: Random seed\n",
    "        \"\"\"\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        self.use_proba = use_proba\n",
    "        self.random_state = random_state\n",
    "        self.base_model_preds = {}  # Store predictions from base models\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the stacking model\n",
    "        \n",
    "        Parameters:\n",
    "        X: Training features\n",
    "        y: Training labels\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        \n",
    "        # Create cross-validation object\n",
    "        kf = StratifiedKFold(n_splits=self.n_folds, shuffle=True, random_state=self.random_state)\n",
    "        \n",
    "        # Store meta features\n",
    "        meta_features = {}\n",
    "        \n",
    "        # Perform cross-validation predictions for each base model\n",
    "        for name, model in self.base_models.items():\n",
    "            print(f\"\\nTraining base model: {name}\")\n",
    "            \n",
    "            # Initialize an array to store predictions\n",
    "            if self.use_proba:\n",
    "                n_classes = len(np.unique(y))\n",
    "                train_meta_preds = np.zeros((X.shape[0], n_classes))\n",
    "            else:\n",
    "                train_meta_preds = np.zeros(X.shape[0])\n",
    "            \n",
    "            # Perform K-fold cross-validation\n",
    "            for i, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "                print(f\"  Fold {i+1}/{self.n_folds}\")\n",
    "                \n",
    "                if isinstance(X, pd.DataFrame):\n",
    "                    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "                else:\n",
    "                    X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "\n",
    "                if isinstance(y, pd.Series):\n",
    "                    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                else:\n",
    "                    y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "                \n",
    "                # Train the model\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Generate predictions\n",
    "    \n",
    "                if self.use_proba:\n",
    "                    fold_preds = model.predict_proba(X_val_fold)\n",
    "    \n",
    "                    for i, idx in enumerate(val_idx):\n",
    "                        train_meta_preds[idx] = fold_preds[i]\n",
    "                else:\n",
    "                    fold_preds = model.predict(X_val_fold)\n",
    "    \n",
    "                    for i, idx in enumerate(val_idx):\n",
    "                        train_meta_preds[idx] = fold_preds[i]\n",
    "            \n",
    "            meta_features[name] = train_meta_preds\n",
    "            \n",
    "            # Retrain the model on the entire dataset\n",
    "            print(f\"  Training {name} on the entire data\")\n",
    "            model.fit(X, y)\n",
    "        \n",
    "        # Prepare training data for meta model\n",
    "        meta_X = self._prepare_meta_features(meta_features)\n",
    "        \n",
    "        # Train the meta model\n",
    "        print(\"\\nTraining meta model...\")\n",
    "        self.meta_model.fit(meta_X, y)\n",
    "        \n",
    "        # Save base models' predictions\n",
    "        self.base_model_preds = meta_features\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained stacking model to predict\n",
    "        \n",
    "        Parameters:\n",
    "        X: Test features\n",
    "        \n",
    "        Returns:\n",
    "        Predicted labels\n",
    "        \"\"\"\n",
    "        meta_features = {}\n",
    "        \n",
    "        # Generate predictions using each base model\n",
    "        for name, model in self.base_models.items():\n",
    "            if self.use_proba:\n",
    "                preds = model.predict_proba(X)\n",
    "            else:\n",
    "                preds = model.predict(X)\n",
    "            meta_features[name] = preds\n",
    "        \n",
    "        # Prepare meta features for meta model\n",
    "        meta_X = self._prepare_meta_features(meta_features)\n",
    "        \n",
    "        # Final prediction using meta model\n",
    "        return self.meta_model.predict(meta_X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained stacking model to predict probabilities\n",
    "        \n",
    "        Parameters:\n",
    "        X: Test features\n",
    "        \n",
    "        Returns:\n",
    "        Predicted probabilities\n",
    "        \"\"\"\n",
    "        meta_features = {}\n",
    "        \n",
    "        # Generate predictions using each base model\n",
    "        for name, model in self.base_models.items():\n",
    "            if self.use_proba:\n",
    "                preds = model.predict_proba(X)\n",
    "            else:\n",
    "                preds = model.predict(X)\n",
    "            meta_features[name] = preds\n",
    "        \n",
    "        # Prepare meta features for meta model\n",
    "        meta_X = self._prepare_meta_features(meta_features)\n",
    "        \n",
    "        # Final probability prediction using meta model\n",
    "        return self.meta_model.predict_proba(meta_X)\n",
    "    \n",
    "    def _prepare_meta_features(self, meta_features):\n",
    "        \"\"\"\n",
    "        Prepare meta features\n",
    "        \n",
    "        Parameters:\n",
    "        meta_features: Dictionary of predictions from base models\n",
    "        \n",
    "        Returns:\n",
    "        Combined meta features array\n",
    "        \"\"\"\n",
    "        # Combine predictions from all base models into one array\n",
    "        all_features = []\n",
    "        \n",
    "        for name, preds in meta_features.items():\n",
    "            # Ensure predictions are 2D arrays\n",
    "            if preds.ndim == 1:\n",
    "                preds = preds.reshape(-1, 1)\n",
    "            all_features.append(preds)\n",
    "        \n",
    "        # Concatenate all features horizontally\n",
    "        return np.hstack(all_features)\n",
    "\n",
    "\n",
    "# Custom transformer: Feature transformation (Yeo-Johnson)\n",
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_transform=None, binary_features=None, method='yeo-johnson'):\n",
    "        self.features_to_transform = features_to_transform\n",
    "        self.binary_features = binary_features if binary_features is not None else []\n",
    "        self.method = method\n",
    "        self.lambdas_ = {}  # Store lambda values for each feature\n",
    "        self.transformed_feature_names_ = []  # Store transformed feature names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no features specified, use all numeric features\n",
    "        if self.features_to_transform is None:\n",
    "            self.features_to_transform = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Exclude binary features\n",
    "        features_to_process = list(set(self.features_to_transform) - set(self.binary_features))\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature in tqdm(features_to_process, desc=\"Fitting transformations\"):\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "                \n",
    "            if not np.issubdtype(X[feature].dtype, np.number):\n",
    "                continue\n",
    "            \n",
    "            # For Yeo-Johnson transformation, save lambda values\n",
    "            if self.method == 'yeo-johnson':\n",
    "                try:\n",
    "                    # Fit transformer and save lambda value\n",
    "                    _, lmbda = stats.yeojohnson(X[feature])\n",
    "                    self.lambdas_[feature] = lmbda\n",
    "                    self.transformed_feature_names_.append(f\"{feature}_yeojohnson\")\n",
    "                    print(f\"Fitted yeo-johnson transformation for {feature} with lambda={lmbda:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error fitting {self.method} transformation to {feature}: {str(e)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature, lmbda in tqdm(self.lambdas_.items(), desc=\"Applying transformations\"):\n",
    "            if feature in X.columns:\n",
    "                try:\n",
    "                    # Apply transformation using saved lambda value\n",
    "                    X[f\"{feature}_yeojohnson\"] = stats.yeojohnson(X[feature], lmbda=lmbda)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error applying {self.method} transformation to {feature}: {str(e)}\")\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        # Return transformed feature names\n",
    "        return self.transformed_feature_names_\n",
    "\n",
    "\n",
    "# Custom transformer: Feature standardization\n",
    "class FeatureStandardizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features_to_standardize=None):\n",
    "        self.features_to_standardize = features_to_standardize\n",
    "        self.scalers_ = {}  # Store a separate scaler for each feature\n",
    "        self.standardized_feature_names_ = []  # Store standardized feature names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If no features specified, use all numeric features\n",
    "        if self.features_to_standardize is None:\n",
    "            self.features_to_standardize = X.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature in tqdm(self.features_to_standardize, desc=\"Fitting standardization\"):\n",
    "            if feature in X.columns and np.issubdtype(X[feature].dtype, np.number):\n",
    "                # Create a separate scaler for each feature\n",
    "                scaler = StandardScaler()\n",
    "                scaler.fit(X[[feature]])\n",
    "                self.scalers_[feature] = scaler\n",
    "                self.standardized_feature_names_.append(f\"{feature}_std\")\n",
    "                print(f\"Fitted standardization for {feature} with mean={scaler.mean_[0]:.4f}, scale={scaler.scale_[0]:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X.copy())\n",
    "        \n",
    "        # Add progress bar\n",
    "        for feature, scaler in tqdm(self.scalers_.items(), desc=\"Applying standardization\"):\n",
    "            if feature in X.columns:\n",
    "                # Apply standardization using saved scaler\n",
    "                X[f\"{feature}_std\"] = scaler.transform(X[[feature]])\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        # Return standardized feature names\n",
    "        return self.standardized_feature_names_\n",
    "\n",
    "\n",
    "# Feature selector: Select specified feature columns\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names=None):\n",
    "        self.feature_names = feature_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # Select specified features\n",
    "        if self.feature_names:\n",
    "            # Check which features exist in the data\n",
    "            available_features = [f for f in self.feature_names if f in X.columns]\n",
    "            if len(available_features) < len(self.feature_names):\n",
    "                missing = set(self.feature_names) - set(available_features)\n",
    "                print(f\"Warning: Some features are missing: {missing}\")\n",
    "            \n",
    "            return X[available_features]\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with confusion matrix and metrics\n",
    "    \n",
    "    Parameters:\n",
    "    y_true: True labels\n",
    "    y_pred: Predicted labels\n",
    "    dataset_name: Name of the dataset for display purposes\n",
    "    \"\"\"\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nModel Evaluation on {dataset_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    \n",
    "    # Generate classification report\n",
    "    try:\n",
    "        report = classification_report(y_true, y_pred, labels=classes, output_dict=False)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating classification report: {str(e)}\")\n",
    "    \n",
    "    # Attempt to plot confusion matrix\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=classes, \n",
    "                    yticklabels=classes)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{dataset_name.replace(\" \", \"_\")}.png')\n",
    "        print(f\"Confusion matrix saved to confusion_matrix_{dataset_name.replace(' ', '_')}.png\")\n",
    "        plt.close()  # Close the plot to avoid display issues\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting confusion matrix: {str(e)}\")\n",
    "        # Print confusion matrix as fallback\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "    \n",
    "    return accuracy, cm, report\n",
    "\n",
    "\n",
    "class PCATransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=None, variance_threshold=0.95):\n",
    "        self.n_components = n_components\n",
    "        self.variance_threshold = variance_threshold\n",
    "        self.pca_ = None\n",
    "        self.feature_names_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # If n_components is None, use variance threshold\n",
    "        if self.n_components is None:\n",
    "            # First, fit a full PCA\n",
    "            temp_pca = PCA()\n",
    "            temp_pca.fit(X)\n",
    "            \n",
    "            # Determine how many components are needed to reach the variance threshold\n",
    "            cumulative_variance = np.cumsum(temp_pca.explained_variance_ratio_)\n",
    "            self.n_components = np.argmax(cumulative_variance >= self.variance_threshold) + 1\n",
    "            print(f\"Selected {self.n_components} components to explain {self.variance_threshold*100:.1f}% of variance\")\n",
    "        \n",
    "        # Create and fit PCA\n",
    "        self.pca_ = PCA(n_components=self.n_components)\n",
    "        self.pca_.fit(X)\n",
    "        \n",
    "        # Create feature names\n",
    "        self.feature_names_ = [f\"PC{i+1}\" for i in range(self.n_components)]\n",
    "        \n",
    "        # Print explained variance ratios\n",
    "        explained_variance = self.pca_.explained_variance_ratio_\n",
    "        print(f\"Top 5 components explain: {explained_variance[:5].sum()*100:.2f}% of variance\")\n",
    "        print(f\"All {self.n_components} components explain: {explained_variance.sum()*100:.2f}% of variance\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame\n",
    "        X = pd.DataFrame(X)\n",
    "        \n",
    "        # Apply PCA transformation\n",
    "        X_pca = self.pca_.transform(X)\n",
    "        \n",
    "        # Convert to DataFrame and add column names\n",
    "        X_pca_df = pd.DataFrame(X_pca, columns=self.feature_names_, index=X.index)\n",
    "        \n",
    "        return X_pca_df\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.feature_names_\n",
    "\n",
    "\n",
    "class TrainingVisualizationCallback(Callback):\n",
    "    def __init__(self, validation_data=None):\n",
    "        super(TrainingVisualizationCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.train_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_loss = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.train_acc.append(logs.get('accuracy'))\n",
    "        \n",
    "        if self.validation_data:\n",
    "            val_logs = self.model.evaluate(\n",
    "                self.validation_data[0], self.validation_data[1], \n",
    "                verbose=0\n",
    "            )\n",
    "            self.val_loss.append(val_logs[0])\n",
    "            self.val_acc.append(val_logs[1])\n",
    "    \n",
    "    def plot_metrics(self):\n",
    "        epochs = range(1, len(self.train_loss) + 1)\n",
    "        \n",
    "        # Create a 2x1 subplot layout\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot loss curves\n",
    "        ax1.plot(epochs, self.train_loss, 'r-', label='Training')\n",
    "        if self.validation_data:\n",
    "            ax1.plot(epochs, self.val_loss, 'b-', label='Validation')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot accuracy curves\n",
    "        ax2.plot(epochs, self.train_acc, 'r-', label='Training')\n",
    "        if self.validation_data:\n",
    "            ax2.plot(epochs, self.val_acc, 'b-', label='Validation')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_metrics.png')\n",
    "        plt.close()\n",
    "        print(\"Training metrics plot saved to training_metrics.png\")\n",
    "        \n",
    "        # Calculate error rate and plot\n",
    "        train_error = [1 - acc for acc in self.train_acc]\n",
    "        if self.validation_data:\n",
    "            val_error = [1 - acc for acc in self.val_acc]\n",
    "            \n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.plot(epochs, train_error, 'r-', label='Training')\n",
    "            plt.plot(epochs, val_error, 'b-', label='Validation')\n",
    "            plt.title('Model Error Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Error Rate')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('error_rate.png')\n",
    "            plt.close()\n",
    "            print(\"Error rate plot saved to error_rate.png\")\n",
    "\n",
    "class NeuralNetworkClassifier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, input_dim=200, epochs=15, batch_size=64, learning_rate=0.001):\n",
    "        self.input_dim = input_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.history = None\n",
    "        self.visualization_callback = None\n",
    "        \n",
    "    def build_model(self, input_shape, n_classes):\n",
    "        model = Sequential([\n",
    "            # First layer\n",
    "            Dense(128, activation='relu', input_shape=(input_shape,),\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Second layer\n",
    "            Dense(64, activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Third layer\n",
    "            Dense(32, activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            # Output layer\n",
    "            Dense(n_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.learning_rate),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        n_classes = len(np.unique(y_encoded))\n",
    "        \n",
    "        # Convert to one-hot encoding\n",
    "        y_onehot = to_categorical(y_encoded)\n",
    "        \n",
    "        # Prepare validation data\n",
    "        validation_data = None\n",
    "        if X_val is not None and y_val is not None:\n",
    "            if isinstance(X_val, pd.DataFrame):\n",
    "                X_val = X_val.values\n",
    "            y_val_encoded = self.label_encoder.transform(y_val)\n",
    "            y_val_onehot = to_categorical(y_val_encoded)\n",
    "            validation_data = (X_val, y_val_onehot)\n",
    "        \n",
    "        # Compute class weights\n",
    "        y_integers = np.argmax(y_onehot, axis=1)\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_integers), y=y_integers)\n",
    "        class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(X.shape[1], n_classes)\n",
    "        \n",
    "        # Create visualization callback\n",
    "        self.visualization_callback = TrainingVisualizationCallback(validation_data)\n",
    "        \n",
    "        # Add callbacks\n",
    "        callbacks = [\n",
    "            # Early stopping\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            # Learning rate scheduler\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-6\n",
    "            ),\n",
    "            # Visualization callback\n",
    "            self.visualization_callback\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            X, y_onehot,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,\n",
    "            validation_data=validation_data\n",
    "        )\n",
    "        \n",
    "        # Plot training metrics\n",
    "        self.visualization_callback.plot_metrics()\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history curves\"\"\"\n",
    "        history = self.history\n",
    "        \n",
    "        # Create a 2x1 subplot layout\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot loss curve\n",
    "        ax1.plot(history.history['loss'], 'r-', label='Training Loss')\n",
    "        if 'val_loss' in history.history:\n",
    "            ax1.plot(history.history['val_loss'], 'b-', label='Validation Loss')\n",
    "        ax1.set_title('Model Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot accuracy curve\n",
    "        ax2.plot(history.history['accuracy'], 'r-', label='Training Accuracy')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            ax2.plot(history.history['val_accuracy'], 'b-', label='Validation Accuracy')\n",
    "        ax2.set_title('Model Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('nn_training_history.png')\n",
    "        plt.close()\n",
    "        print(\"Neural network training history saved to nn_training_history.png\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Predict probabilities\n",
    "        y_pred_probs = self.model.predict(X, verbose=0)\n",
    "        \n",
    "        # Adjust the threshold for class 1 to improve recall\n",
    "        adjusted_probs = y_pred_probs.copy()\n",
    "        adjusted_probs[:, 1] = adjusted_probs[:, 1] * 1.15  # Increase probability for class 1\n",
    "        \n",
    "        # Convert to class labels\n",
    "        y_pred = np.argmax(adjusted_probs, axis=1)\n",
    "        \n",
    "        # Convert back to original labels\n",
    "        return self.label_encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Ensure X is a numpy array\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Predict probabilities\n",
    "        return self.model.predict(X, verbose=0)        \n",
    "\n",
    "\n",
    "def train_advanced_models(train_df, target_column, skewed_features=None, binary_features=None, \n",
    "                          high_kurtosis_features=None, use_pca=False, use_smote=True,\n",
    "                          n_pca_components=None, pca_variance=0.95, use_bayes_opt=False,\n",
    "                          use_stacking=False):\n",
    "    \"\"\"\n",
    "    Train advanced models and evaluate performance\n",
    "    \n",
    "    Parameters:\n",
    "    train_df: Training data DataFrame\n",
    "    target_column: Target variable column name\n",
    "    skewed_features: List of skewed features\n",
    "    binary_features: List of binary features\n",
    "    high_kurtosis_features: List of high kurtosis features\n",
    "    use_pca: Whether to use PCA\n",
    "    use_smote: Whether to use SMOTE oversampling\n",
    "    n_pca_components: Number of PCA components\n",
    "    pca_variance: PCA variance threshold\n",
    "    use_bayes_opt: Whether to use Bayesian Optimization for XGBoost parameters\n",
    "    use_stacking: Whether to use Stacking ensemble method\n",
    "    \n",
    "    Returns:\n",
    "    Best model, results dictionary, preprocessor\n",
    "    \"\"\"\n",
    "    train_df = train_df.copy()\n",
    "    train_df.columns = train_df.columns.astype(str)\n",
    "    \n",
    "    # Convert features to string if needed\n",
    "    if skewed_features is not None:\n",
    "        skewed_features = [str(f) for f in skewed_features]\n",
    "    \n",
    "    if binary_features is not None:\n",
    "        binary_features = [str(f) for f in binary_features]\n",
    "    \n",
    "    if high_kurtosis_features is not None:\n",
    "        high_kurtosis_features = [str(f) for f in high_kurtosis_features]\n",
    "    \n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ADVANCED MODEL TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Target column: {target_column}\")\n",
    "    print(f\"Using PCA: {use_pca}\")\n",
    "    print(f\"Using SMOTE: {use_smote}\")\n",
    "    print(f\"Using Bayes Optimization: {use_bayes_opt}\")\n",
    "    print(f\"Using Stacking: {use_stacking}\")\n",
    "    \n",
    "    \n",
    "    X = train_df.drop(columns=[target_column])\n",
    "    y = train_df[target_column]\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    print(f\"Class distribution in training set: {np.bincount(y_train)}\")\n",
    "    print(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
    "    \n",
    "    \n",
    "    if skewed_features is not None:\n",
    "        skewed_in_data = [f for f in skewed_features if f in X.columns]\n",
    "        features_to_transform = list(set(skewed_in_data) - set(binary_features or []))\n",
    "    else:\n",
    "        features_to_transform = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    # Features to standardize\n",
    "    if high_kurtosis_features is not None:\n",
    "        kurt_in_data = [f for f in high_kurtosis_features if f in X.columns]\n",
    "        features_to_standardize = kurt_in_data.copy()\n",
    "    else:\n",
    "        features_to_standardize = []\n",
    "    \n",
    "    # Create preprocessing steps\n",
    "    preprocessing_steps = [\n",
    "        ('transformer', FeatureTransformer(features_to_transform=features_to_transform, \n",
    "                                             binary_features=binary_features)),\n",
    "        ('standardizer', FeatureStandardizer(features_to_standardize=None))  # Standardizes all features\n",
    "    ]\n",
    "    \n",
    "    if use_pca:\n",
    "        preprocessing_steps.append(\n",
    "            ('pca', PCATransformer(n_components=n_pca_components, variance_threshold=pca_variance))\n",
    "        )\n",
    "    \n",
    "    # Create preprocessor pipeline\n",
    "    preprocessing = Pipeline(preprocessing_steps)\n",
    "    \n",
    "    # Create preprocessing pipeline for neural network (first standardize then normalize)\n",
    "    nn_preprocessing_steps = [\n",
    "        ('transformer', FeatureTransformer(features_to_transform=features_to_transform, \n",
    "                                             binary_features=binary_features)),\n",
    "        ('standardizer', FeatureStandardizer(features_to_standardize=None)),  # Standardize first\n",
    "        ('normalizer', FeatureNormalizer())  # Then normalize\n",
    "    ]\n",
    "    \n",
    "    if use_pca:\n",
    "        nn_preprocessing_steps.append(\n",
    "            ('pca', PCATransformer(n_components=n_pca_components, variance_threshold=pca_variance))\n",
    "        )\n",
    "    \n",
    "    nn_preprocessing = Pipeline(nn_preprocessing_steps)\n",
    "    \n",
    "    # Fit preprocessor\n",
    "    print(\"\\nFitting regular preprocessor pipeline...\")\n",
    "    X_train_processed = preprocessing.fit_transform(X_train)\n",
    "    X_test_processed = preprocessing.transform(X_test)\n",
    "    \n",
    "    print(\"\\nFitting neural network preprocessor pipeline...\")\n",
    "    X_train_nn_processed = nn_preprocessing.fit_transform(X_train)\n",
    "    X_test_nn_processed = nn_preprocessing.transform(X_test)\n",
    "    \n",
    "    print(f\"Shape of training data after regular processing: {X_train_processed.shape}\")\n",
    "    print(f\"Shape of test data after regular processing: {X_test_processed.shape}\")\n",
    "    print(f\"Shape of training data after neural network processing: {X_train_nn_processed.shape}\")\n",
    "    print(f\"Shape of test data after neural network processing: {X_test_nn_processed.shape}\")\n",
    "    \n",
    "    # Apply SMOTE oversampling if enabled\n",
    "    if use_smote:\n",
    "        print(\"\\nApplying SMOTE oversampling...\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "        X_train_nn_resampled, y_train_nn_resampled = smote.fit_resample(X_train_nn_processed, y_train)\n",
    "        print(f\"Shape of training data after resampling: {X_train_resampled.shape}\")\n",
    "        print(f\"Class distribution after resampling: {np.bincount(y_train_resampled)}\")\n",
    "    else:\n",
    "        X_train_resampled, y_train_resampled = X_train_processed, y_train\n",
    "        X_train_nn_resampled, y_train_nn_resampled = X_train_nn_processed, y_train\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    print(f\"Class weights: {weight_dict}\")\n",
    "    \n",
    "    # Prepare class weights for neural network\n",
    "    nn_class_weight = {i: weight for i, weight in enumerate(class_weights)}\n",
    "    \n",
    "    # If using Bayesian Optimization, optimize XGBoost parameters\n",
    "    if use_bayes_opt:\n",
    "        print(\"\\nUsing Bayesian Optimization to tune XGBoost parameters...\")\n",
    "        # Split training set into training and validation sets\n",
    "        X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n",
    "            X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled\n",
    "        )\n",
    "        \n",
    "        # Perform Bayesian Optimization\n",
    "        best_params, _ = optimize_xgboost_params(\n",
    "            X_train_opt, y_train_opt, X_val_opt, y_val_opt, n_iter=30\n",
    "        )\n",
    "        \n",
    "        # Create XGBoost model using best parameters\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            **best_params,\n",
    "            objective='multi:softprob',\n",
    "            num_class=len(np.unique(y)),\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=['mlogloss', 'merror']\n",
    "        )\n",
    "    else:\n",
    "        # Use default parameters\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.01,\n",
    "            gamma=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            colsample_bylevel=0.8,\n",
    "            objective='multi:softprob',\n",
    "            num_class=len(np.unique(y)),\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            reg_alpha=0.2,\n",
    "            reg_lambda=1.5,\n",
    "            # Note: scale_pos_weight should be a scalar value. For multiclass problems, you may omit this parameter or use class weights.\n",
    "            tree_method='hist',\n",
    "            eval_metric=['mlogloss', 'merror']\n",
    "        )\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        \"XGBoost\": xgb_model,\n",
    "        \n",
    "        \"LightGBM\": lgb.LGBMClassifier(\n",
    "            n_estimators=200, max_depth=8, learning_rate=0.05, num_leaves=31,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1,\n",
    "            objective='multiclass', num_class=len(np.unique(y)), \n",
    "            class_weight='balanced',\n",
    "            random_state=42, n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        \"NeuralNetwork\": NeuralNetworkClassifier(\n",
    "            input_dim=X_train_nn_processed.shape[1],\n",
    "            learning_rate=0.001,\n",
    "            batch_size=128,\n",
    "            epochs=30\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Compare results\n",
    "    results = {}\n",
    "    \n",
    "    # If using Stacking, create Stacking model\n",
    "    if use_stacking:\n",
    "        print(\"\\nPreparing Stacking model...\")\n",
    "        \n",
    "        # Prepare base models\n",
    "        base_models = {\n",
    "            \"XGBoost\": models[\"XGBoost\"],\n",
    "            \"LightGBM\": models[\"LightGBM\"]\n",
    "        }\n",
    "        \n",
    "        # Create meta model (using Logistic Regression)\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        meta_model = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            C=1.0,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create Stacking Classifier\n",
    "        stacking_model = StackingClassifier(\n",
    "            base_models=base_models,\n",
    "            meta_model=meta_model,\n",
    "            n_folds=5,\n",
    "            use_proba=True,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Add to models dictionary\n",
    "        models[\"Stacking\"] = stacking_model\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # For XGBoost, add early stopping and evaluation set\n",
    "        if name == \"XGBoost\":\n",
    "            eval_set = [(X_train_processed, y_train_resampled), (X_test_processed, y_test)]\n",
    "            model.fit(\n",
    "                X_train_resampled, y_train_resampled,\n",
    "                eval_set=eval_set,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Plot XGBoost training process\n",
    "            results_df = pd.DataFrame({\n",
    "                'train_mlogloss': model.evals_result()['validation_0']['mlogloss'],\n",
    "                'test_mlogloss': model.evals_result()['validation_1']['mlogloss'],\n",
    "                'train_merror': model.evals_result()['validation_0']['merror'],\n",
    "                'test_merror': model.evals_result()['validation_1']['merror']\n",
    "            })\n",
    "            \n",
    "            # Plot loss curves\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(results_df['train_mlogloss'], 'r-', label='Training')\n",
    "            plt.plot(results_df['test_mlogloss'], 'b-', label='Test')\n",
    "            plt.title('XGBoost Log Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Log Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            # Plot error rate curves\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(results_df['train_merror'], 'r-', label='Training')\n",
    "            plt.plot(results_df['test_merror'], 'b-', label='Test')\n",
    "            plt.title('XGBoost Error Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Error Rate')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('xgboost_training_metrics.png')\n",
    "            plt.close()\n",
    "            print(\"XGBoost training metrics saved to xgboost_training_metrics.png\")\n",
    "            \n",
    "            # Plot feature importance\n",
    "            if not use_pca:  # Only plot feature importance if PCA is not used\n",
    "                feature_names = X_train_processed.columns.tolist()\n",
    "                importance_df = plot_feature_importance(model, feature_names, top_n=20, model_name=\"XGBoost\")\n",
    "                print(\"Top 20 XGBoost important features:\")\n",
    "                print(importance_df.head(20))\n",
    "            \n",
    "        elif name == \"LightGBM\":\n",
    "            model.fit(X_train_resampled, y_train_resampled)\n",
    "            \n",
    "            # Plot feature importance\n",
    "            if not use_pca:  # Only plot feature importance if PCA is not used\n",
    "                feature_names = X_train_processed.columns.tolist()\n",
    "                importance_df = plot_feature_importance(model, feature_names, top_n=20, model_name=\"LightGBM\")\n",
    "                print(\"Top 20 LightGBM important features:\")\n",
    "                print(importance_df.head(20))\n",
    "                \n",
    "        elif name == \"NeuralNetwork\":\n",
    "            # Train neural network using specially preprocessed data\n",
    "            model.fit(X_train_nn_resampled, y_train_nn_resampled, X_test_nn_processed, y_test)\n",
    "        \n",
    "        elif name == \"Stacking\":\n",
    "            # Convert data to numpy arrays\n",
    "            if isinstance(X_train_resampled, pd.DataFrame):\n",
    "                X_train_resampled = X_train_resampled.reset_index(drop=True)\n",
    "            if isinstance(y_train_resampled, pd.Series):\n",
    "                y_train_resampled = y_train_resampled.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "            X_train_np = X_train_resampled.values if isinstance(X_train_resampled, pd.DataFrame) else X_train_resampled\n",
    "            X_test_np = X_test_processed.values if isinstance(X_test_processed, pd.DataFrame) else X_test_processed\n",
    "    \n",
    "    \n",
    "            model.fit(X_train_np, y_train_resampled)\n",
    "        \n",
    "        # Evaluate on training set - using corresponding preprocessed data\n",
    "        if name == \"NeuralNetwork\":\n",
    "            train_preds = model.predict(X_train_nn_processed)\n",
    "            test_preds = model.predict(X_test_nn_processed)\n",
    "        else:\n",
    "            train_preds = model.predict(X_train_processed)\n",
    "            test_preds = model.predict(X_test_processed)\n",
    "            \n",
    "        train_acc = accuracy_score(y_train, train_preds)\n",
    "        train_balanced_acc = balanced_accuracy_score(y_train, train_preds)\n",
    "        train_f1_macro = f1_score(y_train, train_preds, average='macro')\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_acc = accuracy_score(y_test, test_preds)\n",
    "        test_balanced_acc = balanced_accuracy_score(y_test, test_preds)\n",
    "        test_f1_macro = f1_score(y_test, test_preds, average='macro')\n",
    "        \n",
    "        # Save results\n",
    "        results[name] = {\n",
    "            'train_acc': train_acc,\n",
    "            'train_balanced_acc': train_balanced_acc,\n",
    "            'train_f1_macro': train_f1_macro,\n",
    "            'test_acc': test_acc,\n",
    "            'test_balanced_acc': test_balanced_acc,\n",
    "            'test_f1_macro': test_f1_macro,\n",
    "            'model': model,\n",
    "            'test_report': classification_report(y_test, test_preds)\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"  Training Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"  Training Balanced Accuracy: {train_balanced_acc:.4f}\")\n",
    "        print(f\"  Training F1 (macro): {train_f1_macro:.4f}\")\n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"  Test Balanced Accuracy: {test_balanced_acc:.4f}\")\n",
    "        print(f\"  Test F1 (macro): {test_f1_macro:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results[name]['test_report'])\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_test, test_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=np.unique(y_test), \n",
    "                    yticklabels=np.unique(y_test))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'confusion_matrix_{name}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model\n",
    "        if name == \"NeuralNetwork\":\n",
    "            full_model = {\n",
    "                'preprocessing': nn_preprocessing,\n",
    "                'model': model\n",
    "            }\n",
    "        else:\n",
    "            full_model = {\n",
    "                'preprocessing': preprocessing,\n",
    "                'model': model\n",
    "            }\n",
    "        joblib.dump(full_model, f'{name.lower()}_model.joblib')\n",
    "        print(f\"Model saved to {name.lower()}_model.joblib\")\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['test_f1_macro'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    print(f\"\\nBest model: {best_model_name} with Test F1 (macro): {results[best_model_name]['test_f1_macro']:.4f}\")\n",
    "    \n",
    "    # Return the appropriate preprocessor\n",
    "    if best_model_name == \"NeuralNetwork\":\n",
    "        return best_model, results, nn_preprocessing\n",
    "    elif best_model_name == \"Stacking\":\n",
    "        return best_model, results, preprocessing\n",
    "    else:\n",
    "        return best_model, results, preprocessing\n",
    "\n",
    "\n",
    "def optimize_xgboost_params(X_train, y_train, X_val, y_val, n_iter=10):\n",
    "    \"\"\"\n",
    "    Find the best XGBoost parameters using Bayesian Optimization\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training features\n",
    "    y_train: Training labels\n",
    "    X_val: Validation features\n",
    "    y_val: Validation labels\n",
    "    n_iter: Number of optimization iterations\n",
    "    \n",
    "    Returns:\n",
    "    best_params: Best parameters dictionary\n",
    "    best_score: Best score\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    \n",
    "    # Use a smaller sample for optimization if necessary\n",
    "    if X_train.shape[0] > 10000:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_sample, _, y_sample, _ = train_test_split(\n",
    "            X_train, y_train, train_size=10000, random_state=42, stratify=y_train\n",
    "        )\n",
    "    else:\n",
    "        X_sample, y_sample = X_train, y_train\n",
    "    \n",
    "    # Define a narrower parameter space\n",
    "    param_space = {\n",
    "        'n_estimators': Integer(300, 600),  # Reduced range\n",
    "        'max_depth': Integer(3, 8),         # Reduced range\n",
    "        'learning_rate': Real(0.005, 0.2),     # Reduced range\n",
    "        'subsample': Real(0.6, 0.9),\n",
    "        'colsample_bytree': Real(0.6, 0.9),\n",
    "        'gamma': Real(0, 2),                # Reduced range\n",
    "        'reg_alpha': Real(0, 2),            # Reduced range\n",
    "        'reg_lambda': Real(1, 5)            # Reduced range\n",
    "    }\n",
    "    \n",
    "    # Create XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        tree_method='hist',  # Use histogram method for speed\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create Bayesian search object\n",
    "    bayes_search = BayesSearchCV(\n",
    "        model,\n",
    "        param_space,\n",
    "        n_iter=n_iter,\n",
    "        cv=3,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Start Bayesian Optimization\n",
    "    print(\"Starting Bayesian Optimization...\")\n",
    "    bayes_search.fit(X_sample, y_sample)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "    \n",
    "    print(f\"Best F1 score: {best_score:.4f}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Evaluate on validation set using best model\n",
    "    best_model = bayes_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    print(f\"Validation F1 score: {val_f1:.4f}\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=20, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot feature importance\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    feature_names: List of feature names\n",
    "    top_n: Show top N important features\n",
    "    model_name: Model name\n",
    "    \n",
    "    Returns:\n",
    "    importance_df: DataFrame of feature importances\n",
    "    \"\"\"\n",
    "    # Get feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif model_name == \"LightGBM\":\n",
    "        importances = model.booster_.feature_importance(importance_type='gain')\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not support feature importance extraction\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df.head(top_n))\n",
    "    plt.title(f'{model_name} Feature Importance (Top {top_n})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower()}_feature_importance.png')\n",
    "    plt.close()\n",
    "    print(f\"{model_name} feature importance plot saved to {model_name.lower()}_feature_importance.png\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def apply_advanced_model(new_df, target_column=None, model_path=None):\n",
    "    \"\"\"\n",
    "    Apply the saved advanced model to new data\n",
    "    \n",
    "    Parameters:\n",
    "    new_df: New data DataFrame\n",
    "    target_column: Target variable column name (if any)\n",
    "    model_path: Model path\n",
    "    \n",
    "    Returns:\n",
    "    Predictions and probabilities\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"APPLYING ADVANCED MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"New data shape: {new_df.shape}\")\n",
    "    \n",
    "    # Convert all column names to strings\n",
    "    new_df = new_df.copy()\n",
    "    new_df.columns = new_df.columns.astype(str)\n",
    "    \n",
    "    # Prepare data\n",
    "    if target_column and target_column in new_df.columns:\n",
    "        X_new = new_df.drop(columns=[target_column])\n",
    "        y_true = new_df[target_column]\n",
    "        has_target = True\n",
    "        print(f\"Target column '{target_column}' found in data.\")\n",
    "    else:\n",
    "        X_new = new_df\n",
    "        has_target = False\n",
    "        print(\"No target column provided or not found in data.\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    full_model = joblib.load(model_path)\n",
    "    preprocessing = full_model['preprocessing']\n",
    "    model = full_model['model']\n",
    "    print(f\"Model loaded: {type(model).__name__}\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    print(\"Applying preprocessing...\")\n",
    "    X_new_processed = preprocessing.transform(X_new)\n",
    "    print(f\"Processed data shape: {X_new_processed.shape}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = model.predict(X_new_processed)\n",
    "    \n",
    "    # Get probabilities (if supported)\n",
    "    try:\n",
    "        probabilities = model.predict_proba(X_new_processed)\n",
    "        has_probabilities = True\n",
    "    except:\n",
    "        probabilities = None\n",
    "        has_probabilities = False\n",
    "    \n",
    "    print(\"Prediction complete\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    if has_probabilities:\n",
    "        print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "    \n",
    "    # If target is provided, evaluate predictions\n",
    "    if has_target:\n",
    "        print(\"\\nEVALUATING MODEL ON NEW DATA:\")\n",
    "        acc = accuracy_score(y_true, predictions)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, predictions)\n",
    "        f1_macro = f1_score(y_true, predictions, average='macro')\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "        print(f\"F1 (macro): {f1_macro:.4f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, predictions))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=np.unique(y_true), \n",
    "                    yticklabels=np.unique(y_true))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix - New Data')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix_new_data.png')\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    return predictions, probabilities\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def optimize_xgboost_params(X_train, y_train, X_val, y_val, n_iter=10):\n",
    "    \"\"\"\n",
    "    Find the best XGBoost parameters using Bayesian Optimization\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training features\n",
    "    y_train: Training labels\n",
    "    X_val: Validation features\n",
    "    y_val: Validation labels\n",
    "    n_iter: Number of optimization iterations\n",
    "    \n",
    "    Returns:\n",
    "    best_params: Best parameters dictionary\n",
    "    best_score: Best score\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import f1_score\n",
    "    from skopt import BayesSearchCV\n",
    "    from skopt.space import Real, Integer, Categorical\n",
    "    \n",
    "    # Use a smaller sample for optimization if necessary\n",
    "    if X_train.shape[0] > 10000:\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_sample, _, y_sample, _ = train_test_split(\n",
    "            X_train, y_train, train_size=10000, random_state=42, stratify=y_train\n",
    "        )\n",
    "    else:\n",
    "        X_sample, y_sample = X_train, y_train\n",
    "    \n",
    "    # Define a narrower parameter space\n",
    "    param_space = {\n",
    "        'n_estimators': Integer(300, 800),  # Reduced range\n",
    "        'max_depth': Integer(3, 10),         # Reduced range\n",
    "        'learning_rate': Real(0.001, 0.2),     # Reduced range\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Real(0.5, 1.0),\n",
    "        'gamma': Real(0, 4),                # Reduced range\n",
    "        'reg_alpha': Real(0, 4),            # Reduced range\n",
    "        'reg_lambda': Real(0.5, 10),            # Reduced range\n",
    "        'min_child_weight': Integer(1, 10)\n",
    "    }\n",
    "    \n",
    "    # Create XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(np.unique(y_train)),\n",
    "        tree_method='hist',  # Use histogram method for speed\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create Bayesian search object\n",
    "    bayes_search = BayesSearchCV(\n",
    "        model,\n",
    "        param_space,\n",
    "        n_iter=n_iter,\n",
    "        cv=3,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Start Bayesian Optimization\n",
    "    print(\"Starting Bayesian Optimization...\")\n",
    "    bayes_search.fit(X_sample, y_sample)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = bayes_search.best_params_\n",
    "    best_score = bayes_search.best_score_\n",
    "    \n",
    "    print(f\"Best F1 score: {best_score:.4f}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Evaluate on validation set using best model\n",
    "    best_model = bayes_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "    print(f\"Validation F1 score: {val_f1:.4f}\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=20, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot feature importance\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    feature_names: List of feature names\n",
    "    top_n: Show top N important features\n",
    "    model_name: Model name\n",
    "    \n",
    "    Returns:\n",
    "    importance_df: DataFrame of feature importances\n",
    "    \"\"\"\n",
    "    # Get feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    elif model_name == \"LightGBM\":\n",
    "        importances = model.booster_.feature_importance(importance_type='gain')\n",
    "    else:\n",
    "        print(f\"Model {model_name} does not support feature importance extraction\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df.head(top_n))\n",
    "    plt.title(f'{model_name} Feature Importance (Top {top_n})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name.lower()}_feature_importance.png')\n",
    "    plt.close()\n",
    "    print(f\"{model_name} feature importance plot saved to {model_name.lower()}_feature_importance.png\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def apply_advanced_model(new_df, target_column=None, model_path=None):\n",
    "    \"\"\"\n",
    "    Apply the saved advanced model to new data\n",
    "    \n",
    "    Parameters:\n",
    "    new_df: New data DataFrame\n",
    "    target_column: Target variable column name (if any)\n",
    "    model_path: Model path\n",
    "    \n",
    "    Returns:\n",
    "    Predictions and probabilities\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"APPLYING ADVANCED MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"New data shape: {new_df.shape}\")\n",
    "    \n",
    "    # Convert all column names to strings\n",
    "    new_df = new_df.copy()\n",
    "    new_df.columns = new_df.columns.astype(str)\n",
    "    \n",
    "    # Prepare data\n",
    "    if target_column and target_column in new_df.columns:\n",
    "        X_new = new_df.drop(columns=[target_column])\n",
    "        y_true = new_df[target_column]\n",
    "        has_target = True\n",
    "        print(f\"Target column '{target_column}' found in data.\")\n",
    "    else:\n",
    "        X_new = new_df\n",
    "        has_target = False\n",
    "        print(\"No target column provided or not found in data.\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    full_model = joblib.load(model_path)\n",
    "    preprocessing = full_model['preprocessing']\n",
    "    model = full_model['model']\n",
    "    print(f\"Model loaded: {type(model).__name__}\")\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    print(\"Applying preprocessing...\")\n",
    "    X_new_processed = preprocessing.transform(X_new)\n",
    "    print(f\"Processed data shape: {X_new_processed.shape}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = model.predict(X_new_processed)\n",
    "    \n",
    "    # Get probabilities (if supported)\n",
    "    try:\n",
    "        probabilities = model.predict_proba(X_new_processed)\n",
    "        has_probabilities = True\n",
    "    except:\n",
    "        probabilities = None\n",
    "        has_probabilities = False\n",
    "    \n",
    "    print(\"Prediction complete\")\n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    if has_probabilities:\n",
    "        print(f\"Probabilities shape: {probabilities.shape}\")\n",
    "    \n",
    "    # If target is provided, evaluate predictions\n",
    "    if has_target:\n",
    "        print(\"\\nEVALUATING MODEL ON NEW DATA:\")\n",
    "        acc = accuracy_score(y_true, predictions)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, predictions)\n",
    "        f1_macro = f1_score(y_true, predictions, average='macro')\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "        print(f\"F1 (macro): {f1_macro:.4f}\")\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, predictions))\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, predictions)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=np.unique(y_true), \n",
    "                    yticklabels=np.unique(y_true))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix - New Data')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix_new_data.png')\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    return predictions, probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = '/Users/mouyasushi/Desktop/vici holdings/Test/data/eval_data.npy'\n",
    "test_data = np.load(path3)\n",
    "\n",
    "path4 = '/Users/mouyasushi/Desktop/vici holdings/Test/data/eval_labels.npy'\n",
    "test_labels = np.load(path4)\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df['label'] = test_labels  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ADVANCED MODEL TRAINING\n",
      "==================================================\n",
      "Training data shape: (1174461, 71)\n",
      "Target column: label\n",
      "Using PCA: False\n",
      "Using SMOTE: False\n",
      "Using Bayes Optimization: True\n",
      "Using Stacking: True\n",
      "Training set shape: (939568, 70), Test set shape: (234893, 70)\n",
      "Class distribution in training set: [509635 205557 224376]\n",
      "Class distribution in test set: [127409  51389  56095]\n",
      "\n",
      "Fitting regular preprocessor pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:   3%|▎         | 1/30 [00:00<00:10,  2.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 68 with lambda=-9.5392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:   7%|▋         | 2/30 [00:00<00:08,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 2 with lambda=1.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  10%|█         | 3/30 [00:01<00:14,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 43 with lambda=2.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  13%|█▎        | 4/30 [00:01<00:13,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 5 with lambda=0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  17%|█▋        | 5/30 [00:02<00:10,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 67 with lambda=-13.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  20%|██        | 6/30 [00:02<00:10,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 44 with lambda=2.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  23%|██▎       | 7/30 [00:03<00:10,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 64 with lambda=0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  27%|██▋       | 8/30 [00:03<00:08,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 66 with lambda=-0.9896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  30%|███       | 9/30 [00:03<00:07,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 3 with lambda=0.5652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  33%|███▎      | 10/30 [00:03<00:06,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 53 with lambda=-1.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  37%|███▋      | 11/30 [00:04<00:07,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 49 with lambda=0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  43%|████▎     | 13/30 [00:04<00:05,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 15 with lambda=1.1072\n",
      "Fitted yeo-johnson transformation for 55 with lambda=0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  47%|████▋     | 14/30 [00:05<00:05,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 39 with lambda=2.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  50%|█████     | 15/30 [00:05<00:05,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 34 with lambda=1.7605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  53%|█████▎    | 16/30 [00:06<00:04,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 69 with lambda=-32.4964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  57%|█████▋    | 17/30 [00:06<00:04,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 14 with lambda=1.3551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  60%|██████    | 18/30 [00:06<00:04,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 63 with lambda=-0.3036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  63%|██████▎   | 19/30 [00:07<00:05,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 62 with lambda=0.1496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  67%|██████▋   | 20/30 [00:08<00:04,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 31 with lambda=1.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  70%|███████   | 21/30 [00:08<00:04,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 41 with lambda=2.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  73%|███████▎  | 22/30 [00:09<00:03,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 40 with lambda=2.8365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  77%|███████▋  | 23/30 [00:09<00:03,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 54 with lambda=-0.6452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  80%|████████  | 24/30 [00:10<00:03,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 42 with lambda=2.2412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  83%|████████▎ | 25/30 [00:10<00:02,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 4 with lambda=1.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  87%|████████▋ | 26/30 [00:10<00:01,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 51 with lambda=0.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  90%|█████████ | 27/30 [00:11<00:01,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 52 with lambda=-0.8524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  93%|█████████▎| 28/30 [00:11<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 61 with lambda=0.2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  97%|█████████▋| 29/30 [00:12<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 16 with lambda=1.3351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations: 100%|██████████| 30/30 [00:12<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 65 with lambda=0.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying transformations: 100%|██████████| 30/30 [00:00<00:00, 45.19it/s]\n",
      "Fitting standardization:  44%|████▍     | 44/100 [00:00<00:00, 207.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted standardization for 0 with mean=-0.0000, scale=0.9999\n",
      "Fitted standardization for 1 with mean=-0.0002, scale=0.9999\n",
      "Fitted standardization for 2 with mean=0.0000, scale=1.0012\n",
      "Fitted standardization for 3 with mean=-0.0000, scale=1.0009\n",
      "Fitted standardization for 4 with mean=0.0008, scale=0.9979\n",
      "Fitted standardization for 5 with mean=-0.0011, scale=0.9974\n",
      "Fitted standardization for 6 with mean=-0.0003, scale=1.0000\n",
      "Fitted standardization for 7 with mean=0.0004, scale=0.9998\n",
      "Fitted standardization for 8 with mean=-0.0004, scale=1.0001\n",
      "Fitted standardization for 9 with mean=0.0001, scale=1.0002\n",
      "Fitted standardization for 10 with mean=0.0003, scale=0.9998\n",
      "Fitted standardization for 11 with mean=0.0006, scale=0.9999\n",
      "Fitted standardization for 12 with mean=-0.0002, scale=0.9999\n",
      "Fitted standardization for 13 with mean=-0.0004, scale=1.0001\n",
      "Fitted standardization for 14 with mean=-0.0000, scale=1.0005\n",
      "Fitted standardization for 15 with mean=-0.0003, scale=0.9996\n",
      "Fitted standardization for 16 with mean=-0.0003, scale=1.0000\n",
      "Fitted standardization for 17 with mean=-0.0001, scale=0.9997\n",
      "Fitted standardization for 18 with mean=0.0004, scale=0.9999\n",
      "Fitted standardization for 19 with mean=-0.0000, scale=0.9998\n",
      "Fitted standardization for 20 with mean=0.0003, scale=1.0001\n",
      "Fitted standardization for 21 with mean=-0.0002, scale=0.9997\n",
      "Fitted standardization for 22 with mean=-0.0000, scale=1.0002\n",
      "Fitted standardization for 23 with mean=-0.0001, scale=1.0002\n",
      "Fitted standardization for 24 with mean=0.0004, scale=1.0001\n",
      "Fitted standardization for 25 with mean=-0.0001, scale=1.0000\n",
      "Fitted standardization for 26 with mean=-0.0002, scale=1.0003\n",
      "Fitted standardization for 27 with mean=0.0004, scale=0.9997\n",
      "Fitted standardization for 28 with mean=0.0005, scale=0.9996\n",
      "Fitted standardization for 29 with mean=0.0006, scale=0.9995\n",
      "Fitted standardization for 30 with mean=-0.0003, scale=1.0004\n",
      "Fitted standardization for 31 with mean=-0.0006, scale=1.0007\n",
      "Fitted standardization for 32 with mean=-0.0000, scale=1.0005\n",
      "Fitted standardization for 33 with mean=0.0002, scale=1.0000\n",
      "Fitted standardization for 34 with mean=0.0003, scale=1.0003\n",
      "Fitted standardization for 35 with mean=-0.0002, scale=1.0001\n",
      "Fitted standardization for 36 with mean=-0.0003, scale=0.9997\n",
      "Fitted standardization for 37 with mean=0.0004, scale=0.9993\n",
      "Fitted standardization for 38 with mean=0.0004, scale=0.9994\n",
      "Fitted standardization for 39 with mean=-0.0002, scale=0.9999\n",
      "Fitted standardization for 40 with mean=-0.0000, scale=1.0000\n",
      "Fitted standardization for 41 with mean=-0.0004, scale=1.0003\n",
      "Fitted standardization for 42 with mean=-0.0005, scale=1.0001\n",
      "Fitted standardization for 43 with mean=-0.0003, scale=1.0004\n",
      "Fitted standardization for 44 with mean=-0.0005, scale=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting standardization:  93%|█████████▎| 93/100 [00:00<00:00, 228.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted standardization for 45 with mean=0.0002, scale=1.0002\n",
      "Fitted standardization for 46 with mean=-0.0001, scale=1.0001\n",
      "Fitted standardization for 47 with mean=-0.0002, scale=1.0001\n",
      "Fitted standardization for 48 with mean=0.0006, scale=1.0003\n",
      "Fitted standardization for 49 with mean=0.0003, scale=0.9973\n",
      "Fitted standardization for 50 with mean=-0.0002, scale=0.9997\n",
      "Fitted standardization for 51 with mean=0.0004, scale=1.0003\n",
      "Fitted standardization for 52 with mean=1.3461, scale=8.9012\n",
      "Fitted standardization for 53 with mean=1.1896, scale=1.4834\n",
      "Fitted standardization for 54 with mean=0.9096, scale=0.7265\n",
      "Fitted standardization for 55 with mean=0.0041, scale=0.1430\n",
      "Fitted standardization for 56 with mean=0.1097, scale=0.3125\n",
      "Fitted standardization for 57 with mean=0.1054, scale=0.3071\n",
      "Fitted standardization for 58 with mean=0.6684, scale=0.4708\n",
      "Fitted standardization for 59 with mean=0.3457, scale=0.4756\n",
      "Fitted standardization for 60 with mean=0.3605, scale=0.4801\n",
      "Fitted standardization for 61 with mean=751.4038, scale=31937.9306\n",
      "Fitted standardization for 62 with mean=743.0172, scale=31550.7630\n",
      "Fitted standardization for 63 with mean=82.9839, scale=2667.3007\n",
      "Fitted standardization for 64 with mean=769.0512, scale=32680.2251\n",
      "Fitted standardization for 65 with mean=733.4401, scale=31253.1964\n",
      "Fitted standardization for 66 with mean=2.0782, scale=5.1072\n",
      "Fitted standardization for 67 with mean=0.2671, scale=0.1490\n",
      "Fitted standardization for 68 with mean=0.3682, scale=0.0676\n",
      "Fitted standardization for 69 with mean=0.0412, scale=0.0371\n",
      "Fitted standardization for 68_yeojohnson with mean=0.0991, scale=0.0017\n",
      "Fitted standardization for 2_yeojohnson with mean=0.0637, scale=0.8801\n",
      "Fitted standardization for 43_yeojohnson with mean=0.3076, scale=0.8481\n",
      "Fitted standardization for 5_yeojohnson with mean=-0.0808, scale=0.7992\n",
      "Fitted standardization for 67_yeojohnson with mean=0.0730, scale=0.0012\n",
      "Fitted standardization for 44_yeojohnson with mean=0.3300, scale=0.8212\n",
      "Fitted standardization for 64_yeojohnson with mean=1.2079, scale=2.0324\n",
      "Fitted standardization for 66_yeojohnson with mean=0.4727, scale=0.2376\n",
      "Fitted standardization for 3_yeojohnson with mean=-0.0846, scale=0.8113\n",
      "Fitted standardization for 53_yeojohnson with mean=0.3418, scale=0.0719\n",
      "Fitted standardization for 49_yeojohnson with mean=-0.0867, scale=0.9543\n",
      "Fitted standardization for 15_yeojohnson with mean=0.0252, scale=0.9878\n",
      "Fitted standardization for 55_yeojohnson with mean=0.0033, scale=0.1419\n",
      "Fitted standardization for 39_yeojohnson with mean=0.3342, scale=0.8303\n",
      "Fitted standardization for 34_yeojohnson with mean=0.2439, scale=0.8985\n",
      "Fitted standardization for 69_yeojohnson with mean=0.0206, scale=0.0038\n",
      "Fitted standardization for 14_yeojohnson with mean=0.0963, scale=0.9350\n",
      "Fitted standardization for 63_yeojohnson with mean=1.1498, scale=0.3603\n",
      "Fitted standardization for 62_yeojohnson with mean=0.1303, scale=3.1783\n",
      "Fitted standardization for 31_yeojohnson with mean=0.2033, scale=0.9200\n",
      "Fitted standardization for 41_yeojohnson with mean=0.3187, scale=0.8424\n",
      "Fitted standardization for 40_yeojohnson with mean=0.4235, scale=0.6662\n",
      "Fitted standardization for 54_yeojohnson with mean=0.4637, scale=0.2252\n",
      "Fitted standardization for 42_yeojohnson with mean=0.3493, scale=0.7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting standardization: 100%|██████████| 100/100 [00:00<00:00, 223.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted standardization for 4_yeojohnson with mean=0.0543, scale=0.9044\n",
      "Fitted standardization for 51_yeojohnson with mean=-0.2049, scale=0.9191\n",
      "Fitted standardization for 52_yeojohnson with mean=0.5240, scale=0.1683\n",
      "Fitted standardization for 61_yeojohnson with mean=-0.7230, scale=7.1096\n",
      "Fitted standardization for 16_yeojohnson with mean=0.0892, scale=0.9387\n",
      "Fitted standardization for 65_yeojohnson with mean=-3.0672, scale=9.6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying standardization: 100%|██████████| 100/100 [00:00<00:00, 301.16it/s]\n",
      "Applying transformations: 100%|██████████| 30/30 [00:00<00:00, 202.84it/s]\n",
      "Applying standardization: 100%|██████████| 100/100 [00:00<00:00, 1210.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting neural network preprocessor pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:   3%|▎         | 1/30 [00:00<00:09,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 68 with lambda=-9.5392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:   7%|▋         | 2/30 [00:00<00:08,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 2 with lambda=1.3149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  10%|█         | 3/30 [00:01<00:13,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 43 with lambda=2.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  13%|█▎        | 4/30 [00:01<00:12,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 5 with lambda=0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  17%|█▋        | 5/30 [00:02<00:10,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 67 with lambda=-13.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  20%|██        | 6/30 [00:02<00:11,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 44 with lambda=2.1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  23%|██▎       | 7/30 [00:03<00:10,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 64 with lambda=0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  27%|██▋       | 8/30 [00:03<00:08,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 66 with lambda=-0.9896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  30%|███       | 9/30 [00:03<00:07,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 3 with lambda=0.5652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  33%|███▎      | 10/30 [00:03<00:06,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 53 with lambda=-1.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  37%|███▋      | 11/30 [00:04<00:07,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 49 with lambda=0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  43%|████▎     | 13/30 [00:04<00:05,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 15 with lambda=1.1072\n",
      "Fitted yeo-johnson transformation for 55 with lambda=0.8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  47%|████▋     | 14/30 [00:05<00:05,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 39 with lambda=2.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  50%|█████     | 15/30 [00:05<00:05,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 34 with lambda=1.7605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  53%|█████▎    | 16/30 [00:06<00:04,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 69 with lambda=-32.4964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  57%|█████▋    | 17/30 [00:06<00:04,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 14 with lambda=1.3551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  60%|██████    | 18/30 [00:06<00:04,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 63 with lambda=-0.3036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  63%|██████▎   | 19/30 [00:07<00:04,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 62 with lambda=0.1496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  67%|██████▋   | 20/30 [00:07<00:04,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 31 with lambda=1.6285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  70%|███████   | 21/30 [00:08<00:04,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 41 with lambda=2.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  73%|███████▎  | 22/30 [00:08<00:03,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 40 with lambda=2.8365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  77%|███████▋  | 23/30 [00:09<00:03,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 54 with lambda=-0.6452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  80%|████████  | 24/30 [00:09<00:02,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 42 with lambda=2.2412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  83%|████████▎ | 25/30 [00:10<00:02,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 4 with lambda=1.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  87%|████████▋ | 26/30 [00:10<00:01,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 51 with lambda=0.3618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  90%|█████████ | 27/30 [00:10<00:01,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 52 with lambda=-0.8524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  93%|█████████▎| 28/30 [00:11<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 61 with lambda=0.2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations:  97%|█████████▋| 29/30 [00:11<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 16 with lambda=1.3351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting transformations: 100%|██████████| 30/30 [00:12<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted yeo-johnson transformation for 65 with lambda=0.2489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying transformations: 100%|██████████| 30/30 [00:00<00:00, 39.88it/s]\n",
      "Fitting standardization:  44%|████▍     | 44/100 [00:00<00:00, 223.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted standardization for 0 with mean=-0.0000, scale=0.9999\n",
      "Fitted standardization for 1 with mean=-0.0002, scale=0.9999\n",
      "Fitted standardization for 2 with mean=0.0000, scale=1.0012\n",
      "Fitted standardization for 3 with mean=-0.0000, scale=1.0009\n",
      "Fitted standardization for 4 with mean=0.0008, scale=0.9979\n",
      "Fitted standardization for 5 with mean=-0.0011, scale=0.9974\n",
      "Fitted standardization for 6 with mean=-0.0003, scale=1.0000\n",
      "Fitted standardization for 7 with mean=0.0004, scale=0.9998\n",
      "Fitted standardization for 8 with mean=-0.0004, scale=1.0001\n",
      "Fitted standardization for 9 with mean=0.0001, scale=1.0002\n",
      "Fitted standardization for 10 with mean=0.0003, scale=0.9998\n",
      "Fitted standardization for 11 with mean=0.0006, scale=0.9999\n",
      "Fitted standardization for 12 with mean=-0.0002, scale=0.9999\n",
      "Fitted standardization for 13 with mean=-0.0004, scale=1.0001\n",
      "Fitted standardization for 14 with mean=-0.0000, scale=1.0005\n",
      "Fitted standardization for 15 with mean=-0.0003, scale=0.9996\n",
      "Fitted standardization for 16 with mean=-0.0003, scale=1.0000\n",
      "Fitted standardization for 17 with mean=-0.0001, scale=0.9997\n",
      "Fitted standardization for 18 with mean=0.0004, scale=0.9999\n",
      "Fitted standardization for 19 with mean=-0.0000, scale=0.9998\n",
      "Fitted standardization for 20 with mean=0.0003, scale=1.0001\n",
      "Fitted standardization for 21 with mean=-0.0002, scale=0.9997\n",
      "Fitted standardization for 22 with mean=-0.0000, scale=1.0002\n",
      "Fitted standardization for 23 with mean=-0.0001, scale=1.0002\n",
      "Fitted standardization for 24 with mean=0.0004, scale=1.0001\n",
      "Fitted standardization for 25 with mean=-0.0001, scale=1.0000\n",
      "Fitted standardization for 26 with mean=-0.0002, scale=1.0003\n",
      "Fitted standardization for 27 with mean=0.0004, scale=0.9997\n",
      "Fitted standardization for 28 with mean=0.0005, scale=0.9996\n",
      "Fitted standardization for 29 with mean=0.0006, scale=0.9995\n",
      "Fitted standardization for 30 with mean=-0.0003, scale=1.0004\n",
      "Fitted standardization for 31 with mean=-0.0006, scale=1.0007\n",
      "Fitted standardization for 32 with mean=-0.0000, scale=1.0005\n",
      "Fitted standardization for 33 with mean=0.0002, scale=1.0000\n",
      "Fitted standardization for 34 with mean=0.0003, scale=1.0003\n",
      "Fitted standardization for 35 with mean=-0.0002, scale=1.0001\n",
      "Fitted standardization for 36 with mean=-0.0003, scale=0.9997\n",
      "Fitted standardization for 37 with mean=0.0004, scale=0.9993\n",
      "Fitted standardization for 38 with mean=0.0004, scale=0.9994\n",
      "Fitted standardization for 39 with mean=-0.0002, scale=0.9999\n",
      "Fitted standardization for 40 with mean=-0.0000, scale=1.0000\n",
      "Fitted standardization for 41 with mean=-0.0004, scale=1.0003\n",
      "Fitted standardization for 42 with mean=-0.0005, scale=1.0001\n",
      "Fitted standardization for 43 with mean=-0.0003, scale=1.0004\n",
      "Fitted standardization for 44 with mean=-0.0005, scale=0.9999\n",
      "Fitted standardization for 45 with mean=0.0002, scale=1.0002\n",
      "Fitted standardization for 46 with mean=-0.0001, scale=1.0001\n",
      "Fitted standardization for 47 with mean=-0.0002, scale=1.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting standardization:  93%|█████████▎| 93/100 [00:00<00:00, 223.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted standardization for 48 with mean=0.0006, scale=1.0003\n",
      "Fitted standardization for 49 with mean=0.0003, scale=0.9973\n",
      "Fitted standardization for 50 with mean=-0.0002, scale=0.9997\n",
      "Fitted standardization for 51 with mean=0.0004, scale=1.0003\n",
      "Fitted standardization for 52 with mean=1.3461, scale=8.9012\n",
      "Fitted standardization for 53 with mean=1.1896, scale=1.4834\n",
      "Fitted standardization for 54 with mean=0.9096, scale=0.7265\n",
      "Fitted standardization for 55 with mean=0.0041, scale=0.1430\n",
      "Fitted standardization for 56 with mean=0.1097, scale=0.3125\n",
      "Fitted standardization for 57 with mean=0.1054, scale=0.3071\n",
      "Fitted standardization for 58 with mean=0.6684, scale=0.4708\n",
      "Fitted standardization for 59 with mean=0.3457, scale=0.4756\n",
      "Fitted standardization for 60 with mean=0.3605, scale=0.4801\n",
      "Fitted standardization for 61 with mean=751.4038, scale=31937.9306\n",
      "Fitted standardization for 62 with mean=743.0172, scale=31550.7630\n",
      "Fitted standardization for 63 with mean=82.9839, scale=2667.3007\n",
      "Fitted standardization for 64 with mean=769.0512, scale=32680.2251\n",
      "Fitted standardization for 65 with mean=733.4401, scale=31253.1964\n",
      "Fitted standardization for 66 with mean=2.0782, scale=5.1072\n",
      "Fitted standardization for 67 with mean=0.2671, scale=0.1490\n",
      "Fitted standardization for 68 with mean=0.3682, scale=0.0676\n",
      "Fitted standardization for 69 with mean=0.0412, scale=0.0371\n",
      "Fitted standardization for 68_yeojohnson with mean=0.0991, scale=0.0017\n",
      "Fitted standardization for 2_yeojohnson with mean=0.0637, scale=0.8801\n",
      "Fitted standardization for 43_yeojohnson with mean=0.3076, scale=0.8481\n",
      "Fitted standardization for 5_yeojohnson with mean=-0.0808, scale=0.7992\n",
      "Fitted standardization for 67_yeojohnson with mean=0.0730, scale=0.0012\n",
      "Fitted standardization for 44_yeojohnson with mean=0.3300, scale=0.8212\n",
      "Fitted standardization for 64_yeojohnson with mean=1.2079, scale=2.0324\n",
      "Fitted standardization for 66_yeojohnson with mean=0.4727, scale=0.2376\n",
      "Fitted standardization for 3_yeojohnson with mean=-0.0846, scale=0.8113\n",
      "Fitted standardization for 53_yeojohnson with mean=0.3418, scale=0.0719\n",
      "Fitted standardization for 49_yeojohnson with mean=-0.0867, scale=0.9543\n",
      "Fitted standardization for 15_yeojohnson with mean=0.0252, scale=0.9878\n",
      "Fitted standardization for 55_yeojohnson with mean=0.0033, scale=0.1419\n",
      "Fitted standardization for 39_yeojohnson with mean=0.3342, scale=0.8303\n",
      "Fitted standardization for 34_yeojohnson with mean=0.2439, scale=0.8985\n",
      "Fitted standardization for 69_yeojohnson with mean=0.0206, scale=0.0038\n",
      "Fitted standardization for 14_yeojohnson with mean=0.0963, scale=0.9350\n",
      "Fitted standardization for 63_yeojohnson with mean=1.1498, scale=0.3603\n",
      "Fitted standardization for 62_yeojohnson with mean=0.1303, scale=3.1783\n",
      "Fitted standardization for 31_yeojohnson with mean=0.2033, scale=0.9200\n",
      "Fitted standardization for 41_yeojohnson with mean=0.3187, scale=0.8424\n",
      "Fitted standardization for 40_yeojohnson with mean=0.4235, scale=0.6662\n",
      "Fitted standardization for 54_yeojohnson with mean=0.4637, scale=0.2252\n",
      "Fitted standardization for 42_yeojohnson with mean=0.3493, scale=0.7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting standardization: 100%|██████████| 100/100 [00:00<00:00, 219.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted standardization for 4_yeojohnson with mean=0.0543, scale=0.9044\n",
      "Fitted standardization for 51_yeojohnson with mean=-0.2049, scale=0.9191\n",
      "Fitted standardization for 52_yeojohnson with mean=0.5240, scale=0.1683\n",
      "Fitted standardization for 61_yeojohnson with mean=-0.7230, scale=7.1096\n",
      "Fitted standardization for 16_yeojohnson with mean=0.0892, scale=0.9387\n",
      "Fitted standardization for 65_yeojohnson with mean=-3.0672, scale=9.6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying standardization: 100%|██████████| 100/100 [00:00<00:00, 284.76it/s]\n",
      "Fitting normalization:   2%|▏         | 3/200 [00:00<00:15, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 0 fitted, range: [0, 1]\n",
      "Normalization for 1 fitted, range: [0, 1]\n",
      "Normalization for 2 fitted, range: [0, 1]\n",
      "Normalization for 3 fitted, range: [0, 1]\n",
      "Normalization for 4 fitted, range: [0, 1]\n",
      "Normalization for 5 fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:   6%|▋         | 13/200 [00:00<00:05, 34.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 6 fitted, range: [0, 1]\n",
      "Normalization for 7 fitted, range: [0, 1]\n",
      "Normalization for 8 fitted, range: [0, 1]\n",
      "Normalization for 9 fitted, range: [0, 1]\n",
      "Normalization for 10 fitted, range: [0, 1]\n",
      "Normalization for 11 fitted, range: [0, 1]\n",
      "Normalization for 12 fitted, range: [0, 1]\n",
      "Normalization for 13 fitted, range: [0, 1]\n",
      "Normalization for 14 fitted, range: [0, 1]\n",
      "Normalization for 15 fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  12%|█▏        | 24/200 [00:00<00:04, 41.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 16 fitted, range: [0, 1]\n",
      "Normalization for 17 fitted, range: [0, 1]\n",
      "Normalization for 18 fitted, range: [0, 1]\n",
      "Normalization for 19 fitted, range: [0, 1]\n",
      "Normalization for 20 fitted, range: [0, 1]\n",
      "Normalization for 21 fitted, range: [0, 1]\n",
      "Normalization for 22 fitted, range: [0, 1]\n",
      "Normalization for 23 fitted, range: [0, 1]\n",
      "Normalization for 24 fitted, range: [0, 1]\n",
      "Normalization for 25 fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  18%|█▊        | 35/200 [00:00<00:03, 46.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 26 fitted, range: [0, 1]\n",
      "Normalization for 27 fitted, range: [0, 1]\n",
      "Normalization for 28 fitted, range: [0, 1]\n",
      "Normalization for 29 fitted, range: [0, 1]\n",
      "Normalization for 30 fitted, range: [0, 1]\n",
      "Normalization for 31 fitted, range: [0, 1]\n",
      "Normalization for 32 fitted, range: [0, 1]\n",
      "Normalization for 33 fitted, range: [0, 1]\n",
      "Normalization for 34 fitted, range: [0, 1]\n",
      "Normalization for 35 fitted, range: [0, 1]\n",
      "Normalization for 36 fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  24%|██▎       | 47/200 [00:01<00:03, 48.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 37 fitted, range: [0, 1]\n",
      "Normalization for 38 fitted, range: [0, 1]\n",
      "Normalization for 39 fitted, range: [0, 1]\n",
      "Normalization for 40 fitted, range: [0, 1]\n",
      "Normalization for 41 fitted, range: [0, 1]\n",
      "Normalization for 42 fitted, range: [0, 1]\n",
      "Normalization for 43 fitted, range: [0, 1]\n",
      "Normalization for 44 fitted, range: [0, 1]\n",
      "Normalization for 45 fitted, range: [0, 1]\n",
      "Normalization for 46 fitted, range: [0, 1]\n",
      "Normalization for 47 fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  36%|███▌      | 71/200 [00:01<00:01, 84.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 48 fitted, range: [0, 1]\n",
      "Normalization for 49 fitted, range: [0, 1]\n",
      "Normalization for 50 fitted, range: [0, 1]\n",
      "Normalization for 51 fitted, range: [0, 1]\n",
      "Normalization for 52 fitted, range: [0, 1]\n",
      "Normalization for 53 fitted, range: [0, 1]\n",
      "Normalization for 54 fitted, range: [0, 1]\n",
      "Normalization for 55 fitted, range: [0, 1]\n",
      "Normalization for 56 fitted, range: [0, 1]\n",
      "Normalization for 57 fitted, range: [0, 1]\n",
      "Normalization for 58 fitted, range: [0, 1]\n",
      "Normalization for 59 fitted, range: [0, 1]\n",
      "Normalization for 60 fitted, range: [0, 1]\n",
      "Normalization for 61 fitted, range: [0, 1]\n",
      "Normalization for 62 fitted, range: [0, 1]\n",
      "Normalization for 63 fitted, range: [0, 1]\n",
      "Normalization for 64 fitted, range: [0, 1]\n",
      "Normalization for 65 fitted, range: [0, 1]\n",
      "Normalization for 66 fitted, range: [0, 1]\n",
      "Normalization for 67 fitted, range: [0, 1]\n",
      "Normalization for 68 fitted, range: [0, 1]\n",
      "Normalization for 69 fitted, range: [0, 1]\n",
      "Normalization for 68_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 2_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 43_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 5_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 67_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 44_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 64_yeojohnson fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  50%|█████     | 101/200 [00:01<00:00, 143.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 66_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 3_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 53_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 49_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 15_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 55_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 39_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 34_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 69_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 14_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 63_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 62_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 31_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 41_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 40_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 54_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 42_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 4_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 51_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 52_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 61_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 16_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 65_yeojohnson fitted, range: [0, 1]\n",
      "Normalization for 0_std fitted, range: [0, 1]\n",
      "Normalization for 1_std fitted, range: [0, 1]\n",
      "Normalization for 2_std fitted, range: [0, 1]\n",
      "Normalization for 3_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  58%|█████▊    | 116/200 [00:01<00:00, 94.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 4_std fitted, range: [0, 1]\n",
      "Normalization for 5_std fitted, range: [0, 1]\n",
      "Normalization for 6_std fitted, range: [0, 1]\n",
      "Normalization for 7_std fitted, range: [0, 1]\n",
      "Normalization for 8_std fitted, range: [0, 1]\n",
      "Normalization for 9_std fitted, range: [0, 1]\n",
      "Normalization for 10_std fitted, range: [0, 1]\n",
      "Normalization for 11_std fitted, range: [0, 1]\n",
      "Normalization for 12_std fitted, range: [0, 1]\n",
      "Normalization for 13_std fitted, range: [0, 1]\n",
      "Normalization for 14_std fitted, range: [0, 1]\n",
      "Normalization for 15_std fitted, range: [0, 1]\n",
      "Normalization for 16_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  70%|███████   | 140/200 [00:01<00:00, 101.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 17_std fitted, range: [0, 1]\n",
      "Normalization for 18_std fitted, range: [0, 1]\n",
      "Normalization for 19_std fitted, range: [0, 1]\n",
      "Normalization for 20_std fitted, range: [0, 1]\n",
      "Normalization for 21_std fitted, range: [0, 1]\n",
      "Normalization for 22_std fitted, range: [0, 1]\n",
      "Normalization for 23_std fitted, range: [0, 1]\n",
      "Normalization for 24_std fitted, range: [0, 1]\n",
      "Normalization for 25_std fitted, range: [0, 1]\n",
      "Normalization for 26_std fitted, range: [0, 1]\n",
      "Normalization for 27_std fitted, range: [0, 1]\n",
      "Normalization for 28_std fitted, range: [0, 1]\n",
      "Normalization for 29_std fitted, range: [0, 1]\n",
      "Normalization for 30_std fitted, range: [0, 1]\n",
      "Normalization for 31_std fitted, range: [0, 1]\n",
      "Normalization for 32_std fitted, range: [0, 1]\n",
      "Normalization for 33_std fitted, range: [0, 1]\n",
      "Normalization for 34_std fitted, range: [0, 1]\n",
      "Normalization for 35_std fitted, range: [0, 1]\n",
      "Normalization for 36_std fitted, range: [0, 1]\n",
      "Normalization for 37_std fitted, range: [0, 1]\n",
      "Normalization for 38_std fitted, range: [0, 1]\n",
      "Normalization for 39_std fitted, range: [0, 1]\n",
      "Normalization for 40_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  76%|███████▌  | 152/200 [00:02<00:00, 85.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 41_std fitted, range: [0, 1]\n",
      "Normalization for 42_std fitted, range: [0, 1]\n",
      "Normalization for 43_std fitted, range: [0, 1]\n",
      "Normalization for 44_std fitted, range: [0, 1]\n",
      "Normalization for 45_std fitted, range: [0, 1]\n",
      "Normalization for 46_std fitted, range: [0, 1]\n",
      "Normalization for 47_std fitted, range: [0, 1]\n",
      "Normalization for 48_std fitted, range: [0, 1]\n",
      "Normalization for 49_std fitted, range: [0, 1]\n",
      "Normalization for 50_std fitted, range: [0, 1]\n",
      "Normalization for 51_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  81%|████████  | 162/200 [00:02<00:00, 75.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 52_std fitted, range: [0, 1]\n",
      "Normalization for 53_std fitted, range: [0, 1]\n",
      "Normalization for 54_std fitted, range: [0, 1]\n",
      "Normalization for 55_std fitted, range: [0, 1]\n",
      "Normalization for 56_std fitted, range: [0, 1]\n",
      "Normalization for 57_std fitted, range: [0, 1]\n",
      "Normalization for 58_std fitted, range: [0, 1]\n",
      "Normalization for 59_std fitted, range: [0, 1]\n",
      "Normalization for 60_std fitted, range: [0, 1]\n",
      "Normalization for 61_std fitted, range: [0, 1]\n",
      "Normalization for 62_std fitted, range: [0, 1]\n",
      "Normalization for 63_std fitted, range: [0, 1]\n",
      "Normalization for 64_std fitted, range: [0, 1]\n",
      "Normalization for 65_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  86%|████████▌ | 171/200 [00:02<00:00, 66.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 66_std fitted, range: [0, 1]\n",
      "Normalization for 67_std fitted, range: [0, 1]\n",
      "Normalization for 68_std fitted, range: [0, 1]\n",
      "Normalization for 69_std fitted, range: [0, 1]\n",
      "Normalization for 68_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 2_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 43_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 5_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 67_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 44_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 64_yeojohnson_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  93%|█████████▎| 186/200 [00:02<00:00, 60.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 66_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 3_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 53_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 49_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 15_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 55_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 39_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 34_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 69_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 14_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 63_yeojohnson_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization:  96%|█████████▋| 193/200 [00:02<00:00, 58.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 62_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 31_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 41_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 40_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 54_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 42_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 4_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 51_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 52_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 61_yeojohnson_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting normalization: 100%|██████████| 200/200 [00:03<00:00, 64.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization for 16_yeojohnson_std fitted, range: [0, 1]\n",
      "Normalization for 65_yeojohnson_std fitted, range: [0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying normalization: 100%|██████████| 200/200 [00:01<00:00, 144.55it/s]\n",
      "Applying transformations: 100%|██████████| 30/30 [00:00<00:00, 115.12it/s]\n",
      "Applying standardization: 100%|██████████| 100/100 [00:00<00:00, 427.58it/s]\n",
      "Applying normalization: 100%|██████████| 200/200 [00:00<00:00, 352.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data after regular processing: (939568, 200)\n",
      "Shape of test data after regular processing: (234893, 200)\n",
      "Shape of training data after neural network processing: (939568, 400)\n",
      "Shape of test data after neural network processing: (234893, 400)\n",
      "Class weights: {0: 0.6145365473983014, 1: 1.5236130773135108, 2: 1.3958236769232597}\n",
      "\n",
      "Using Bayesian Optimization to tune XGBoost parameters...\n",
      "Starting Bayesian Optimization...\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Best F1 score: 0.5487\n",
      "Best parameters:\n",
      "  colsample_bytree: 0.9856871320071308\n",
      "  gamma: 2.4008452304093133\n",
      "  learning_rate: 0.05756906293210057\n",
      "  max_depth: 6\n",
      "  min_child_weight: 2\n",
      "  n_estimators: 779\n",
      "  reg_alpha: 3.117402698101539\n",
      "  reg_lambda: 9.543507438434661\n",
      "  subsample: 0.5825027039121933\n",
      "Validation F1 score: 0.5496\n",
      "\n",
      "Preparing Stacking model...\n",
      "\n",
      "Training XGBoost...\n",
      "[0]\tvalidation_0-mlogloss:1.07526\tvalidation_0-merror:0.41602\tvalidation_1-mlogloss:1.07527\tvalidation_1-merror:0.41493\n",
      "[1]\tvalidation_0-mlogloss:1.05367\tvalidation_0-merror:0.41383\tvalidation_1-mlogloss:1.05369\tvalidation_1-merror:0.41248\n",
      "[2]\tvalidation_0-mlogloss:1.03364\tvalidation_0-merror:0.41317\tvalidation_1-mlogloss:1.03368\tvalidation_1-merror:0.41255\n",
      "[3]\tvalidation_0-mlogloss:1.01501\tvalidation_0-merror:0.41239\tvalidation_1-mlogloss:1.01507\tvalidation_1-merror:0.41177\n",
      "[4]\tvalidation_0-mlogloss:0.99766\tvalidation_0-merror:0.41183\tvalidation_1-mlogloss:0.99773\tvalidation_1-merror:0.41134\n",
      "[5]\tvalidation_0-mlogloss:0.98146\tvalidation_0-merror:0.41151\tvalidation_1-mlogloss:0.98156\tvalidation_1-merror:0.41042\n",
      "[6]\tvalidation_0-mlogloss:0.96636\tvalidation_0-merror:0.41080\tvalidation_1-mlogloss:0.96648\tvalidation_1-merror:0.40997\n",
      "[7]\tvalidation_0-mlogloss:0.95221\tvalidation_0-merror:0.41063\tvalidation_1-mlogloss:0.95235\tvalidation_1-merror:0.40963\n",
      "[8]\tvalidation_0-mlogloss:0.93895\tvalidation_0-merror:0.41025\tvalidation_1-mlogloss:0.93913\tvalidation_1-merror:0.40939\n",
      "[9]\tvalidation_0-mlogloss:0.92648\tvalidation_0-merror:0.41003\tvalidation_1-mlogloss:0.92669\tvalidation_1-merror:0.40889\n",
      "[10]\tvalidation_0-mlogloss:0.91477\tvalidation_0-merror:0.40980\tvalidation_1-mlogloss:0.91500\tvalidation_1-merror:0.40902\n",
      "[11]\tvalidation_0-mlogloss:0.90373\tvalidation_0-merror:0.40934\tvalidation_1-mlogloss:0.90398\tvalidation_1-merror:0.40867\n",
      "[12]\tvalidation_0-mlogloss:0.89334\tvalidation_0-merror:0.40911\tvalidation_1-mlogloss:0.89360\tvalidation_1-merror:0.40884\n",
      "[13]\tvalidation_0-mlogloss:0.88354\tvalidation_0-merror:0.40882\tvalidation_1-mlogloss:0.88384\tvalidation_1-merror:0.40881\n",
      "[14]\tvalidation_0-mlogloss:0.87428\tvalidation_0-merror:0.40874\tvalidation_1-mlogloss:0.87461\tvalidation_1-merror:0.40884\n",
      "[15]\tvalidation_0-mlogloss:0.86551\tvalidation_0-merror:0.40850\tvalidation_1-mlogloss:0.86585\tvalidation_1-merror:0.40872\n",
      "[16]\tvalidation_0-mlogloss:0.85716\tvalidation_0-merror:0.40822\tvalidation_1-mlogloss:0.85753\tvalidation_1-merror:0.40835\n",
      "[17]\tvalidation_0-mlogloss:0.84926\tvalidation_0-merror:0.40784\tvalidation_1-mlogloss:0.84965\tvalidation_1-merror:0.40765\n",
      "[18]\tvalidation_0-mlogloss:0.84176\tvalidation_0-merror:0.40747\tvalidation_1-mlogloss:0.84219\tvalidation_1-merror:0.40719\n",
      "[19]\tvalidation_0-mlogloss:0.83468\tvalidation_0-merror:0.40726\tvalidation_1-mlogloss:0.83513\tvalidation_1-merror:0.40719\n",
      "[20]\tvalidation_0-mlogloss:0.82796\tvalidation_0-merror:0.40686\tvalidation_1-mlogloss:0.82845\tvalidation_1-merror:0.40698\n",
      "[21]\tvalidation_0-mlogloss:0.82156\tvalidation_0-merror:0.40657\tvalidation_1-mlogloss:0.82207\tvalidation_1-merror:0.40668\n",
      "[22]\tvalidation_0-mlogloss:0.81542\tvalidation_0-merror:0.40631\tvalidation_1-mlogloss:0.81596\tvalidation_1-merror:0.40667\n",
      "[23]\tvalidation_0-mlogloss:0.80958\tvalidation_0-merror:0.40587\tvalidation_1-mlogloss:0.81015\tvalidation_1-merror:0.40651\n",
      "[24]\tvalidation_0-mlogloss:0.80400\tvalidation_0-merror:0.40535\tvalidation_1-mlogloss:0.80458\tvalidation_1-merror:0.40639\n",
      "[25]\tvalidation_0-mlogloss:0.79867\tvalidation_0-merror:0.40509\tvalidation_1-mlogloss:0.79929\tvalidation_1-merror:0.40570\n",
      "[26]\tvalidation_0-mlogloss:0.79356\tvalidation_0-merror:0.40454\tvalidation_1-mlogloss:0.79422\tvalidation_1-merror:0.40526\n",
      "[27]\tvalidation_0-mlogloss:0.78869\tvalidation_0-merror:0.40423\tvalidation_1-mlogloss:0.78938\tvalidation_1-merror:0.40481\n",
      "[28]\tvalidation_0-mlogloss:0.78400\tvalidation_0-merror:0.40382\tvalidation_1-mlogloss:0.78471\tvalidation_1-merror:0.40441\n",
      "[29]\tvalidation_0-mlogloss:0.77954\tvalidation_0-merror:0.40349\tvalidation_1-mlogloss:0.78027\tvalidation_1-merror:0.40394\n",
      "[30]\tvalidation_0-mlogloss:0.77524\tvalidation_0-merror:0.40309\tvalidation_1-mlogloss:0.77601\tvalidation_1-merror:0.40382\n",
      "[31]\tvalidation_0-mlogloss:0.77114\tvalidation_0-merror:0.40267\tvalidation_1-mlogloss:0.77193\tvalidation_1-merror:0.40349\n",
      "[32]\tvalidation_0-mlogloss:0.76719\tvalidation_0-merror:0.40224\tvalidation_1-mlogloss:0.76801\tvalidation_1-merror:0.40321\n",
      "[33]\tvalidation_0-mlogloss:0.76343\tvalidation_0-merror:0.40188\tvalidation_1-mlogloss:0.76427\tvalidation_1-merror:0.40295\n",
      "[34]\tvalidation_0-mlogloss:0.75983\tvalidation_0-merror:0.40153\tvalidation_1-mlogloss:0.76071\tvalidation_1-merror:0.40251\n",
      "[35]\tvalidation_0-mlogloss:0.75634\tvalidation_0-merror:0.40104\tvalidation_1-mlogloss:0.75725\tvalidation_1-merror:0.40206\n",
      "[36]\tvalidation_0-mlogloss:0.75301\tvalidation_0-merror:0.40082\tvalidation_1-mlogloss:0.75394\tvalidation_1-merror:0.40155\n",
      "[37]\tvalidation_0-mlogloss:0.74978\tvalidation_0-merror:0.40040\tvalidation_1-mlogloss:0.75074\tvalidation_1-merror:0.40134\n",
      "[38]\tvalidation_0-mlogloss:0.74671\tvalidation_0-merror:0.40005\tvalidation_1-mlogloss:0.74770\tvalidation_1-merror:0.40070\n",
      "[39]\tvalidation_0-mlogloss:0.74375\tvalidation_0-merror:0.39967\tvalidation_1-mlogloss:0.74476\tvalidation_1-merror:0.40049\n",
      "[40]\tvalidation_0-mlogloss:0.74090\tvalidation_0-merror:0.39947\tvalidation_1-mlogloss:0.74196\tvalidation_1-merror:0.40046\n",
      "[41]\tvalidation_0-mlogloss:0.73814\tvalidation_0-merror:0.39915\tvalidation_1-mlogloss:0.73923\tvalidation_1-merror:0.39970\n",
      "[42]\tvalidation_0-mlogloss:0.73549\tvalidation_0-merror:0.39887\tvalidation_1-mlogloss:0.73661\tvalidation_1-merror:0.39950\n",
      "[43]\tvalidation_0-mlogloss:0.73294\tvalidation_0-merror:0.39854\tvalidation_1-mlogloss:0.73409\tvalidation_1-merror:0.39916\n",
      "[44]\tvalidation_0-mlogloss:0.73051\tvalidation_0-merror:0.39837\tvalidation_1-mlogloss:0.73169\tvalidation_1-merror:0.39890\n",
      "[45]\tvalidation_0-mlogloss:0.72813\tvalidation_0-merror:0.39814\tvalidation_1-mlogloss:0.72935\tvalidation_1-merror:0.39884\n",
      "[46]\tvalidation_0-mlogloss:0.72585\tvalidation_0-merror:0.39773\tvalidation_1-mlogloss:0.72709\tvalidation_1-merror:0.39848\n",
      "[47]\tvalidation_0-mlogloss:0.72362\tvalidation_0-merror:0.39736\tvalidation_1-mlogloss:0.72488\tvalidation_1-merror:0.39837\n",
      "[48]\tvalidation_0-mlogloss:0.72150\tvalidation_0-merror:0.39710\tvalidation_1-mlogloss:0.72280\tvalidation_1-merror:0.39811\n",
      "[49]\tvalidation_0-mlogloss:0.71946\tvalidation_0-merror:0.39685\tvalidation_1-mlogloss:0.72078\tvalidation_1-merror:0.39780\n",
      "[50]\tvalidation_0-mlogloss:0.71746\tvalidation_0-merror:0.39658\tvalidation_1-mlogloss:0.71882\tvalidation_1-merror:0.39736\n",
      "[51]\tvalidation_0-mlogloss:0.71555\tvalidation_0-merror:0.39634\tvalidation_1-mlogloss:0.71694\tvalidation_1-merror:0.39706\n",
      "[52]\tvalidation_0-mlogloss:0.71373\tvalidation_0-merror:0.39607\tvalidation_1-mlogloss:0.71514\tvalidation_1-merror:0.39687\n",
      "[53]\tvalidation_0-mlogloss:0.71195\tvalidation_0-merror:0.39583\tvalidation_1-mlogloss:0.71339\tvalidation_1-merror:0.39671\n",
      "[54]\tvalidation_0-mlogloss:0.71021\tvalidation_0-merror:0.39551\tvalidation_1-mlogloss:0.71169\tvalidation_1-merror:0.39664\n",
      "[55]\tvalidation_0-mlogloss:0.70857\tvalidation_0-merror:0.39534\tvalidation_1-mlogloss:0.71008\tvalidation_1-merror:0.39658\n",
      "[56]\tvalidation_0-mlogloss:0.70699\tvalidation_0-merror:0.39499\tvalidation_1-mlogloss:0.70852\tvalidation_1-merror:0.39620\n",
      "[57]\tvalidation_0-mlogloss:0.70545\tvalidation_0-merror:0.39481\tvalidation_1-mlogloss:0.70702\tvalidation_1-merror:0.39605\n",
      "[58]\tvalidation_0-mlogloss:0.70390\tvalidation_0-merror:0.39448\tvalidation_1-mlogloss:0.70549\tvalidation_1-merror:0.39570\n",
      "[59]\tvalidation_0-mlogloss:0.70243\tvalidation_0-merror:0.39422\tvalidation_1-mlogloss:0.70405\tvalidation_1-merror:0.39524\n",
      "[60]\tvalidation_0-mlogloss:0.70102\tvalidation_0-merror:0.39385\tvalidation_1-mlogloss:0.70267\tvalidation_1-merror:0.39496\n",
      "[61]\tvalidation_0-mlogloss:0.69965\tvalidation_0-merror:0.39361\tvalidation_1-mlogloss:0.70133\tvalidation_1-merror:0.39485\n",
      "[62]\tvalidation_0-mlogloss:0.69837\tvalidation_0-merror:0.39337\tvalidation_1-mlogloss:0.70006\tvalidation_1-merror:0.39471\n",
      "[63]\tvalidation_0-mlogloss:0.69706\tvalidation_0-merror:0.39311\tvalidation_1-mlogloss:0.69878\tvalidation_1-merror:0.39476\n",
      "[64]\tvalidation_0-mlogloss:0.69583\tvalidation_0-merror:0.39296\tvalidation_1-mlogloss:0.69758\tvalidation_1-merror:0.39458\n",
      "[65]\tvalidation_0-mlogloss:0.69462\tvalidation_0-merror:0.39278\tvalidation_1-mlogloss:0.69640\tvalidation_1-merror:0.39432\n",
      "[66]\tvalidation_0-mlogloss:0.69342\tvalidation_0-merror:0.39249\tvalidation_1-mlogloss:0.69522\tvalidation_1-merror:0.39415\n",
      "[67]\tvalidation_0-mlogloss:0.69230\tvalidation_0-merror:0.39230\tvalidation_1-mlogloss:0.69411\tvalidation_1-merror:0.39400\n",
      "[68]\tvalidation_0-mlogloss:0.69122\tvalidation_0-merror:0.39214\tvalidation_1-mlogloss:0.69306\tvalidation_1-merror:0.39396\n",
      "[69]\tvalidation_0-mlogloss:0.69014\tvalidation_0-merror:0.39179\tvalidation_1-mlogloss:0.69201\tvalidation_1-merror:0.39366\n",
      "[70]\tvalidation_0-mlogloss:0.68915\tvalidation_0-merror:0.39168\tvalidation_1-mlogloss:0.69104\tvalidation_1-merror:0.39351\n",
      "[71]\tvalidation_0-mlogloss:0.68817\tvalidation_0-merror:0.39144\tvalidation_1-mlogloss:0.69009\tvalidation_1-merror:0.39317\n",
      "[72]\tvalidation_0-mlogloss:0.68721\tvalidation_0-merror:0.39127\tvalidation_1-mlogloss:0.68916\tvalidation_1-merror:0.39297\n",
      "[73]\tvalidation_0-mlogloss:0.68628\tvalidation_0-merror:0.39126\tvalidation_1-mlogloss:0.68826\tvalidation_1-merror:0.39289\n",
      "[74]\tvalidation_0-mlogloss:0.68537\tvalidation_0-merror:0.39105\tvalidation_1-mlogloss:0.68738\tvalidation_1-merror:0.39288\n",
      "[75]\tvalidation_0-mlogloss:0.68450\tvalidation_0-merror:0.39091\tvalidation_1-mlogloss:0.68653\tvalidation_1-merror:0.39264\n",
      "[76]\tvalidation_0-mlogloss:0.68362\tvalidation_0-merror:0.39078\tvalidation_1-mlogloss:0.68568\tvalidation_1-merror:0.39239\n",
      "[77]\tvalidation_0-mlogloss:0.68281\tvalidation_0-merror:0.39066\tvalidation_1-mlogloss:0.68489\tvalidation_1-merror:0.39230\n",
      "[78]\tvalidation_0-mlogloss:0.68201\tvalidation_0-merror:0.39056\tvalidation_1-mlogloss:0.68411\tvalidation_1-merror:0.39245\n",
      "[79]\tvalidation_0-mlogloss:0.68123\tvalidation_0-merror:0.39039\tvalidation_1-mlogloss:0.68336\tvalidation_1-merror:0.39239\n",
      "[80]\tvalidation_0-mlogloss:0.68049\tvalidation_0-merror:0.39026\tvalidation_1-mlogloss:0.68264\tvalidation_1-merror:0.39227\n",
      "[81]\tvalidation_0-mlogloss:0.67975\tvalidation_0-merror:0.39019\tvalidation_1-mlogloss:0.68193\tvalidation_1-merror:0.39227\n",
      "[82]\tvalidation_0-mlogloss:0.67903\tvalidation_0-merror:0.38991\tvalidation_1-mlogloss:0.68125\tvalidation_1-merror:0.39196\n",
      "[83]\tvalidation_0-mlogloss:0.67833\tvalidation_0-merror:0.38982\tvalidation_1-mlogloss:0.68057\tvalidation_1-merror:0.39191\n",
      "[84]\tvalidation_0-mlogloss:0.67765\tvalidation_0-merror:0.38960\tvalidation_1-mlogloss:0.67992\tvalidation_1-merror:0.39177\n",
      "[85]\tvalidation_0-mlogloss:0.67697\tvalidation_0-merror:0.38942\tvalidation_1-mlogloss:0.67927\tvalidation_1-merror:0.39168\n",
      "[86]\tvalidation_0-mlogloss:0.67632\tvalidation_0-merror:0.38923\tvalidation_1-mlogloss:0.67866\tvalidation_1-merror:0.39146\n",
      "[87]\tvalidation_0-mlogloss:0.67570\tvalidation_0-merror:0.38900\tvalidation_1-mlogloss:0.67807\tvalidation_1-merror:0.39143\n",
      "[88]\tvalidation_0-mlogloss:0.67509\tvalidation_0-merror:0.38889\tvalidation_1-mlogloss:0.67748\tvalidation_1-merror:0.39124\n",
      "[89]\tvalidation_0-mlogloss:0.67450\tvalidation_0-merror:0.38873\tvalidation_1-mlogloss:0.67692\tvalidation_1-merror:0.39117\n",
      "[90]\tvalidation_0-mlogloss:0.67395\tvalidation_0-merror:0.38859\tvalidation_1-mlogloss:0.67640\tvalidation_1-merror:0.39117\n",
      "[91]\tvalidation_0-mlogloss:0.67337\tvalidation_0-merror:0.38836\tvalidation_1-mlogloss:0.67584\tvalidation_1-merror:0.39101\n",
      "[92]\tvalidation_0-mlogloss:0.67283\tvalidation_0-merror:0.38829\tvalidation_1-mlogloss:0.67532\tvalidation_1-merror:0.39078\n",
      "[93]\tvalidation_0-mlogloss:0.67231\tvalidation_0-merror:0.38819\tvalidation_1-mlogloss:0.67482\tvalidation_1-merror:0.39071\n",
      "[94]\tvalidation_0-mlogloss:0.67178\tvalidation_0-merror:0.38809\tvalidation_1-mlogloss:0.67432\tvalidation_1-merror:0.39043\n",
      "[95]\tvalidation_0-mlogloss:0.67126\tvalidation_0-merror:0.38793\tvalidation_1-mlogloss:0.67383\tvalidation_1-merror:0.39034\n",
      "[96]\tvalidation_0-mlogloss:0.67074\tvalidation_0-merror:0.38767\tvalidation_1-mlogloss:0.67335\tvalidation_1-merror:0.39035\n",
      "[97]\tvalidation_0-mlogloss:0.67027\tvalidation_0-merror:0.38753\tvalidation_1-mlogloss:0.67290\tvalidation_1-merror:0.39016\n",
      "[98]\tvalidation_0-mlogloss:0.66981\tvalidation_0-merror:0.38745\tvalidation_1-mlogloss:0.67246\tvalidation_1-merror:0.38989\n",
      "[99]\tvalidation_0-mlogloss:0.66936\tvalidation_0-merror:0.38733\tvalidation_1-mlogloss:0.67203\tvalidation_1-merror:0.38996\n",
      "[100]\tvalidation_0-mlogloss:0.66891\tvalidation_0-merror:0.38714\tvalidation_1-mlogloss:0.67160\tvalidation_1-merror:0.38977\n",
      "[101]\tvalidation_0-mlogloss:0.66849\tvalidation_0-merror:0.38702\tvalidation_1-mlogloss:0.67121\tvalidation_1-merror:0.38968\n",
      "[102]\tvalidation_0-mlogloss:0.66808\tvalidation_0-merror:0.38688\tvalidation_1-mlogloss:0.67081\tvalidation_1-merror:0.38959\n",
      "[103]\tvalidation_0-mlogloss:0.66767\tvalidation_0-merror:0.38675\tvalidation_1-mlogloss:0.67044\tvalidation_1-merror:0.38967\n",
      "[104]\tvalidation_0-mlogloss:0.66727\tvalidation_0-merror:0.38659\tvalidation_1-mlogloss:0.67006\tvalidation_1-merror:0.38965\n",
      "[105]\tvalidation_0-mlogloss:0.66688\tvalidation_0-merror:0.38645\tvalidation_1-mlogloss:0.66970\tvalidation_1-merror:0.38938\n",
      "[106]\tvalidation_0-mlogloss:0.66649\tvalidation_0-merror:0.38646\tvalidation_1-mlogloss:0.66933\tvalidation_1-merror:0.38923\n",
      "[107]\tvalidation_0-mlogloss:0.66610\tvalidation_0-merror:0.38627\tvalidation_1-mlogloss:0.66896\tvalidation_1-merror:0.38925\n",
      "[108]\tvalidation_0-mlogloss:0.66576\tvalidation_0-merror:0.38621\tvalidation_1-mlogloss:0.66864\tvalidation_1-merror:0.38902\n",
      "[109]\tvalidation_0-mlogloss:0.66540\tvalidation_0-merror:0.38600\tvalidation_1-mlogloss:0.66831\tvalidation_1-merror:0.38886\n",
      "[110]\tvalidation_0-mlogloss:0.66500\tvalidation_0-merror:0.38574\tvalidation_1-mlogloss:0.66795\tvalidation_1-merror:0.38869\n",
      "[111]\tvalidation_0-mlogloss:0.66464\tvalidation_0-merror:0.38563\tvalidation_1-mlogloss:0.66762\tvalidation_1-merror:0.38848\n",
      "[112]\tvalidation_0-mlogloss:0.66431\tvalidation_0-merror:0.38548\tvalidation_1-mlogloss:0.66732\tvalidation_1-merror:0.38849\n",
      "[113]\tvalidation_0-mlogloss:0.66397\tvalidation_0-merror:0.38535\tvalidation_1-mlogloss:0.66702\tvalidation_1-merror:0.38849\n",
      "[114]\tvalidation_0-mlogloss:0.66364\tvalidation_0-merror:0.38512\tvalidation_1-mlogloss:0.66672\tvalidation_1-merror:0.38845\n",
      "[115]\tvalidation_0-mlogloss:0.66332\tvalidation_0-merror:0.38497\tvalidation_1-mlogloss:0.66643\tvalidation_1-merror:0.38830\n",
      "[116]\tvalidation_0-mlogloss:0.66301\tvalidation_0-merror:0.38489\tvalidation_1-mlogloss:0.66614\tvalidation_1-merror:0.38834\n",
      "[117]\tvalidation_0-mlogloss:0.66273\tvalidation_0-merror:0.38479\tvalidation_1-mlogloss:0.66588\tvalidation_1-merror:0.38826\n",
      "[118]\tvalidation_0-mlogloss:0.66246\tvalidation_0-merror:0.38462\tvalidation_1-mlogloss:0.66564\tvalidation_1-merror:0.38827\n",
      "[119]\tvalidation_0-mlogloss:0.66220\tvalidation_0-merror:0.38457\tvalidation_1-mlogloss:0.66540\tvalidation_1-merror:0.38816\n",
      "[120]\tvalidation_0-mlogloss:0.66194\tvalidation_0-merror:0.38443\tvalidation_1-mlogloss:0.66517\tvalidation_1-merror:0.38804\n",
      "[121]\tvalidation_0-mlogloss:0.66168\tvalidation_0-merror:0.38438\tvalidation_1-mlogloss:0.66493\tvalidation_1-merror:0.38805\n",
      "[122]\tvalidation_0-mlogloss:0.66142\tvalidation_0-merror:0.38424\tvalidation_1-mlogloss:0.66469\tvalidation_1-merror:0.38799\n",
      "[123]\tvalidation_0-mlogloss:0.66117\tvalidation_0-merror:0.38417\tvalidation_1-mlogloss:0.66446\tvalidation_1-merror:0.38789\n",
      "[124]\tvalidation_0-mlogloss:0.66091\tvalidation_0-merror:0.38404\tvalidation_1-mlogloss:0.66423\tvalidation_1-merror:0.38791\n",
      "[125]\tvalidation_0-mlogloss:0.66064\tvalidation_0-merror:0.38396\tvalidation_1-mlogloss:0.66399\tvalidation_1-merror:0.38791\n",
      "[126]\tvalidation_0-mlogloss:0.66041\tvalidation_0-merror:0.38383\tvalidation_1-mlogloss:0.66379\tvalidation_1-merror:0.38773\n",
      "[127]\tvalidation_0-mlogloss:0.66017\tvalidation_0-merror:0.38377\tvalidation_1-mlogloss:0.66358\tvalidation_1-merror:0.38767\n",
      "[128]\tvalidation_0-mlogloss:0.65985\tvalidation_0-merror:0.38353\tvalidation_1-mlogloss:0.66329\tvalidation_1-merror:0.38744\n",
      "[129]\tvalidation_0-mlogloss:0.65962\tvalidation_0-merror:0.38338\tvalidation_1-mlogloss:0.66308\tvalidation_1-merror:0.38747\n",
      "[130]\tvalidation_0-mlogloss:0.65940\tvalidation_0-merror:0.38330\tvalidation_1-mlogloss:0.66288\tvalidation_1-merror:0.38751\n",
      "[131]\tvalidation_0-mlogloss:0.65917\tvalidation_0-merror:0.38310\tvalidation_1-mlogloss:0.66268\tvalidation_1-merror:0.38761\n",
      "[132]\tvalidation_0-mlogloss:0.65897\tvalidation_0-merror:0.38303\tvalidation_1-mlogloss:0.66250\tvalidation_1-merror:0.38753\n",
      "[133]\tvalidation_0-mlogloss:0.65876\tvalidation_0-merror:0.38297\tvalidation_1-mlogloss:0.66232\tvalidation_1-merror:0.38738\n",
      "[134]\tvalidation_0-mlogloss:0.65857\tvalidation_0-merror:0.38286\tvalidation_1-mlogloss:0.66216\tvalidation_1-merror:0.38729\n",
      "[135]\tvalidation_0-mlogloss:0.65836\tvalidation_0-merror:0.38274\tvalidation_1-mlogloss:0.66198\tvalidation_1-merror:0.38717\n",
      "[136]\tvalidation_0-mlogloss:0.65817\tvalidation_0-merror:0.38269\tvalidation_1-mlogloss:0.66180\tvalidation_1-merror:0.38713\n",
      "[137]\tvalidation_0-mlogloss:0.65798\tvalidation_0-merror:0.38262\tvalidation_1-mlogloss:0.66164\tvalidation_1-merror:0.38716\n",
      "[138]\tvalidation_0-mlogloss:0.65778\tvalidation_0-merror:0.38248\tvalidation_1-mlogloss:0.66145\tvalidation_1-merror:0.38703\n",
      "[139]\tvalidation_0-mlogloss:0.65758\tvalidation_0-merror:0.38234\tvalidation_1-mlogloss:0.66128\tvalidation_1-merror:0.38702\n",
      "[140]\tvalidation_0-mlogloss:0.65741\tvalidation_0-merror:0.38225\tvalidation_1-mlogloss:0.66113\tvalidation_1-merror:0.38686\n",
      "[141]\tvalidation_0-mlogloss:0.65722\tvalidation_0-merror:0.38207\tvalidation_1-mlogloss:0.66098\tvalidation_1-merror:0.38684\n",
      "[142]\tvalidation_0-mlogloss:0.65705\tvalidation_0-merror:0.38201\tvalidation_1-mlogloss:0.66082\tvalidation_1-merror:0.38675\n",
      "[143]\tvalidation_0-mlogloss:0.65688\tvalidation_0-merror:0.38196\tvalidation_1-mlogloss:0.66067\tvalidation_1-merror:0.38675\n",
      "[144]\tvalidation_0-mlogloss:0.65667\tvalidation_0-merror:0.38181\tvalidation_1-mlogloss:0.66050\tvalidation_1-merror:0.38672\n",
      "[145]\tvalidation_0-mlogloss:0.65651\tvalidation_0-merror:0.38176\tvalidation_1-mlogloss:0.66036\tvalidation_1-merror:0.38655\n",
      "[146]\tvalidation_0-mlogloss:0.65634\tvalidation_0-merror:0.38163\tvalidation_1-mlogloss:0.66021\tvalidation_1-merror:0.38653\n",
      "[147]\tvalidation_0-mlogloss:0.65619\tvalidation_0-merror:0.38160\tvalidation_1-mlogloss:0.66008\tvalidation_1-merror:0.38659\n",
      "[148]\tvalidation_0-mlogloss:0.65602\tvalidation_0-merror:0.38152\tvalidation_1-mlogloss:0.65994\tvalidation_1-merror:0.38648\n",
      "[149]\tvalidation_0-mlogloss:0.65589\tvalidation_0-merror:0.38146\tvalidation_1-mlogloss:0.65983\tvalidation_1-merror:0.38652\n",
      "[150]\tvalidation_0-mlogloss:0.65575\tvalidation_0-merror:0.38135\tvalidation_1-mlogloss:0.65971\tvalidation_1-merror:0.38644\n",
      "[151]\tvalidation_0-mlogloss:0.65558\tvalidation_0-merror:0.38131\tvalidation_1-mlogloss:0.65958\tvalidation_1-merror:0.38638\n",
      "[152]\tvalidation_0-mlogloss:0.65544\tvalidation_0-merror:0.38114\tvalidation_1-mlogloss:0.65946\tvalidation_1-merror:0.38621\n",
      "[153]\tvalidation_0-mlogloss:0.65530\tvalidation_0-merror:0.38110\tvalidation_1-mlogloss:0.65935\tvalidation_1-merror:0.38614\n",
      "[154]\tvalidation_0-mlogloss:0.65516\tvalidation_0-merror:0.38101\tvalidation_1-mlogloss:0.65925\tvalidation_1-merror:0.38618\n",
      "[155]\tvalidation_0-mlogloss:0.65502\tvalidation_0-merror:0.38091\tvalidation_1-mlogloss:0.65913\tvalidation_1-merror:0.38623\n",
      "[156]\tvalidation_0-mlogloss:0.65486\tvalidation_0-merror:0.38078\tvalidation_1-mlogloss:0.65900\tvalidation_1-merror:0.38608\n",
      "[157]\tvalidation_0-mlogloss:0.65471\tvalidation_0-merror:0.38069\tvalidation_1-mlogloss:0.65887\tvalidation_1-merror:0.38600\n",
      "[158]\tvalidation_0-mlogloss:0.65457\tvalidation_0-merror:0.38060\tvalidation_1-mlogloss:0.65876\tvalidation_1-merror:0.38601\n",
      "[159]\tvalidation_0-mlogloss:0.65444\tvalidation_0-merror:0.38054\tvalidation_1-mlogloss:0.65865\tvalidation_1-merror:0.38602\n",
      "[160]\tvalidation_0-mlogloss:0.65433\tvalidation_0-merror:0.38042\tvalidation_1-mlogloss:0.65856\tvalidation_1-merror:0.38597\n",
      "[161]\tvalidation_0-mlogloss:0.65420\tvalidation_0-merror:0.38033\tvalidation_1-mlogloss:0.65845\tvalidation_1-merror:0.38592\n",
      "[162]\tvalidation_0-mlogloss:0.65407\tvalidation_0-merror:0.38028\tvalidation_1-mlogloss:0.65835\tvalidation_1-merror:0.38591\n",
      "[163]\tvalidation_0-mlogloss:0.65396\tvalidation_0-merror:0.38021\tvalidation_1-mlogloss:0.65826\tvalidation_1-merror:0.38578\n",
      "[164]\tvalidation_0-mlogloss:0.65384\tvalidation_0-merror:0.38007\tvalidation_1-mlogloss:0.65816\tvalidation_1-merror:0.38579\n",
      "[165]\tvalidation_0-mlogloss:0.65372\tvalidation_0-merror:0.38006\tvalidation_1-mlogloss:0.65807\tvalidation_1-merror:0.38584\n",
      "[166]\tvalidation_0-mlogloss:0.65357\tvalidation_0-merror:0.37987\tvalidation_1-mlogloss:0.65794\tvalidation_1-merror:0.38579\n",
      "[167]\tvalidation_0-mlogloss:0.65344\tvalidation_0-merror:0.37970\tvalidation_1-mlogloss:0.65784\tvalidation_1-merror:0.38566\n",
      "[168]\tvalidation_0-mlogloss:0.65332\tvalidation_0-merror:0.37961\tvalidation_1-mlogloss:0.65775\tvalidation_1-merror:0.38548\n",
      "[169]\tvalidation_0-mlogloss:0.65321\tvalidation_0-merror:0.37946\tvalidation_1-mlogloss:0.65766\tvalidation_1-merror:0.38541\n",
      "[170]\tvalidation_0-mlogloss:0.65308\tvalidation_0-merror:0.37940\tvalidation_1-mlogloss:0.65756\tvalidation_1-merror:0.38547\n",
      "[171]\tvalidation_0-mlogloss:0.65297\tvalidation_0-merror:0.37932\tvalidation_1-mlogloss:0.65747\tvalidation_1-merror:0.38535\n",
      "[172]\tvalidation_0-mlogloss:0.65284\tvalidation_0-merror:0.37926\tvalidation_1-mlogloss:0.65737\tvalidation_1-merror:0.38523\n",
      "[173]\tvalidation_0-mlogloss:0.65275\tvalidation_0-merror:0.37923\tvalidation_1-mlogloss:0.65730\tvalidation_1-merror:0.38521\n",
      "[174]\tvalidation_0-mlogloss:0.65266\tvalidation_0-merror:0.37914\tvalidation_1-mlogloss:0.65723\tvalidation_1-merror:0.38520\n",
      "[175]\tvalidation_0-mlogloss:0.65255\tvalidation_0-merror:0.37910\tvalidation_1-mlogloss:0.65714\tvalidation_1-merror:0.38515\n",
      "[176]\tvalidation_0-mlogloss:0.65243\tvalidation_0-merror:0.37900\tvalidation_1-mlogloss:0.65705\tvalidation_1-merror:0.38507\n",
      "[177]\tvalidation_0-mlogloss:0.65230\tvalidation_0-merror:0.37877\tvalidation_1-mlogloss:0.65694\tvalidation_1-merror:0.38508\n",
      "[178]\tvalidation_0-mlogloss:0.65219\tvalidation_0-merror:0.37872\tvalidation_1-mlogloss:0.65686\tvalidation_1-merror:0.38498\n",
      "[179]\tvalidation_0-mlogloss:0.65207\tvalidation_0-merror:0.37862\tvalidation_1-mlogloss:0.65677\tvalidation_1-merror:0.38505\n",
      "[180]\tvalidation_0-mlogloss:0.65197\tvalidation_0-merror:0.37852\tvalidation_1-mlogloss:0.65669\tvalidation_1-merror:0.38498\n",
      "[181]\tvalidation_0-mlogloss:0.65186\tvalidation_0-merror:0.37851\tvalidation_1-mlogloss:0.65661\tvalidation_1-merror:0.38480\n",
      "[182]\tvalidation_0-mlogloss:0.65176\tvalidation_0-merror:0.37836\tvalidation_1-mlogloss:0.65654\tvalidation_1-merror:0.38469\n",
      "[183]\tvalidation_0-mlogloss:0.65165\tvalidation_0-merror:0.37831\tvalidation_1-mlogloss:0.65645\tvalidation_1-merror:0.38469\n",
      "[184]\tvalidation_0-mlogloss:0.65157\tvalidation_0-merror:0.37832\tvalidation_1-mlogloss:0.65639\tvalidation_1-merror:0.38455\n",
      "[185]\tvalidation_0-mlogloss:0.65147\tvalidation_0-merror:0.37822\tvalidation_1-mlogloss:0.65632\tvalidation_1-merror:0.38463\n",
      "[186]\tvalidation_0-mlogloss:0.65138\tvalidation_0-merror:0.37813\tvalidation_1-mlogloss:0.65626\tvalidation_1-merror:0.38445\n",
      "[187]\tvalidation_0-mlogloss:0.65129\tvalidation_0-merror:0.37806\tvalidation_1-mlogloss:0.65619\tvalidation_1-merror:0.38440\n",
      "[188]\tvalidation_0-mlogloss:0.65120\tvalidation_0-merror:0.37805\tvalidation_1-mlogloss:0.65611\tvalidation_1-merror:0.38437\n",
      "[189]\tvalidation_0-mlogloss:0.65110\tvalidation_0-merror:0.37790\tvalidation_1-mlogloss:0.65604\tvalidation_1-merror:0.38430\n",
      "[190]\tvalidation_0-mlogloss:0.65101\tvalidation_0-merror:0.37786\tvalidation_1-mlogloss:0.65597\tvalidation_1-merror:0.38426\n",
      "[191]\tvalidation_0-mlogloss:0.65092\tvalidation_0-merror:0.37781\tvalidation_1-mlogloss:0.65590\tvalidation_1-merror:0.38418\n",
      "[192]\tvalidation_0-mlogloss:0.65084\tvalidation_0-merror:0.37772\tvalidation_1-mlogloss:0.65584\tvalidation_1-merror:0.38413\n",
      "[193]\tvalidation_0-mlogloss:0.65076\tvalidation_0-merror:0.37771\tvalidation_1-mlogloss:0.65578\tvalidation_1-merror:0.38429\n",
      "[194]\tvalidation_0-mlogloss:0.65068\tvalidation_0-merror:0.37766\tvalidation_1-mlogloss:0.65572\tvalidation_1-merror:0.38418\n",
      "[195]\tvalidation_0-mlogloss:0.65060\tvalidation_0-merror:0.37756\tvalidation_1-mlogloss:0.65566\tvalidation_1-merror:0.38412\n",
      "[196]\tvalidation_0-mlogloss:0.65050\tvalidation_0-merror:0.37741\tvalidation_1-mlogloss:0.65558\tvalidation_1-merror:0.38399\n",
      "[197]\tvalidation_0-mlogloss:0.65040\tvalidation_0-merror:0.37729\tvalidation_1-mlogloss:0.65551\tvalidation_1-merror:0.38390\n",
      "[198]\tvalidation_0-mlogloss:0.65033\tvalidation_0-merror:0.37721\tvalidation_1-mlogloss:0.65545\tvalidation_1-merror:0.38392\n",
      "[199]\tvalidation_0-mlogloss:0.65025\tvalidation_0-merror:0.37712\tvalidation_1-mlogloss:0.65540\tvalidation_1-merror:0.38382\n",
      "[200]\tvalidation_0-mlogloss:0.65015\tvalidation_0-merror:0.37701\tvalidation_1-mlogloss:0.65532\tvalidation_1-merror:0.38377\n",
      "[201]\tvalidation_0-mlogloss:0.65008\tvalidation_0-merror:0.37692\tvalidation_1-mlogloss:0.65527\tvalidation_1-merror:0.38386\n",
      "[202]\tvalidation_0-mlogloss:0.64999\tvalidation_0-merror:0.37682\tvalidation_1-mlogloss:0.65521\tvalidation_1-merror:0.38384\n",
      "[203]\tvalidation_0-mlogloss:0.64990\tvalidation_0-merror:0.37674\tvalidation_1-mlogloss:0.65515\tvalidation_1-merror:0.38384\n",
      "[204]\tvalidation_0-mlogloss:0.64983\tvalidation_0-merror:0.37665\tvalidation_1-mlogloss:0.65510\tvalidation_1-merror:0.38378\n",
      "[205]\tvalidation_0-mlogloss:0.64975\tvalidation_0-merror:0.37656\tvalidation_1-mlogloss:0.65504\tvalidation_1-merror:0.38368\n",
      "[206]\tvalidation_0-mlogloss:0.64966\tvalidation_0-merror:0.37650\tvalidation_1-mlogloss:0.65498\tvalidation_1-merror:0.38357\n",
      "[207]\tvalidation_0-mlogloss:0.64957\tvalidation_0-merror:0.37640\tvalidation_1-mlogloss:0.65492\tvalidation_1-merror:0.38352\n",
      "[208]\tvalidation_0-mlogloss:0.64948\tvalidation_0-merror:0.37635\tvalidation_1-mlogloss:0.65485\tvalidation_1-merror:0.38344\n",
      "[209]\tvalidation_0-mlogloss:0.64939\tvalidation_0-merror:0.37627\tvalidation_1-mlogloss:0.65478\tvalidation_1-merror:0.38333\n",
      "[210]\tvalidation_0-mlogloss:0.64931\tvalidation_0-merror:0.37612\tvalidation_1-mlogloss:0.65472\tvalidation_1-merror:0.38338\n",
      "[211]\tvalidation_0-mlogloss:0.64923\tvalidation_0-merror:0.37608\tvalidation_1-mlogloss:0.65467\tvalidation_1-merror:0.38330\n",
      "[212]\tvalidation_0-mlogloss:0.64915\tvalidation_0-merror:0.37601\tvalidation_1-mlogloss:0.65460\tvalidation_1-merror:0.38327\n",
      "[213]\tvalidation_0-mlogloss:0.64909\tvalidation_0-merror:0.37595\tvalidation_1-mlogloss:0.65457\tvalidation_1-merror:0.38318\n",
      "[214]\tvalidation_0-mlogloss:0.64900\tvalidation_0-merror:0.37586\tvalidation_1-mlogloss:0.65451\tvalidation_1-merror:0.38313\n",
      "[215]\tvalidation_0-mlogloss:0.64894\tvalidation_0-merror:0.37583\tvalidation_1-mlogloss:0.65447\tvalidation_1-merror:0.38318\n",
      "[216]\tvalidation_0-mlogloss:0.64886\tvalidation_0-merror:0.37577\tvalidation_1-mlogloss:0.65441\tvalidation_1-merror:0.38306\n",
      "[217]\tvalidation_0-mlogloss:0.64880\tvalidation_0-merror:0.37568\tvalidation_1-mlogloss:0.65437\tvalidation_1-merror:0.38300\n",
      "[218]\tvalidation_0-mlogloss:0.64873\tvalidation_0-merror:0.37556\tvalidation_1-mlogloss:0.65433\tvalidation_1-merror:0.38300\n",
      "[219]\tvalidation_0-mlogloss:0.64865\tvalidation_0-merror:0.37549\tvalidation_1-mlogloss:0.65427\tvalidation_1-merror:0.38292\n",
      "[220]\tvalidation_0-mlogloss:0.64857\tvalidation_0-merror:0.37545\tvalidation_1-mlogloss:0.65422\tvalidation_1-merror:0.38275\n",
      "[221]\tvalidation_0-mlogloss:0.64847\tvalidation_0-merror:0.37535\tvalidation_1-mlogloss:0.65416\tvalidation_1-merror:0.38280\n",
      "[222]\tvalidation_0-mlogloss:0.64840\tvalidation_0-merror:0.37524\tvalidation_1-mlogloss:0.65411\tvalidation_1-merror:0.38266\n",
      "[223]\tvalidation_0-mlogloss:0.64833\tvalidation_0-merror:0.37517\tvalidation_1-mlogloss:0.65406\tvalidation_1-merror:0.38263\n",
      "[224]\tvalidation_0-mlogloss:0.64824\tvalidation_0-merror:0.37512\tvalidation_1-mlogloss:0.65399\tvalidation_1-merror:0.38251\n",
      "[225]\tvalidation_0-mlogloss:0.64815\tvalidation_0-merror:0.37507\tvalidation_1-mlogloss:0.65394\tvalidation_1-merror:0.38250\n",
      "[226]\tvalidation_0-mlogloss:0.64809\tvalidation_0-merror:0.37506\tvalidation_1-mlogloss:0.65389\tvalidation_1-merror:0.38263\n",
      "[227]\tvalidation_0-mlogloss:0.64803\tvalidation_0-merror:0.37491\tvalidation_1-mlogloss:0.65385\tvalidation_1-merror:0.38249\n",
      "[228]\tvalidation_0-mlogloss:0.64797\tvalidation_0-merror:0.37488\tvalidation_1-mlogloss:0.65382\tvalidation_1-merror:0.38243\n",
      "[229]\tvalidation_0-mlogloss:0.64790\tvalidation_0-merror:0.37479\tvalidation_1-mlogloss:0.65378\tvalidation_1-merror:0.38233\n",
      "[230]\tvalidation_0-mlogloss:0.64782\tvalidation_0-merror:0.37471\tvalidation_1-mlogloss:0.65373\tvalidation_1-merror:0.38231\n",
      "[231]\tvalidation_0-mlogloss:0.64775\tvalidation_0-merror:0.37463\tvalidation_1-mlogloss:0.65367\tvalidation_1-merror:0.38222\n",
      "[232]\tvalidation_0-mlogloss:0.64767\tvalidation_0-merror:0.37448\tvalidation_1-mlogloss:0.65362\tvalidation_1-merror:0.38217\n",
      "[233]\tvalidation_0-mlogloss:0.64759\tvalidation_0-merror:0.37442\tvalidation_1-mlogloss:0.65357\tvalidation_1-merror:0.38205\n",
      "[234]\tvalidation_0-mlogloss:0.64752\tvalidation_0-merror:0.37432\tvalidation_1-mlogloss:0.65351\tvalidation_1-merror:0.38212\n",
      "[235]\tvalidation_0-mlogloss:0.64746\tvalidation_0-merror:0.37423\tvalidation_1-mlogloss:0.65348\tvalidation_1-merror:0.38227\n",
      "[236]\tvalidation_0-mlogloss:0.64740\tvalidation_0-merror:0.37414\tvalidation_1-mlogloss:0.65345\tvalidation_1-merror:0.38212\n",
      "[237]\tvalidation_0-mlogloss:0.64735\tvalidation_0-merror:0.37405\tvalidation_1-mlogloss:0.65342\tvalidation_1-merror:0.38205\n",
      "[238]\tvalidation_0-mlogloss:0.64729\tvalidation_0-merror:0.37401\tvalidation_1-mlogloss:0.65338\tvalidation_1-merror:0.38197\n",
      "[239]\tvalidation_0-mlogloss:0.64722\tvalidation_0-merror:0.37397\tvalidation_1-mlogloss:0.65334\tvalidation_1-merror:0.38198\n",
      "[240]\tvalidation_0-mlogloss:0.64716\tvalidation_0-merror:0.37388\tvalidation_1-mlogloss:0.65330\tvalidation_1-merror:0.38186\n",
      "[241]\tvalidation_0-mlogloss:0.64710\tvalidation_0-merror:0.37383\tvalidation_1-mlogloss:0.65327\tvalidation_1-merror:0.38195\n",
      "[242]\tvalidation_0-mlogloss:0.64703\tvalidation_0-merror:0.37376\tvalidation_1-mlogloss:0.65323\tvalidation_1-merror:0.38191\n",
      "[243]\tvalidation_0-mlogloss:0.64695\tvalidation_0-merror:0.37373\tvalidation_1-mlogloss:0.65318\tvalidation_1-merror:0.38182\n",
      "[244]\tvalidation_0-mlogloss:0.64688\tvalidation_0-merror:0.37361\tvalidation_1-mlogloss:0.65313\tvalidation_1-merror:0.38180\n",
      "[245]\tvalidation_0-mlogloss:0.64681\tvalidation_0-merror:0.37356\tvalidation_1-mlogloss:0.65309\tvalidation_1-merror:0.38185\n",
      "[246]\tvalidation_0-mlogloss:0.64671\tvalidation_0-merror:0.37343\tvalidation_1-mlogloss:0.65302\tvalidation_1-merror:0.38152\n",
      "[247]\tvalidation_0-mlogloss:0.64664\tvalidation_0-merror:0.37343\tvalidation_1-mlogloss:0.65298\tvalidation_1-merror:0.38153\n",
      "[248]\tvalidation_0-mlogloss:0.64656\tvalidation_0-merror:0.37329\tvalidation_1-mlogloss:0.65293\tvalidation_1-merror:0.38145\n",
      "[249]\tvalidation_0-mlogloss:0.64650\tvalidation_0-merror:0.37319\tvalidation_1-mlogloss:0.65290\tvalidation_1-merror:0.38150\n",
      "[250]\tvalidation_0-mlogloss:0.64643\tvalidation_0-merror:0.37310\tvalidation_1-mlogloss:0.65285\tvalidation_1-merror:0.38142\n",
      "[251]\tvalidation_0-mlogloss:0.64635\tvalidation_0-merror:0.37305\tvalidation_1-mlogloss:0.65281\tvalidation_1-merror:0.38131\n",
      "[252]\tvalidation_0-mlogloss:0.64627\tvalidation_0-merror:0.37293\tvalidation_1-mlogloss:0.65275\tvalidation_1-merror:0.38132\n",
      "[253]\tvalidation_0-mlogloss:0.64619\tvalidation_0-merror:0.37284\tvalidation_1-mlogloss:0.65271\tvalidation_1-merror:0.38125\n",
      "[254]\tvalidation_0-mlogloss:0.64612\tvalidation_0-merror:0.37279\tvalidation_1-mlogloss:0.65267\tvalidation_1-merror:0.38127\n",
      "[255]\tvalidation_0-mlogloss:0.64604\tvalidation_0-merror:0.37273\tvalidation_1-mlogloss:0.65261\tvalidation_1-merror:0.38120\n",
      "[256]\tvalidation_0-mlogloss:0.64597\tvalidation_0-merror:0.37262\tvalidation_1-mlogloss:0.65256\tvalidation_1-merror:0.38124\n",
      "[257]\tvalidation_0-mlogloss:0.64589\tvalidation_0-merror:0.37253\tvalidation_1-mlogloss:0.65253\tvalidation_1-merror:0.38117\n",
      "[258]\tvalidation_0-mlogloss:0.64583\tvalidation_0-merror:0.37247\tvalidation_1-mlogloss:0.65249\tvalidation_1-merror:0.38115\n",
      "[259]\tvalidation_0-mlogloss:0.64576\tvalidation_0-merror:0.37240\tvalidation_1-mlogloss:0.65245\tvalidation_1-merror:0.38128\n",
      "[260]\tvalidation_0-mlogloss:0.64570\tvalidation_0-merror:0.37224\tvalidation_1-mlogloss:0.65242\tvalidation_1-merror:0.38122\n",
      "[261]\tvalidation_0-mlogloss:0.64565\tvalidation_0-merror:0.37224\tvalidation_1-mlogloss:0.65240\tvalidation_1-merror:0.38110\n",
      "[262]\tvalidation_0-mlogloss:0.64558\tvalidation_0-merror:0.37211\tvalidation_1-mlogloss:0.65238\tvalidation_1-merror:0.38114\n",
      "[263]\tvalidation_0-mlogloss:0.64550\tvalidation_0-merror:0.37208\tvalidation_1-mlogloss:0.65233\tvalidation_1-merror:0.38088\n",
      "[264]\tvalidation_0-mlogloss:0.64543\tvalidation_0-merror:0.37205\tvalidation_1-mlogloss:0.65230\tvalidation_1-merror:0.38090\n",
      "[265]\tvalidation_0-mlogloss:0.64537\tvalidation_0-merror:0.37197\tvalidation_1-mlogloss:0.65227\tvalidation_1-merror:0.38084\n",
      "[266]\tvalidation_0-mlogloss:0.64531\tvalidation_0-merror:0.37190\tvalidation_1-mlogloss:0.65225\tvalidation_1-merror:0.38065\n",
      "[267]\tvalidation_0-mlogloss:0.64523\tvalidation_0-merror:0.37172\tvalidation_1-mlogloss:0.65220\tvalidation_1-merror:0.38065\n",
      "[268]\tvalidation_0-mlogloss:0.64517\tvalidation_0-merror:0.37167\tvalidation_1-mlogloss:0.65217\tvalidation_1-merror:0.38073\n",
      "[269]\tvalidation_0-mlogloss:0.64511\tvalidation_0-merror:0.37161\tvalidation_1-mlogloss:0.65215\tvalidation_1-merror:0.38071\n",
      "[270]\tvalidation_0-mlogloss:0.64504\tvalidation_0-merror:0.37154\tvalidation_1-mlogloss:0.65211\tvalidation_1-merror:0.38064\n",
      "[271]\tvalidation_0-mlogloss:0.64496\tvalidation_0-merror:0.37151\tvalidation_1-mlogloss:0.65207\tvalidation_1-merror:0.38060\n",
      "[272]\tvalidation_0-mlogloss:0.64490\tvalidation_0-merror:0.37140\tvalidation_1-mlogloss:0.65204\tvalidation_1-merror:0.38068\n",
      "[273]\tvalidation_0-mlogloss:0.64483\tvalidation_0-merror:0.37130\tvalidation_1-mlogloss:0.65200\tvalidation_1-merror:0.38050\n",
      "[274]\tvalidation_0-mlogloss:0.64474\tvalidation_0-merror:0.37117\tvalidation_1-mlogloss:0.65195\tvalidation_1-merror:0.38044\n",
      "[275]\tvalidation_0-mlogloss:0.64467\tvalidation_0-merror:0.37097\tvalidation_1-mlogloss:0.65193\tvalidation_1-merror:0.38029\n",
      "[276]\tvalidation_0-mlogloss:0.64460\tvalidation_0-merror:0.37095\tvalidation_1-mlogloss:0.65189\tvalidation_1-merror:0.38022\n",
      "[277]\tvalidation_0-mlogloss:0.64453\tvalidation_0-merror:0.37089\tvalidation_1-mlogloss:0.65185\tvalidation_1-merror:0.38006\n",
      "[278]\tvalidation_0-mlogloss:0.64446\tvalidation_0-merror:0.37077\tvalidation_1-mlogloss:0.65181\tvalidation_1-merror:0.38011\n",
      "[279]\tvalidation_0-mlogloss:0.64439\tvalidation_0-merror:0.37071\tvalidation_1-mlogloss:0.65177\tvalidation_1-merror:0.38020\n",
      "[280]\tvalidation_0-mlogloss:0.64431\tvalidation_0-merror:0.37055\tvalidation_1-mlogloss:0.65174\tvalidation_1-merror:0.38005\n",
      "[281]\tvalidation_0-mlogloss:0.64424\tvalidation_0-merror:0.37046\tvalidation_1-mlogloss:0.65169\tvalidation_1-merror:0.38000\n",
      "[282]\tvalidation_0-mlogloss:0.64414\tvalidation_0-merror:0.37036\tvalidation_1-mlogloss:0.65163\tvalidation_1-merror:0.37992\n",
      "[283]\tvalidation_0-mlogloss:0.64407\tvalidation_0-merror:0.37028\tvalidation_1-mlogloss:0.65160\tvalidation_1-merror:0.37987\n",
      "[284]\tvalidation_0-mlogloss:0.64398\tvalidation_0-merror:0.37025\tvalidation_1-mlogloss:0.65154\tvalidation_1-merror:0.37962\n",
      "[285]\tvalidation_0-mlogloss:0.64390\tvalidation_0-merror:0.37019\tvalidation_1-mlogloss:0.65151\tvalidation_1-merror:0.37965\n",
      "[286]\tvalidation_0-mlogloss:0.64384\tvalidation_0-merror:0.37012\tvalidation_1-mlogloss:0.65147\tvalidation_1-merror:0.37970\n",
      "[287]\tvalidation_0-mlogloss:0.64378\tvalidation_0-merror:0.37006\tvalidation_1-mlogloss:0.65144\tvalidation_1-merror:0.37966\n",
      "[288]\tvalidation_0-mlogloss:0.64371\tvalidation_0-merror:0.36992\tvalidation_1-mlogloss:0.65141\tvalidation_1-merror:0.37957\n",
      "[289]\tvalidation_0-mlogloss:0.64365\tvalidation_0-merror:0.36984\tvalidation_1-mlogloss:0.65137\tvalidation_1-merror:0.37945\n",
      "[290]\tvalidation_0-mlogloss:0.64358\tvalidation_0-merror:0.36979\tvalidation_1-mlogloss:0.65134\tvalidation_1-merror:0.37947\n",
      "[291]\tvalidation_0-mlogloss:0.64350\tvalidation_0-merror:0.36969\tvalidation_1-mlogloss:0.65129\tvalidation_1-merror:0.37933\n",
      "[292]\tvalidation_0-mlogloss:0.64344\tvalidation_0-merror:0.36959\tvalidation_1-mlogloss:0.65126\tvalidation_1-merror:0.37933\n",
      "[293]\tvalidation_0-mlogloss:0.64337\tvalidation_0-merror:0.36941\tvalidation_1-mlogloss:0.65122\tvalidation_1-merror:0.37948\n",
      "[294]\tvalidation_0-mlogloss:0.64331\tvalidation_0-merror:0.36933\tvalidation_1-mlogloss:0.65119\tvalidation_1-merror:0.37938\n",
      "[295]\tvalidation_0-mlogloss:0.64326\tvalidation_0-merror:0.36927\tvalidation_1-mlogloss:0.65117\tvalidation_1-merror:0.37936\n",
      "[296]\tvalidation_0-mlogloss:0.64319\tvalidation_0-merror:0.36915\tvalidation_1-mlogloss:0.65113\tvalidation_1-merror:0.37926\n",
      "[297]\tvalidation_0-mlogloss:0.64312\tvalidation_0-merror:0.36913\tvalidation_1-mlogloss:0.65109\tvalidation_1-merror:0.37915\n",
      "[298]\tvalidation_0-mlogloss:0.64304\tvalidation_0-merror:0.36908\tvalidation_1-mlogloss:0.65106\tvalidation_1-merror:0.37922\n",
      "[299]\tvalidation_0-mlogloss:0.64299\tvalidation_0-merror:0.36899\tvalidation_1-mlogloss:0.65104\tvalidation_1-merror:0.37913\n",
      "[300]\tvalidation_0-mlogloss:0.64291\tvalidation_0-merror:0.36884\tvalidation_1-mlogloss:0.65099\tvalidation_1-merror:0.37903\n",
      "[301]\tvalidation_0-mlogloss:0.64285\tvalidation_0-merror:0.36877\tvalidation_1-mlogloss:0.65096\tvalidation_1-merror:0.37898\n",
      "[302]\tvalidation_0-mlogloss:0.64280\tvalidation_0-merror:0.36861\tvalidation_1-mlogloss:0.65093\tvalidation_1-merror:0.37895\n",
      "[303]\tvalidation_0-mlogloss:0.64271\tvalidation_0-merror:0.36862\tvalidation_1-mlogloss:0.65087\tvalidation_1-merror:0.37883\n",
      "[304]\tvalidation_0-mlogloss:0.64265\tvalidation_0-merror:0.36853\tvalidation_1-mlogloss:0.65084\tvalidation_1-merror:0.37879\n",
      "[305]\tvalidation_0-mlogloss:0.64257\tvalidation_0-merror:0.36853\tvalidation_1-mlogloss:0.65080\tvalidation_1-merror:0.37877\n",
      "[306]\tvalidation_0-mlogloss:0.64252\tvalidation_0-merror:0.36842\tvalidation_1-mlogloss:0.65078\tvalidation_1-merror:0.37889\n",
      "[307]\tvalidation_0-mlogloss:0.64245\tvalidation_0-merror:0.36841\tvalidation_1-mlogloss:0.65076\tvalidation_1-merror:0.37874\n",
      "[308]\tvalidation_0-mlogloss:0.64238\tvalidation_0-merror:0.36828\tvalidation_1-mlogloss:0.65073\tvalidation_1-merror:0.37876\n",
      "[309]\tvalidation_0-mlogloss:0.64232\tvalidation_0-merror:0.36817\tvalidation_1-mlogloss:0.65070\tvalidation_1-merror:0.37876\n",
      "[310]\tvalidation_0-mlogloss:0.64226\tvalidation_0-merror:0.36812\tvalidation_1-mlogloss:0.65068\tvalidation_1-merror:0.37856\n",
      "[311]\tvalidation_0-mlogloss:0.64220\tvalidation_0-merror:0.36804\tvalidation_1-mlogloss:0.65064\tvalidation_1-merror:0.37859\n",
      "[312]\tvalidation_0-mlogloss:0.64214\tvalidation_0-merror:0.36801\tvalidation_1-mlogloss:0.65062\tvalidation_1-merror:0.37863\n",
      "[313]\tvalidation_0-mlogloss:0.64208\tvalidation_0-merror:0.36796\tvalidation_1-mlogloss:0.65059\tvalidation_1-merror:0.37874\n",
      "[314]\tvalidation_0-mlogloss:0.64200\tvalidation_0-merror:0.36773\tvalidation_1-mlogloss:0.65055\tvalidation_1-merror:0.37869\n",
      "[315]\tvalidation_0-mlogloss:0.64194\tvalidation_0-merror:0.36773\tvalidation_1-mlogloss:0.65053\tvalidation_1-merror:0.37865\n",
      "[316]\tvalidation_0-mlogloss:0.64187\tvalidation_0-merror:0.36766\tvalidation_1-mlogloss:0.65049\tvalidation_1-merror:0.37859\n",
      "[317]\tvalidation_0-mlogloss:0.64181\tvalidation_0-merror:0.36761\tvalidation_1-mlogloss:0.65047\tvalidation_1-merror:0.37852\n",
      "[318]\tvalidation_0-mlogloss:0.64175\tvalidation_0-merror:0.36757\tvalidation_1-mlogloss:0.65044\tvalidation_1-merror:0.37860\n",
      "[319]\tvalidation_0-mlogloss:0.64168\tvalidation_0-merror:0.36743\tvalidation_1-mlogloss:0.65041\tvalidation_1-merror:0.37852\n",
      "[320]\tvalidation_0-mlogloss:0.64162\tvalidation_0-merror:0.36728\tvalidation_1-mlogloss:0.65039\tvalidation_1-merror:0.37858\n",
      "[321]\tvalidation_0-mlogloss:0.64155\tvalidation_0-merror:0.36720\tvalidation_1-mlogloss:0.65035\tvalidation_1-merror:0.37855\n",
      "[322]\tvalidation_0-mlogloss:0.64148\tvalidation_0-merror:0.36712\tvalidation_1-mlogloss:0.65032\tvalidation_1-merror:0.37848\n",
      "[323]\tvalidation_0-mlogloss:0.64142\tvalidation_0-merror:0.36702\tvalidation_1-mlogloss:0.65029\tvalidation_1-merror:0.37834\n",
      "[324]\tvalidation_0-mlogloss:0.64136\tvalidation_0-merror:0.36698\tvalidation_1-mlogloss:0.65028\tvalidation_1-merror:0.37839\n",
      "[325]\tvalidation_0-mlogloss:0.64129\tvalidation_0-merror:0.36688\tvalidation_1-mlogloss:0.65025\tvalidation_1-merror:0.37847\n",
      "[326]\tvalidation_0-mlogloss:0.64123\tvalidation_0-merror:0.36679\tvalidation_1-mlogloss:0.65021\tvalidation_1-merror:0.37835\n",
      "[327]\tvalidation_0-mlogloss:0.64118\tvalidation_0-merror:0.36678\tvalidation_1-mlogloss:0.65019\tvalidation_1-merror:0.37836\n",
      "[328]\tvalidation_0-mlogloss:0.64112\tvalidation_0-merror:0.36665\tvalidation_1-mlogloss:0.65016\tvalidation_1-merror:0.37823\n",
      "[329]\tvalidation_0-mlogloss:0.64106\tvalidation_0-merror:0.36655\tvalidation_1-mlogloss:0.65014\tvalidation_1-merror:0.37821\n",
      "[330]\tvalidation_0-mlogloss:0.64098\tvalidation_0-merror:0.36655\tvalidation_1-mlogloss:0.65010\tvalidation_1-merror:0.37826\n",
      "[331]\tvalidation_0-mlogloss:0.64091\tvalidation_0-merror:0.36644\tvalidation_1-mlogloss:0.65007\tvalidation_1-merror:0.37813\n",
      "[332]\tvalidation_0-mlogloss:0.64086\tvalidation_0-merror:0.36637\tvalidation_1-mlogloss:0.65005\tvalidation_1-merror:0.37816\n",
      "[333]\tvalidation_0-mlogloss:0.64079\tvalidation_0-merror:0.36637\tvalidation_1-mlogloss:0.65002\tvalidation_1-merror:0.37808\n",
      "[334]\tvalidation_0-mlogloss:0.64072\tvalidation_0-merror:0.36632\tvalidation_1-mlogloss:0.65000\tvalidation_1-merror:0.37802\n",
      "[335]\tvalidation_0-mlogloss:0.64066\tvalidation_0-merror:0.36613\tvalidation_1-mlogloss:0.64997\tvalidation_1-merror:0.37798\n",
      "[336]\tvalidation_0-mlogloss:0.64060\tvalidation_0-merror:0.36605\tvalidation_1-mlogloss:0.64995\tvalidation_1-merror:0.37801\n",
      "[337]\tvalidation_0-mlogloss:0.64053\tvalidation_0-merror:0.36596\tvalidation_1-mlogloss:0.64992\tvalidation_1-merror:0.37801\n",
      "[338]\tvalidation_0-mlogloss:0.64047\tvalidation_0-merror:0.36585\tvalidation_1-mlogloss:0.64989\tvalidation_1-merror:0.37804\n",
      "[339]\tvalidation_0-mlogloss:0.64041\tvalidation_0-merror:0.36585\tvalidation_1-mlogloss:0.64985\tvalidation_1-merror:0.37804\n",
      "[340]\tvalidation_0-mlogloss:0.64033\tvalidation_0-merror:0.36573\tvalidation_1-mlogloss:0.64981\tvalidation_1-merror:0.37792\n",
      "[341]\tvalidation_0-mlogloss:0.64025\tvalidation_0-merror:0.36559\tvalidation_1-mlogloss:0.64977\tvalidation_1-merror:0.37790\n",
      "[342]\tvalidation_0-mlogloss:0.64019\tvalidation_0-merror:0.36549\tvalidation_1-mlogloss:0.64974\tvalidation_1-merror:0.37795\n",
      "[343]\tvalidation_0-mlogloss:0.64013\tvalidation_0-merror:0.36537\tvalidation_1-mlogloss:0.64972\tvalidation_1-merror:0.37784\n",
      "[344]\tvalidation_0-mlogloss:0.64007\tvalidation_0-merror:0.36531\tvalidation_1-mlogloss:0.64969\tvalidation_1-merror:0.37784\n",
      "[345]\tvalidation_0-mlogloss:0.64001\tvalidation_0-merror:0.36519\tvalidation_1-mlogloss:0.64966\tvalidation_1-merror:0.37776\n",
      "[346]\tvalidation_0-mlogloss:0.63995\tvalidation_0-merror:0.36513\tvalidation_1-mlogloss:0.64964\tvalidation_1-merror:0.37760\n",
      "[347]\tvalidation_0-mlogloss:0.63987\tvalidation_0-merror:0.36510\tvalidation_1-mlogloss:0.64960\tvalidation_1-merror:0.37759\n",
      "[348]\tvalidation_0-mlogloss:0.63983\tvalidation_0-merror:0.36509\tvalidation_1-mlogloss:0.64959\tvalidation_1-merror:0.37769\n",
      "[349]\tvalidation_0-mlogloss:0.63978\tvalidation_0-merror:0.36499\tvalidation_1-mlogloss:0.64956\tvalidation_1-merror:0.37764\n",
      "[350]\tvalidation_0-mlogloss:0.63970\tvalidation_0-merror:0.36494\tvalidation_1-mlogloss:0.64952\tvalidation_1-merror:0.37770\n",
      "[351]\tvalidation_0-mlogloss:0.63964\tvalidation_0-merror:0.36484\tvalidation_1-mlogloss:0.64950\tvalidation_1-merror:0.37781\n",
      "[352]\tvalidation_0-mlogloss:0.63957\tvalidation_0-merror:0.36476\tvalidation_1-mlogloss:0.64947\tvalidation_1-merror:0.37771\n",
      "[353]\tvalidation_0-mlogloss:0.63951\tvalidation_0-merror:0.36473\tvalidation_1-mlogloss:0.64945\tvalidation_1-merror:0.37772\n",
      "[354]\tvalidation_0-mlogloss:0.63945\tvalidation_0-merror:0.36463\tvalidation_1-mlogloss:0.64943\tvalidation_1-merror:0.37773\n",
      "[355]\tvalidation_0-mlogloss:0.63940\tvalidation_0-merror:0.36451\tvalidation_1-mlogloss:0.64941\tvalidation_1-merror:0.37764\n",
      "[356]\tvalidation_0-mlogloss:0.63933\tvalidation_0-merror:0.36447\tvalidation_1-mlogloss:0.64938\tvalidation_1-merror:0.37765\n",
      "[357]\tvalidation_0-mlogloss:0.63928\tvalidation_0-merror:0.36440\tvalidation_1-mlogloss:0.64936\tvalidation_1-merror:0.37771\n",
      "[358]\tvalidation_0-mlogloss:0.63923\tvalidation_0-merror:0.36432\tvalidation_1-mlogloss:0.64933\tvalidation_1-merror:0.37775\n",
      "[359]\tvalidation_0-mlogloss:0.63918\tvalidation_0-merror:0.36425\tvalidation_1-mlogloss:0.64932\tvalidation_1-merror:0.37773\n",
      "[360]\tvalidation_0-mlogloss:0.63913\tvalidation_0-merror:0.36417\tvalidation_1-mlogloss:0.64931\tvalidation_1-merror:0.37778\n",
      "[361]\tvalidation_0-mlogloss:0.63908\tvalidation_0-merror:0.36413\tvalidation_1-mlogloss:0.64930\tvalidation_1-merror:0.37777\n",
      "[362]\tvalidation_0-mlogloss:0.63902\tvalidation_0-merror:0.36399\tvalidation_1-mlogloss:0.64927\tvalidation_1-merror:0.37771\n",
      "[363]\tvalidation_0-mlogloss:0.63896\tvalidation_0-merror:0.36397\tvalidation_1-mlogloss:0.64925\tvalidation_1-merror:0.37765\n",
      "[364]\tvalidation_0-mlogloss:0.63891\tvalidation_0-merror:0.36391\tvalidation_1-mlogloss:0.64923\tvalidation_1-merror:0.37760\n",
      "[365]\tvalidation_0-mlogloss:0.63886\tvalidation_0-merror:0.36388\tvalidation_1-mlogloss:0.64922\tvalidation_1-merror:0.37761\n",
      "[366]\tvalidation_0-mlogloss:0.63883\tvalidation_0-merror:0.36382\tvalidation_1-mlogloss:0.64920\tvalidation_1-merror:0.37755\n",
      "[367]\tvalidation_0-mlogloss:0.63875\tvalidation_0-merror:0.36363\tvalidation_1-mlogloss:0.64917\tvalidation_1-merror:0.37758\n",
      "[368]\tvalidation_0-mlogloss:0.63868\tvalidation_0-merror:0.36354\tvalidation_1-mlogloss:0.64914\tvalidation_1-merror:0.37744\n",
      "[369]\tvalidation_0-mlogloss:0.63862\tvalidation_0-merror:0.36353\tvalidation_1-mlogloss:0.64911\tvalidation_1-merror:0.37742\n",
      "[370]\tvalidation_0-mlogloss:0.63856\tvalidation_0-merror:0.36347\tvalidation_1-mlogloss:0.64907\tvalidation_1-merror:0.37734\n",
      "[371]\tvalidation_0-mlogloss:0.63852\tvalidation_0-merror:0.36340\tvalidation_1-mlogloss:0.64906\tvalidation_1-merror:0.37724\n",
      "[372]\tvalidation_0-mlogloss:0.63847\tvalidation_0-merror:0.36333\tvalidation_1-mlogloss:0.64905\tvalidation_1-merror:0.37726\n",
      "[373]\tvalidation_0-mlogloss:0.63842\tvalidation_0-merror:0.36324\tvalidation_1-mlogloss:0.64903\tvalidation_1-merror:0.37731\n",
      "[374]\tvalidation_0-mlogloss:0.63835\tvalidation_0-merror:0.36314\tvalidation_1-mlogloss:0.64901\tvalidation_1-merror:0.37738\n",
      "[375]\tvalidation_0-mlogloss:0.63830\tvalidation_0-merror:0.36302\tvalidation_1-mlogloss:0.64898\tvalidation_1-merror:0.37728\n",
      "[376]\tvalidation_0-mlogloss:0.63823\tvalidation_0-merror:0.36301\tvalidation_1-mlogloss:0.64895\tvalidation_1-merror:0.37728\n",
      "[377]\tvalidation_0-mlogloss:0.63815\tvalidation_0-merror:0.36281\tvalidation_1-mlogloss:0.64891\tvalidation_1-merror:0.37714\n",
      "[378]\tvalidation_0-mlogloss:0.63810\tvalidation_0-merror:0.36278\tvalidation_1-mlogloss:0.64890\tvalidation_1-merror:0.37710\n",
      "[379]\tvalidation_0-mlogloss:0.63805\tvalidation_0-merror:0.36271\tvalidation_1-mlogloss:0.64887\tvalidation_1-merror:0.37718\n",
      "[380]\tvalidation_0-mlogloss:0.63799\tvalidation_0-merror:0.36269\tvalidation_1-mlogloss:0.64886\tvalidation_1-merror:0.37729\n",
      "[381]\tvalidation_0-mlogloss:0.63795\tvalidation_0-merror:0.36259\tvalidation_1-mlogloss:0.64885\tvalidation_1-merror:0.37736\n",
      "[382]\tvalidation_0-mlogloss:0.63788\tvalidation_0-merror:0.36252\tvalidation_1-mlogloss:0.64882\tvalidation_1-merror:0.37734\n",
      "[383]\tvalidation_0-mlogloss:0.63782\tvalidation_0-merror:0.36246\tvalidation_1-mlogloss:0.64880\tvalidation_1-merror:0.37712\n",
      "[384]\tvalidation_0-mlogloss:0.63776\tvalidation_0-merror:0.36239\tvalidation_1-mlogloss:0.64878\tvalidation_1-merror:0.37715\n",
      "[385]\tvalidation_0-mlogloss:0.63771\tvalidation_0-merror:0.36230\tvalidation_1-mlogloss:0.64876\tvalidation_1-merror:0.37711\n",
      "[386]\tvalidation_0-mlogloss:0.63766\tvalidation_0-merror:0.36227\tvalidation_1-mlogloss:0.64874\tvalidation_1-merror:0.37702\n",
      "[387]\tvalidation_0-mlogloss:0.63759\tvalidation_0-merror:0.36211\tvalidation_1-mlogloss:0.64870\tvalidation_1-merror:0.37709\n",
      "[388]\tvalidation_0-mlogloss:0.63754\tvalidation_0-merror:0.36211\tvalidation_1-mlogloss:0.64867\tvalidation_1-merror:0.37698\n",
      "[389]\tvalidation_0-mlogloss:0.63747\tvalidation_0-merror:0.36193\tvalidation_1-mlogloss:0.64865\tvalidation_1-merror:0.37702\n",
      "[390]\tvalidation_0-mlogloss:0.63740\tvalidation_0-merror:0.36181\tvalidation_1-mlogloss:0.64863\tvalidation_1-merror:0.37701\n",
      "[391]\tvalidation_0-mlogloss:0.63733\tvalidation_0-merror:0.36173\tvalidation_1-mlogloss:0.64859\tvalidation_1-merror:0.37700\n",
      "[392]\tvalidation_0-mlogloss:0.63727\tvalidation_0-merror:0.36168\tvalidation_1-mlogloss:0.64857\tvalidation_1-merror:0.37700\n",
      "[393]\tvalidation_0-mlogloss:0.63721\tvalidation_0-merror:0.36161\tvalidation_1-mlogloss:0.64854\tvalidation_1-merror:0.37699\n",
      "[394]\tvalidation_0-mlogloss:0.63715\tvalidation_0-merror:0.36157\tvalidation_1-mlogloss:0.64853\tvalidation_1-merror:0.37695\n",
      "[395]\tvalidation_0-mlogloss:0.63710\tvalidation_0-merror:0.36150\tvalidation_1-mlogloss:0.64851\tvalidation_1-merror:0.37696\n",
      "[396]\tvalidation_0-mlogloss:0.63704\tvalidation_0-merror:0.36141\tvalidation_1-mlogloss:0.64848\tvalidation_1-merror:0.37686\n",
      "[397]\tvalidation_0-mlogloss:0.63698\tvalidation_0-merror:0.36137\tvalidation_1-mlogloss:0.64847\tvalidation_1-merror:0.37676\n",
      "[398]\tvalidation_0-mlogloss:0.63692\tvalidation_0-merror:0.36126\tvalidation_1-mlogloss:0.64845\tvalidation_1-merror:0.37679\n",
      "[399]\tvalidation_0-mlogloss:0.63687\tvalidation_0-merror:0.36125\tvalidation_1-mlogloss:0.64842\tvalidation_1-merror:0.37686\n",
      "[400]\tvalidation_0-mlogloss:0.63681\tvalidation_0-merror:0.36113\tvalidation_1-mlogloss:0.64840\tvalidation_1-merror:0.37687\n",
      "[401]\tvalidation_0-mlogloss:0.63676\tvalidation_0-merror:0.36109\tvalidation_1-mlogloss:0.64838\tvalidation_1-merror:0.37690\n",
      "[402]\tvalidation_0-mlogloss:0.63670\tvalidation_0-merror:0.36100\tvalidation_1-mlogloss:0.64836\tvalidation_1-merror:0.37694\n",
      "[403]\tvalidation_0-mlogloss:0.63665\tvalidation_0-merror:0.36095\tvalidation_1-mlogloss:0.64835\tvalidation_1-merror:0.37689\n",
      "[404]\tvalidation_0-mlogloss:0.63659\tvalidation_0-merror:0.36091\tvalidation_1-mlogloss:0.64832\tvalidation_1-merror:0.37679\n",
      "[405]\tvalidation_0-mlogloss:0.63653\tvalidation_0-merror:0.36081\tvalidation_1-mlogloss:0.64829\tvalidation_1-merror:0.37666\n",
      "[406]\tvalidation_0-mlogloss:0.63648\tvalidation_0-merror:0.36070\tvalidation_1-mlogloss:0.64827\tvalidation_1-merror:0.37653\n",
      "[407]\tvalidation_0-mlogloss:0.63642\tvalidation_0-merror:0.36059\tvalidation_1-mlogloss:0.64825\tvalidation_1-merror:0.37661\n",
      "[408]\tvalidation_0-mlogloss:0.63636\tvalidation_0-merror:0.36057\tvalidation_1-mlogloss:0.64822\tvalidation_1-merror:0.37650\n",
      "[409]\tvalidation_0-mlogloss:0.63630\tvalidation_0-merror:0.36048\tvalidation_1-mlogloss:0.64819\tvalidation_1-merror:0.37644\n",
      "[410]\tvalidation_0-mlogloss:0.63625\tvalidation_0-merror:0.36043\tvalidation_1-mlogloss:0.64818\tvalidation_1-merror:0.37654\n",
      "[411]\tvalidation_0-mlogloss:0.63621\tvalidation_0-merror:0.36038\tvalidation_1-mlogloss:0.64816\tvalidation_1-merror:0.37663\n",
      "[412]\tvalidation_0-mlogloss:0.63616\tvalidation_0-merror:0.36031\tvalidation_1-mlogloss:0.64815\tvalidation_1-merror:0.37652\n",
      "[413]\tvalidation_0-mlogloss:0.63612\tvalidation_0-merror:0.36028\tvalidation_1-mlogloss:0.64813\tvalidation_1-merror:0.37638\n",
      "[414]\tvalidation_0-mlogloss:0.63607\tvalidation_0-merror:0.36023\tvalidation_1-mlogloss:0.64811\tvalidation_1-merror:0.37643\n",
      "[415]\tvalidation_0-mlogloss:0.63601\tvalidation_0-merror:0.36014\tvalidation_1-mlogloss:0.64809\tvalidation_1-merror:0.37640\n",
      "[416]\tvalidation_0-mlogloss:0.63596\tvalidation_0-merror:0.36014\tvalidation_1-mlogloss:0.64807\tvalidation_1-merror:0.37640\n",
      "[417]\tvalidation_0-mlogloss:0.63591\tvalidation_0-merror:0.36002\tvalidation_1-mlogloss:0.64805\tvalidation_1-merror:0.37638\n",
      "[418]\tvalidation_0-mlogloss:0.63585\tvalidation_0-merror:0.36004\tvalidation_1-mlogloss:0.64802\tvalidation_1-merror:0.37637\n",
      "[419]\tvalidation_0-mlogloss:0.63580\tvalidation_0-merror:0.36001\tvalidation_1-mlogloss:0.64801\tvalidation_1-merror:0.37647\n",
      "[420]\tvalidation_0-mlogloss:0.63575\tvalidation_0-merror:0.36000\tvalidation_1-mlogloss:0.64799\tvalidation_1-merror:0.37642\n",
      "[421]\tvalidation_0-mlogloss:0.63568\tvalidation_0-merror:0.35993\tvalidation_1-mlogloss:0.64796\tvalidation_1-merror:0.37644\n",
      "[422]\tvalidation_0-mlogloss:0.63562\tvalidation_0-merror:0.35986\tvalidation_1-mlogloss:0.64794\tvalidation_1-merror:0.37641\n",
      "[423]\tvalidation_0-mlogloss:0.63557\tvalidation_0-merror:0.35973\tvalidation_1-mlogloss:0.64793\tvalidation_1-merror:0.37634\n",
      "[424]\tvalidation_0-mlogloss:0.63552\tvalidation_0-merror:0.35965\tvalidation_1-mlogloss:0.64791\tvalidation_1-merror:0.37627\n",
      "[425]\tvalidation_0-mlogloss:0.63547\tvalidation_0-merror:0.35959\tvalidation_1-mlogloss:0.64789\tvalidation_1-merror:0.37629\n",
      "[426]\tvalidation_0-mlogloss:0.63542\tvalidation_0-merror:0.35951\tvalidation_1-mlogloss:0.64788\tvalidation_1-merror:0.37632\n",
      "[427]\tvalidation_0-mlogloss:0.63537\tvalidation_0-merror:0.35943\tvalidation_1-mlogloss:0.64787\tvalidation_1-merror:0.37622\n",
      "[428]\tvalidation_0-mlogloss:0.63531\tvalidation_0-merror:0.35940\tvalidation_1-mlogloss:0.64784\tvalidation_1-merror:0.37639\n",
      "[429]\tvalidation_0-mlogloss:0.63525\tvalidation_0-merror:0.35928\tvalidation_1-mlogloss:0.64783\tvalidation_1-merror:0.37638\n",
      "[430]\tvalidation_0-mlogloss:0.63520\tvalidation_0-merror:0.35925\tvalidation_1-mlogloss:0.64781\tvalidation_1-merror:0.37641\n",
      "[431]\tvalidation_0-mlogloss:0.63513\tvalidation_0-merror:0.35912\tvalidation_1-mlogloss:0.64779\tvalidation_1-merror:0.37628\n",
      "[432]\tvalidation_0-mlogloss:0.63507\tvalidation_0-merror:0.35900\tvalidation_1-mlogloss:0.64776\tvalidation_1-merror:0.37625\n",
      "[433]\tvalidation_0-mlogloss:0.63501\tvalidation_0-merror:0.35892\tvalidation_1-mlogloss:0.64775\tvalidation_1-merror:0.37630\n",
      "[434]\tvalidation_0-mlogloss:0.63497\tvalidation_0-merror:0.35885\tvalidation_1-mlogloss:0.64773\tvalidation_1-merror:0.37622\n",
      "[435]\tvalidation_0-mlogloss:0.63493\tvalidation_0-merror:0.35884\tvalidation_1-mlogloss:0.64772\tvalidation_1-merror:0.37627\n",
      "[436]\tvalidation_0-mlogloss:0.63487\tvalidation_0-merror:0.35880\tvalidation_1-mlogloss:0.64770\tvalidation_1-merror:0.37624\n",
      "[437]\tvalidation_0-mlogloss:0.63481\tvalidation_0-merror:0.35867\tvalidation_1-mlogloss:0.64768\tvalidation_1-merror:0.37609\n",
      "[438]\tvalidation_0-mlogloss:0.63475\tvalidation_0-merror:0.35866\tvalidation_1-mlogloss:0.64766\tvalidation_1-merror:0.37605\n",
      "[439]\tvalidation_0-mlogloss:0.63471\tvalidation_0-merror:0.35862\tvalidation_1-mlogloss:0.64764\tvalidation_1-merror:0.37596\n",
      "[440]\tvalidation_0-mlogloss:0.63465\tvalidation_0-merror:0.35855\tvalidation_1-mlogloss:0.64763\tvalidation_1-merror:0.37586\n",
      "[441]\tvalidation_0-mlogloss:0.63462\tvalidation_0-merror:0.35857\tvalidation_1-mlogloss:0.64762\tvalidation_1-merror:0.37591\n",
      "[442]\tvalidation_0-mlogloss:0.63457\tvalidation_0-merror:0.35844\tvalidation_1-mlogloss:0.64760\tvalidation_1-merror:0.37590\n",
      "[443]\tvalidation_0-mlogloss:0.63451\tvalidation_0-merror:0.35839\tvalidation_1-mlogloss:0.64758\tvalidation_1-merror:0.37596\n",
      "[444]\tvalidation_0-mlogloss:0.63446\tvalidation_0-merror:0.35835\tvalidation_1-mlogloss:0.64756\tvalidation_1-merror:0.37599\n",
      "[445]\tvalidation_0-mlogloss:0.63441\tvalidation_0-merror:0.35828\tvalidation_1-mlogloss:0.64754\tvalidation_1-merror:0.37592\n",
      "[446]\tvalidation_0-mlogloss:0.63436\tvalidation_0-merror:0.35818\tvalidation_1-mlogloss:0.64753\tvalidation_1-merror:0.37594\n",
      "[447]\tvalidation_0-mlogloss:0.63429\tvalidation_0-merror:0.35809\tvalidation_1-mlogloss:0.64750\tvalidation_1-merror:0.37592\n",
      "[448]\tvalidation_0-mlogloss:0.63423\tvalidation_0-merror:0.35804\tvalidation_1-mlogloss:0.64748\tvalidation_1-merror:0.37593\n",
      "[449]\tvalidation_0-mlogloss:0.63416\tvalidation_0-merror:0.35799\tvalidation_1-mlogloss:0.64746\tvalidation_1-merror:0.37612\n",
      "[450]\tvalidation_0-mlogloss:0.63411\tvalidation_0-merror:0.35791\tvalidation_1-mlogloss:0.64744\tvalidation_1-merror:0.37609\n",
      "[451]\tvalidation_0-mlogloss:0.63406\tvalidation_0-merror:0.35791\tvalidation_1-mlogloss:0.64742\tvalidation_1-merror:0.37603\n",
      "[452]\tvalidation_0-mlogloss:0.63400\tvalidation_0-merror:0.35775\tvalidation_1-mlogloss:0.64740\tvalidation_1-merror:0.37605\n",
      "[453]\tvalidation_0-mlogloss:0.63394\tvalidation_0-merror:0.35769\tvalidation_1-mlogloss:0.64739\tvalidation_1-merror:0.37598\n",
      "[454]\tvalidation_0-mlogloss:0.63389\tvalidation_0-merror:0.35763\tvalidation_1-mlogloss:0.64737\tvalidation_1-merror:0.37598\n",
      "[455]\tvalidation_0-mlogloss:0.63384\tvalidation_0-merror:0.35759\tvalidation_1-mlogloss:0.64735\tvalidation_1-merror:0.37598\n",
      "[456]\tvalidation_0-mlogloss:0.63378\tvalidation_0-merror:0.35751\tvalidation_1-mlogloss:0.64734\tvalidation_1-merror:0.37600\n",
      "[457]\tvalidation_0-mlogloss:0.63373\tvalidation_0-merror:0.35742\tvalidation_1-mlogloss:0.64733\tvalidation_1-merror:0.37599\n",
      "[458]\tvalidation_0-mlogloss:0.63369\tvalidation_0-merror:0.35734\tvalidation_1-mlogloss:0.64731\tvalidation_1-merror:0.37601\n",
      "[459]\tvalidation_0-mlogloss:0.63363\tvalidation_0-merror:0.35725\tvalidation_1-mlogloss:0.64729\tvalidation_1-merror:0.37601\n",
      "XGBoost training metrics saved to xgboost_training_metrics.png\n",
      "XGBoost feature importance plot saved to xgboost_feature_importance.png\n",
      "Top 20 XGBoost important features:\n",
      "               feature  importance\n",
      "24                  24    0.293787\n",
      "124             24_std    0.258622\n",
      "58                  58    0.020707\n",
      "158             58_std    0.019218\n",
      "79       53_yeojohnson    0.014327\n",
      "147             47_std    0.013882\n",
      "151             51_std    0.012375\n",
      "190  41_yeojohnson_std    0.011031\n",
      "59                  59    0.009827\n",
      "43                  43    0.009654\n",
      "47                  47    0.007884\n",
      "56                  56    0.007645\n",
      "57                  57    0.007636\n",
      "156             56_std    0.007146\n",
      "55                  55    0.005761\n",
      "54                  54    0.005332\n",
      "82       55_yeojohnson    0.005145\n",
      "80       49_yeojohnson    0.004961\n",
      "109              9_std    0.004786\n",
      "92       54_yeojohnson    0.004655\n",
      "XGBoost Results:\n",
      "  Training Accuracy: 0.6414\n",
      "  Training Balanced Accuracy: 0.5973\n",
      "  Training F1 (macro): 0.6125\n",
      "  Test Accuracy: 0.6241\n",
      "  Test Balanced Accuracy: 0.5784\n",
      "  Test F1 (macro): 0.5932\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68    127409\n",
      "           1       0.61      0.49      0.54     51389\n",
      "           2       0.60      0.52      0.56     56095\n",
      "\n",
      "    accuracy                           0.62    234893\n",
      "   macro avg       0.62      0.58      0.59    234893\n",
      "weighted avg       0.62      0.62      0.62    234893\n",
      "\n",
      "Model saved to xgboost_model.joblib\n",
      "\n",
      "Training LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.265386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47395\n",
      "[LightGBM] [Info] Number of data points in the train set: 939568, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "LightGBM feature importance plot saved to lightgbm_feature_importance.png\n",
      "Top 20 LightGBM important features:\n",
      "               feature  importance\n",
      "24                  24         778\n",
      "54                  54         599\n",
      "68                  68         502\n",
      "124             24_std         473\n",
      "69                  69         459\n",
      "2                    2         456\n",
      "0                    0         452\n",
      "4                    4         450\n",
      "67                  67         362\n",
      "49                  49         348\n",
      "14                  14         328\n",
      "21                  21         307\n",
      "18                  18         294\n",
      "180  49_yeojohnson_std         284\n",
      "47                  47         276\n",
      "3                    3         276\n",
      "5                    5         254\n",
      "149             49_std         254\n",
      "55                  55         249\n",
      "53                  53         237\n",
      "LightGBM Results:\n",
      "  Training Accuracy: 0.5229\n",
      "  Training Balanced Accuracy: 0.6867\n",
      "  Training F1 (macro): 0.5205\n",
      "  Test Accuracy: 0.5192\n",
      "  Test Balanced Accuracy: 0.6832\n",
      "  Test F1 (macro): 0.5167\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.16      0.27    127409\n",
      "           1       0.48      0.94      0.64     51389\n",
      "           2       0.49      0.94      0.65     56095\n",
      "\n",
      "    accuracy                           0.52    234893\n",
      "   macro avg       0.58      0.68      0.52    234893\n",
      "weighted avg       0.65      0.52      0.44    234893\n",
      "\n",
      "Model saved to lightgbm_model.joblib\n",
      "\n",
      "Training NeuralNetwork...\n",
      "Epoch 1/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - accuracy: 0.4755 - loss: 0.8664 - val_accuracy: 0.4547 - val_loss: 0.8222 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4839 - loss: 0.6685 - val_accuracy: 0.4551 - val_loss: 0.7966 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4810 - loss: 0.6655 - val_accuracy: 0.4547 - val_loss: 0.8425 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4812 - loss: 0.6608 - val_accuracy: 0.4574 - val_loss: 0.7846 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4817 - loss: 0.6583 - val_accuracy: 0.4799 - val_loss: 0.7865 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4810 - loss: 0.6571 - val_accuracy: 0.4560 - val_loss: 0.7884 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4802 - loss: 0.6567 - val_accuracy: 0.4555 - val_loss: 0.7614 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4815 - loss: 0.6568 - val_accuracy: 0.4816 - val_loss: 0.7638 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4817 - loss: 0.6555 - val_accuracy: 0.4644 - val_loss: 0.7826 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4812 - loss: 0.6551 - val_accuracy: 0.4647 - val_loss: 0.7697 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4816 - loss: 0.6551 - val_accuracy: 0.4698 - val_loss: 0.7806 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4795 - loss: 0.6549 - val_accuracy: 0.4679 - val_loss: 0.7950 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4806 - loss: 0.6544 - val_accuracy: 0.4764 - val_loss: 0.7778 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4799 - loss: 0.6549 - val_accuracy: 0.4546 - val_loss: 0.8045 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4796 - loss: 0.6538 - val_accuracy: 0.4563 - val_loss: 0.8069 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4799 - loss: 0.6536 - val_accuracy: 0.4547 - val_loss: 0.8005 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4816 - loss: 0.6531 - val_accuracy: 0.4608 - val_loss: 0.7810 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4799 - loss: 0.6539 - val_accuracy: 0.4724 - val_loss: 0.8259 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.4799 - loss: 0.6539 - val_accuracy: 0.4886 - val_loss: 0.7533 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4790 - loss: 0.6535 - val_accuracy: 0.4658 - val_loss: 0.7871 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4831 - loss: 0.6465 - val_accuracy: 0.4608 - val_loss: 0.8067 - learning_rate: 5.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4829 - loss: 0.6439 - val_accuracy: 0.4580 - val_loss: 0.7932 - learning_rate: 5.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4845 - loss: 0.6437 - val_accuracy: 0.4681 - val_loss: 0.7674 - learning_rate: 5.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4843 - loss: 0.6435 - val_accuracy: 0.4691 - val_loss: 0.7796 - learning_rate: 5.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4835 - loss: 0.6446 - val_accuracy: 0.4654 - val_loss: 0.7669 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4838 - loss: 0.6447 - val_accuracy: 0.4752 - val_loss: 0.7860 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4861 - loss: 0.6403 - val_accuracy: 0.4796 - val_loss: 0.7614 - learning_rate: 2.5000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4848 - loss: 0.6382 - val_accuracy: 0.4813 - val_loss: 0.7741 - learning_rate: 2.5000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.4885 - loss: 0.6377 - val_accuracy: 0.4794 - val_loss: 0.7664 - learning_rate: 2.5000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m7341/7341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.4871 - loss: 0.6371 - val_accuracy: 0.4709 - val_loss: 0.7608 - learning_rate: 2.5000e-04\n",
      "Training metrics plot saved to training_metrics.png\n",
      "Error rate plot saved to error_rate.png\n",
      "Neural network training history saved to nn_training_history.png\n",
      "NeuralNetwork Results:\n",
      "  Training Accuracy: 0.4644\n",
      "  Training Balanced Accuracy: 0.6664\n",
      "  Training F1 (macro): 0.4325\n",
      "  Test Accuracy: 0.4641\n",
      "  Test Balanced Accuracy: 0.6660\n",
      "  Test F1 (macro): 0.4321\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.02      0.04    127409\n",
      "           1       0.45      0.99      0.62     51389\n",
      "           2       0.47      0.98      0.63     56095\n",
      "\n",
      "    accuracy                           0.46    234893\n",
      "   macro avg       0.57      0.67      0.43    234893\n",
      "weighted avg       0.64      0.46      0.31    234893\n",
      "\n",
      "Model saved to neuralnetwork_model.joblib\n",
      "\n",
      "Training Stacking...\n",
      "\n",
      "Training base model: XGBoost\n",
      "  Fold 1/5\n",
      "  Fold 2/5\n",
      "  Fold 3/5\n",
      "  Fold 4/5\n",
      "  Fold 5/5\n",
      "  Training XGBoost on the entire data\n",
      "\n",
      "Training base model: LightGBM\n",
      "  Fold 1/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.257419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47319\n",
      "[LightGBM] [Info] Number of data points in the train set: 751654, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "  Fold 2/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.271200 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47403\n",
      "[LightGBM] [Info] Number of data points in the train set: 751654, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "  Fold 3/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.317269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47361\n",
      "[LightGBM] [Info] Number of data points in the train set: 751654, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "  Fold 4/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47354\n",
      "[LightGBM] [Info] Number of data points in the train set: 751655, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "  Fold 5/5\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.285410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47388\n",
      "[LightGBM] [Info] Number of data points in the train set: 751655, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "  Training LightGBM on the entire data\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.319328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47395\n",
      "[LightGBM] [Info] Number of data points in the train set: 939568, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "[LightGBM] [Info] Start training from score -1.098612\n",
      "\n",
      "Training meta model...\n",
      "Stacking Results:\n",
      "  Training Accuracy: 0.6653\n",
      "  Training Balanced Accuracy: 0.6270\n",
      "  Training F1 (macro): 0.6410\n",
      "  Test Accuracy: 0.6277\n",
      "  Test Balanced Accuracy: 0.5869\n",
      "  Test F1 (macro): 0.6000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.72      0.68    127409\n",
      "           1       0.61      0.51      0.55     51389\n",
      "           2       0.61      0.54      0.57     56095\n",
      "\n",
      "    accuracy                           0.63    234893\n",
      "   macro avg       0.62      0.59      0.60    234893\n",
      "weighted avg       0.63      0.63      0.62    234893\n",
      "\n",
      "Model saved to stacking_model.joblib\n",
      "\n",
      "Best model: Stacking with Test F1 (macro): 0.6000\n"
     ]
    }
   ],
   "source": [
    "# 训练多个高级模型并比较\n",
    "best_model, results, preprocessing = train_advanced_models(\n",
    "    train_df=train_df,\n",
    "    target_column='label',\n",
    "    skewed_features=[52, 67, 65, 61, 62, 64, 63, 66, 69, 5, 49, 3, 68, 53, 57, 56, 54, 55, 51, 15, 31, 34, 41, 43, 39, 16, 14, 44, 42, 40, 4, 2],\n",
    "    binary_features=[56, 57, 58, 59, 60],\n",
    "    high_kurtosis_features=[52, 67, 65, 62, 61, 64, 63, 66, 55, 49, 69, 4, 2, 5, 3, 17, 15, 68, 53, 54, 40, 14, 16, 57, 56, 42, 44, 13],\n",
    "    use_pca= False, \n",
    "    use_smote= False, \n",
    "    use_bayes_opt= True, \n",
    "    use_stacking= True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "APPLYING ADVANCED MODEL\n",
      "==================================================\n",
      "New data shape: (1175302, 71)\n",
      "Target column 'label' found in data.\n",
      "Loading model from stacking_model.joblib...\n",
      "Model loaded: StackingClassifier\n",
      "Applying preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying transformations: 100%|██████████| 30/30 [00:00<00:00, 39.14it/s]\n",
      "Applying standardization: 100%|██████████| 100/100 [00:00<00:00, 165.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data shape: (1175302, 200)\n",
      "Generating predictions...\n",
      "Prediction complete\n",
      "Predictions shape: (1175302,)\n",
      "Probabilities shape: (1175302, 3)\n",
      "\n",
      "EVALUATING MODEL ON NEW DATA:\n",
      "Accuracy: 0.6011\n",
      "Balanced Accuracy: 0.5470\n",
      "F1 (macro): 0.5636\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.72      0.66    635393\n",
      "           1       0.59      0.45      0.51    249039\n",
      "           2       0.58      0.47      0.52    290870\n",
      "\n",
      "    accuracy                           0.60   1175302\n",
      "   macro avg       0.59      0.55      0.56   1175302\n",
      "weighted avg       0.60      0.60      0.59   1175302\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 应用最佳模型到新数据\n",
    "predictions, probabilities = apply_advanced_model(\n",
    "    new_df=test_df,\n",
    "    target_column='label',\n",
    "    model_path='stacking_model.joblib'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
